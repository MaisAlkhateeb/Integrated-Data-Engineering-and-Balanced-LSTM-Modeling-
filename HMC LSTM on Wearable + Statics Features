{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13047461,"sourceType":"datasetVersion","datasetId":7421066},{"sourceId":268813348,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# âœ… 1. Define the libraries and upload the dataset","metadata":{}},{"cell_type":"code","source":"# Step 1: Create a new environment\n!python -m venv cleanenv\n\n# Step 2: Activate it\n# On Windows:\n!cleanenv\\Scripts\\activate\n# On Mac/Linux:\n#source cleanenv/bin/activate\n\n# Step 3: Install only what you need\n!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.0 imbalanced-learn==0.13.0 tensorflow==2.18.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:53:21.276016Z","iopub.execute_input":"2025-10-31T19:53:21.276712Z","iopub.status.idle":"2025-10-31T19:53:34.691167Z","shell.execute_reply.started":"2025-10-31T19:53:21.276687Z","shell.execute_reply":"2025-10-31T19:53:34.690091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GOOD (pick one)\nimport torch                         # PyTorch only\n# OR\nimport tensorflow as tf              # TensorFlow only\n# OR\nimport jax                           # JAX only","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:53:38.614675Z","iopub.execute_input":"2025-10-31T19:53:38.614967Z","iopub.status.idle":"2025-10-31T19:53:53.930148Z","shell.execute_reply.started":"2025-10-31T19:53:38.614943Z","shell.execute_reply":"2025-10-31T19:53:53.929553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import TF first so cuDNN is registered once\nimport tensorflow as tf\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:54:10.96213Z","iopub.execute_input":"2025-10-31T19:54:10.962725Z","iopub.status.idle":"2025-10-31T19:54:10.966192Z","shell.execute_reply.started":"2025-10-31T19:54:10.9627Z","shell.execute_reply":"2025-10-31T19:54:10.965278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy, scipy, sklearn, imblearn, tensorflow as tf\n\nprint(\"numpy:\", numpy.__version__)\nprint(\"scipy:\", scipy.__version__)\nprint(\"scikit-learn:\", sklearn.__version__)\nprint(\"imbalanced-learn:\", imblearn.__version__)\nprint(\"tensorflow:\", tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:54:16.669976Z","iopub.execute_input":"2025-10-31T19:54:16.670253Z","iopub.status.idle":"2025-10-31T19:54:16.881802Z","shell.execute_reply.started":"2025-10-31T19:54:16.670233Z","shell.execute_reply":"2025-10-31T19:54:16.880604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom sklearn.utils import resample\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:54:43.013453Z","iopub.execute_input":"2025-10-31T19:54:43.01415Z","iopub.status.idle":"2025-10-31T19:54:43.035234Z","shell.execute_reply.started":"2025-10-31T19:54:43.014124Z","shell.execute_reply":"2025-10-31T19:54:43.034395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This script creates **hourly-level dynamic features** for each patient during **Ramadan** using continuous glucose monitoring (CGM) data and lifestyle metrics (activity, sleep, physiology) from wearable devices.\nItâ€™s part of a preprocessing pipeline for modeling glucose behavior and hypoglycemia risk.\n\nHereâ€™s a complete explanation ðŸ‘‡\n\n---\n\n## ðŸ§© **Overall Goal**\n\nTo transform raw timestamped CGM and wearable data into **hourly summarized features** that represent glucose dynamics, lifestyle behavior, and physiological activity during Ramadan â€” ready for statistical or machine-learning analysis.\n\n---\n\n## ðŸ§­ **1ï¸âƒ£ Load and Parse Data**\n\n* Loads the file:\n\n  ```\n  intraday_with_visits.csv\n  ```\n\n  which includes per-minute or per-sample CGM and Huawei sensor data.\n* Converts all timestamps (`start`, `date`) to datetime format.\n* Extracts:\n\n  * `hour` â†’ the nearest hour (e.g., 14:00, 15:00).\n  * `hour_of_day` â†’ the hour index (0â€“23).\n\nðŸ‘‰ *Purpose:* Prepare a unified hourly timeline for every patient.\n\n---\n\n## ðŸ“† **2ï¸âƒ£ Filter for Ramadan Period**\n\n* Keeps only data between **March 22 â€“ April 19, 2023**.\n* Ensures the dataset includes `cgm` readings (continuous glucose values).\n* Adds a **binary flag `hypo`** = `True` when CGM â‰¤ 70 mg/dL (hypoglycemia reading).\n\nðŸ‘‰ *Purpose:* Focus analysis strictly on the fasting month, removing other phases.\n\n---\n\n## â± **3ï¸âƒ£ Validate Hourly Windows**\n\n* Keeps only hours with **â‰¥4 CGM readings** to ensure data quality.\n* This removes incomplete or sparse hours.\n\nðŸ‘‰ *Purpose:* Guarantee each hourly feature represents stable glucose behavior.\n\n---\n\n## ðŸ“Š **4ï¸âƒ£ Compute Hourly CGM Statistics**\n\nFor each patient and hour:\n\n* `cgm_min` â†’ minimum glucose value\n* `cgm_max` â†’ maximum glucose value\n* `cgm_mean` â†’ mean glucose level\n* `cgm_std` â†’ standard deviation (glucose variability)\n\nAlso adds:\n\n* `hypo_label` â†’ `1` if any CGM reading in that hour was â‰¤70 mg/dL.\n\nðŸ‘‰ *Purpose:* Capture both variability and hypoglycemia presence within each hour.\n\n---\n\n## ðŸ§® **5ï¸âƒ£ Composite Glucose Features**\n\nCreates two derived indicators:\n\n* `cgm_mean_plus_std`  â†’ average + variability\n* `cgm_mean_minus_std` â†’ average â€“ variability\n\nðŸ‘‰ *Purpose:* Encode range boundaries for dynamic glucose variation.\n\n---\n\n## ðŸ§  **6ï¸âƒ£ PCA on CGM Variables**\n\n* Runs **Principal Component Analysis (PCA)** on `[cgm_min, cgm_max, cgm_mean, cgm_std]`.\n* Extracts **3 principal components** (`pca_cgm1`, `pca_cgm2`, `pca_cgm3`).\n* Reports explained variance (usually >95%).\n\nðŸ‘‰ *Purpose:* Compress CGM dynamics into orthogonal, interpretable axes â€” summarizing glucose pattern, amplitude, and variability.\n\n---\n\n## ðŸƒâ€â™€ï¸ **7ï¸âƒ£ PCA on Lifestyle / Activity / Sleep Features**\n\n* Selects available columns:\n\n  ```\n  steps, distance, calories, heart_rate, spo2, deep, light, rem, nap, awake\n  ```\n* Averages these per hour per patient.\n* Runs PCA â†’ extracts **3 lifestyle components**:\n\n  * `pc1_activity_energy` â†’ overall activity/energy output\n  * `pc2_physiology` â†’ physiological or heart-rateâ€“related factors\n  * `pc3_sleep_rest` â†’ rest and sleep quality indices\n* Reports explained variance ratio.\n\nðŸ‘‰ *Purpose:* Reduce multiple wearable signals into interpretable latent factors.\n\n---\n\n## ðŸ“‘ **8ï¸âƒ£ Finalize and Sort**\n\n* Orders the dataset by patient and hour.\n* Keeps only relevant feature columns:\n\n  ```\n  cgm_min, cgm_max, cgm_mean, cgm_std,\n  cgm_mean_plus_std, cgm_mean_minus_std,\n  pca_cgm1â€“3, pc1_activity_energy, pc2_physiology, pc3_sleep_rest, hypo_label\n  ```\n* Prints a preview of the final dataset.\n\n---\n\n## ðŸ’¾ **9ï¸âƒ£ Save Hourly Feature File**\n\nExports the final hourly-level dataset to:\n\n```\n/kaggle/working/dynamic_hourly_features_ramadan.csv\n```\n\nEach row now represents **one patient-hour** with fully engineered glucose and lifestyle features.\n\n---\n\n## âœ… **Summary in One Line**\n\n> This code aggregates intraday CGM and wearable sensor data into **hourly-level Ramadan features**, computing glucose statistics, detecting hypoglycemia, and summarizing glucose and lifestyle variability using **PCA-derived composite components** â€” producing a clean, feature-rich dataset for modeling hourly glucose dynamics during fasting.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# =========================\n# CONFIG\n# =========================\nCSV_PATH = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"  # âœ… update path if needed\nOUT_HOURLY_CSV = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\n\n# =========================\n# STEP 0: Load & prepare data\n# =========================\ndf = pd.read_csv(CSV_PATH)\n\n# Parse timestamps\ndf[\"start\"] = pd.to_datetime(df[\"start\"], errors=\"coerce\")\ndf[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\ndf[\"hour\"] = df[\"start\"].dt.floor(\"h\")\ndf[\"hour_of_day\"] = df[\"start\"].dt.hour\n\n# Numeric conversion\nfor col in df.columns:\n    if col not in [\"patientID\", \"huaweiID\", \"visit_assigned\", \"period_main\", \"start\", \"date\", \"hour\", \"hour_of_day\"]:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# =========================\n# STEP 0.1: Ramadan filter\n# =========================\ndf = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n# Ensure CGM exists\nif \"cgm\" not in df.columns:\n    raise ValueError(\"âŒ Dataset must include 'cgm' column.\")\ndf_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n# Hypo reading flag (<= 70 mg/dL)\ndf_cgm[\"hypo\"] = df_cgm[\"cgm\"] <= 70\n\n# =========================\n# STEP 1: Filter valid hours (â‰¥4 CGM readings)\n# =========================\nvalid_hours = (\n    df_cgm.groupby([\"patientID\", \"hour\"])\n    .filter(lambda g: g[\"cgm\"].notna().sum() >= 4)\n)\n\n# =========================\n# STEP 2: Compute hourly CGM statistics\n# =========================\nhourly_features = (\n    valid_hours\n    .groupby([\"patientID\", \"hour_of_day\", \"hour\"], as_index=False)\n    .agg(\n        cgm_min=(\"cgm\", \"min\"),\n        cgm_max=(\"cgm\", \"max\"),\n        cgm_mean=(\"cgm\", \"mean\"),\n        cgm_std=(\"cgm\", \"std\")\n    )\n)\n\n# Hypoglycemia label per hour\nhypo_per_hour = (\n    valid_hours.groupby([\"patientID\", \"hour\"])[\"cgm\"]\n    .apply(lambda x: (x < 70).any())\n    .reset_index(name=\"hypo_label\")\n)\nhourly_features = hourly_features.merge(hypo_per_hour, on=[\"patientID\", \"hour\"], how=\"left\")\n\n# =========================\n# STEP 3: Composite CGM features\n# =========================\nhourly_features[\"cgm_mean_plus_std\"] = hourly_features[\"cgm_mean\"] + hourly_features[\"cgm_std\"]\nhourly_features[\"cgm_mean_minus_std\"] = hourly_features[\"cgm_mean\"] - hourly_features[\"cgm_std\"]\n\n# =========================\n# STEP 4: PCA on CGM stats â†’ 3 components\n# =========================\npca_input_cgm = hourly_features[[\"cgm_min\", \"cgm_max\", \"cgm_mean\", \"cgm_std\"]].fillna(0)\npca_cgm = PCA(n_components=3, random_state=42)\ncgm_components = pca_cgm.fit_transform(pca_input_cgm)\n\nhourly_features[\"pca_cgm1\"] = cgm_components[:, 0]\nhourly_features[\"pca_cgm2\"] = cgm_components[:, 1]\nhourly_features[\"pca_cgm3\"] = cgm_components[:, 2]\n\nprint(\"CGM PCA explained variance:\", pca_cgm.explained_variance_ratio_.round(3))\n\n# =========================\n# STEP 5: PCA on lifestyle/activity/sleep features\n# =========================\nlifestyle_cols = [\"steps\", \"distance\", \"calories\", \"heart_rate\", \"spo2\",\n                  \"deep\", \"light\", \"rem\", \"nap\", \"awake\"]\nlifestyle_cols = [c for c in lifestyle_cols if c in df_cgm.columns]\n\nif lifestyle_cols:\n    lifestyle_hourly = (\n        df_cgm.groupby([\"patientID\", \"hour\"], as_index=False)[lifestyle_cols]\n        .mean()\n        .fillna(0)\n    )\n\n    # Merge lifestyle into hourly_features\n    hourly_features = hourly_features.merge(\n        lifestyle_hourly, on=[\"patientID\", \"hour\"], how=\"left\"\n    ).fillna(0)\n\n    # Run PCA\n    pca_life = PCA(n_components=3, random_state=42)\n    life_components = pca_life.fit_transform(hourly_features[lifestyle_cols])\n\n    hourly_features[\"pc1_activity_energy\"] = life_components[:, 0]\n    hourly_features[\"pc2_physiology\"] = life_components[:, 1]\n    hourly_features[\"pc3_sleep_rest\"] = life_components[:, 2]\n\n    print(\"Lifestyle PCA explained variance:\", pca_life.explained_variance_ratio_.round(3))\n\n# =========================\n# STEP 6: Finalize dataset\n# =========================\nhourly_features = hourly_features.sort_values([\"patientID\", \"hour\"]).reset_index(drop=True)\n\nDYNAMIC_FEATURES = [\n    \"cgm_min\", \"cgm_max\", \"cgm_mean\", \"cgm_std\",\n    \"cgm_mean_plus_std\", \"cgm_mean_minus_std\",\n    \"pca_cgm1\", \"pca_cgm2\", \"pca_cgm3\",\n    \"pc1_activity_energy\", \"pc2_physiology\", \"pc3_sleep_rest\"\n]\n\nprint(hourly_features[[\"patientID\", \"hour\"] + DYNAMIC_FEATURES + [\"hypo_label\"]].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T00:14:52.948272Z","iopub.execute_input":"2025-10-18T00:14:52.948568Z","iopub.status.idle":"2025-10-18T00:15:06.23163Z","shell.execute_reply.started":"2025-10-18T00:14:52.948549Z","shell.execute_reply":"2025-10-18T00:15:06.230766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Leak-safe All static visist and hourly Ramadan features + Balanced LSTM\n(hourly builder + sequences + training utilities)Below is a single, leakâ€‘safe endâ€‘toâ€‘end script that:\n\nRobustly detects patient/date/variable/value columns (handles headers like PatientID (Huawei Data)).\n\nSplits by patient first, then fits PCA & scalers on TRAIN only.\n\nBuilds sequences with optional perâ€‘patient static features.\n\nTrains LSTM variants with classâ€‘weighted focal loss and optional resampling.\n\nChooses decision thresholds on the VALIDATION set (not the test set) to avoid peeking.\n\nEvaluates on test and writes plots + a summary CSV.\n\nWhat changed vs your last version\n\nAdded flexible column pickers (_pick_patient_col, _pick_date_col, â€¦) and used them everywhere (intraday, visit, static).\n\nThresholds now picked on VAL (Youden and PRâ€‘F1) â†’ no test leakage.\n\nBalanced test creation returns X_stat_bal too (keeps seq+static aligned).\n\nResampling with SMOTE is skipped automatically when static input is enabled (canâ€™t synthesize static safely).","metadata":{}},{"cell_type":"code","source":"# ==============================================\n# Leak-safe Ramadan features + Balanced LSTM\n# (hourly builder + sequences + training utilities)\n# ==============================================\nimport os, warnings, random, re\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nSTATIC_CSV      = \"/kaggle/input/hmc-model-static-variables/outcome_static.csv\"\nVISIT_WIDE_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_wide_by_variable.csv\"\nVISIT_LONG_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_long.csv\"\n\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree.npz\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\nOUT_PLOTS_PNG   = \"/kaggle/working/combined_roc_pr_curves.png\"\n\n# ---- Main periods (inclusive) ----\nI_RAMADAN_START = pd.Timestamp(\"2023-03-22\")\nI_RAMADAN_END   = pd.Timestamp(\"2023-04-19\")\nI_SHAWWAL_START = pd.Timestamp(\"2023-04-20\")\nI_SHAWWAL_END   = pd.Timestamp(\"2023-05-19\")\n\n# Use Ramadan window in pipeline\nRAMADAN_START = I_RAMADAN_START\nRAMADAN_END   = I_RAMADAN_END\n\nHYPO_CUTOFF   = 70.0   # mg/dL\nMIN_CGM_PER_H = 4      # minimum CGM points within an hour to keep that hour\nSEQ_LEN       = 36     # sliding window length (hours)\n\n# ---- Visit subperiods (ALL INCLUSIVE) ----\nVISIT_SUBPERIODS = [\n    {\"Visit\": \"Visit 1\", \"start\": \"2023-03-13\", \"end\": \"2023-03-21\"},\n    {\"Visit\": \"Visit 2\", \"start\": \"2023-03-22\", \"end\": \"2023-03-30\"},\n    {\"Visit\": \"Visit 3\", \"start\": \"2023-03-31\", \"end\": \"2023-04-06\"},\n    {\"Visit\": \"Visit 4\", \"start\": \"2023-04-08\", \"end\": \"2023-04-16\"},\n    {\"Visit\": \"Visit 5\", \"start\": \"2023-04-17\", \"end\": \"2023-04-26\"},\n    {\"Visit\": \"Visit 6\", \"start\": \"2023-04-27\", \"end\": \"2023-05-08\"},\n    {\"Visit\": \"Visit 7\", \"start\": \"2023-05-09\", \"end\": \"2023-05-19\"},\n]\n\n# Lifestyle candidates (if present in intraday_with_visits)\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\n# Visit & static feature names\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\n# âœ… Full sequence feature set to feed the model (exactly as requested)\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\",\n    \"cgm_mean_plus_std\",\"cgm_mean_minus_std\",\n    \"pca_cgm1\",\"pca_cgm2\",\"pca_cgm3\",\n    \"pc1_activity_energy\",\"pc2_physiology\",\"pc3_sleep_rest\",\n    \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n)\n\n# Training config\nRANDOM_STATE     = 42\nTHR_MIN, THR_MAX = 0.40, 0.60\nAUGMENT_SIGMA    = 0.01   # small Gaussian jitter on train (set None to disable)\nRESAMPLE_METHODS = [\n    \"none\",            # baseline (class_weight + focal)\n    \"oversample_seq\",  # duplicate minority sequences\n    \"undersample_seq\", # downsample majority sequences\n    # SMOTE-family below only when no static input is used\n    \"smote\", \"smoteenn\", \"smotetomek\"\n]\nUSE_STATIC_INPUT = True  # set False to ignore static input entirely (enables SMOTE variants safely)\n\n# --------------------\n# Utilities (robust column picking)\n# --------------------\ndef set_global_seeds(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_global_seeds(RANDOM_STATE)\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\",\"Visit\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef safe_encode_gender(series):\n    if series.dtype == \"object\":\n        return (series.str.strip().str.lower().map({\"male\":1, \"m\":1, \"female\":0, \"f\":0}))\n    return pd.to_numeric(series, errors=\"coerce\")\n\ndef split_patients(unique_pids, test_size=0.3, val_size=0.1, random_state=RANDOM_STATE):\n    train_pids, test_pids = train_test_split(unique_pids, test_size=test_size, random_state=random_state)\n    val_fraction = val_size / max(1e-9, (1.0 - test_size))\n    train_pids, val_pids = train_test_split(train_pids, test_size=val_fraction, random_state=random_state)\n    return np.array(train_pids), np.array(val_pids), np.array(test_pids)\n\ndef _normalize_date(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    return s.dt.normalize()\n\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef _pick_col_flex(\n    df: pd.DataFrame,\n    preferred=None,\n    required=False,\n    name=\"\",\n    must_contain_all=None,\n    any_contains=None,\n):\n    cols = list(df.columns)\n    norm_map = {c: _norm_col(c) for c in cols}\n\n    # (1) exact by case-insensitive preferred\n    if preferred:\n        lower_pref = {str(p).strip().lower(): p for p in preferred}\n        for c in cols:\n            if str(c).strip().lower() in lower_pref:\n                return c\n\n    # (2) exact by normalized preferred\n    if preferred:\n        pref_norm = {_norm_col(p): p for p in preferred}\n        for c, n in norm_map.items():\n            if n in pref_norm:\n                return c\n\n    # (3) heuristics on normalized names\n    cands = []\n    for c, n in norm_map.items():\n        ok = True\n        if must_contain_all:\n            for tok in must_contain_all:\n                if _norm_col(tok) not in n:\n                    ok = False\n                    break\n        if ok and any_contains:\n            if not any(_norm_col(tok) in n for tok in any_contains):\n                ok = False\n        if ok:\n            cands.append(c)\n    if cands:\n        def _priority(col: str):\n            n = norm_map[col]\n            starts_pid = n.startswith(\"patientid\")\n            has_pid    = \"patientid\" in n\n            return (-(starts_pid or has_pid), len(n))\n        cands.sort(key=_priority)\n        return cands[0]\n\n    if required:\n        raise KeyError(\n            f\"Required column not found for {name}. \"\n            f\"preferred={preferred} | must_contain_all={must_contain_all} | any_contains={any_contains}. \"\n            f\"Available: {cols}\"\n        )\n    return None\n\ndef _pick_patient_col(df: pd.DataFrame) -> str:\n    preferred = [\"patientID\", \"patientId\", \"PatientID (Huawei Data)\", \"subject_id\", \"patid\", \"pid\", \"id\", \"huaweiid\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"patientID\",\n                          must_contain_all=[\"id\"], any_contains=[\"patient\",\"subject\",\"pat\",\"huawei\"])\n\ndef _pick_date_col(df: pd.DataFrame) -> str:\n    preferred = [\"start\",\"date\", \"visit_date\", \"Date\", \"day\", \"timestamp\", \"Visit Date\", \"date_of_visit\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"date_or_start\",\n                          any_contains=[\"start\",\"date\",\"visit\",\"day\",\"timestamp\"])\n\ndef _pick_variable_col(df: pd.DataFrame) -> str:\n    preferred = [\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"variable\",\n                          any_contains=[\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"])\n\ndef _pick_value_col(df: pd.DataFrame) -> str:\n    preferred = [\"value\",\"val\",\"measure_value\",\"reading\",\"amount\",\"score\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"value\",\n                          any_contains=[\"value\",\"val\",\"measurevalue\",\"reading\",\"amount\",\"score\"])\n\n# ---------------------------\n# Header/CSV robustness for VISIT-WIDE\n# ---------------------------\ndef _clean_header_name(s: str) -> str:\n    if not isinstance(s, str):\n        s = str(s)\n    s = s.strip()\n    # Collapse duplicated header text (e.g., \"PatientID (Huawei Data)PatientID (Huawei Data)\")\n    if \"PatientID\" in s and s.count(\"PatientID\") > 1:\n        return \"PatientID (Huawei Data)\"\n    # Remove Unnamed: N\n    if s.startswith(\"Unnamed:\"):\n        return \"\"\n    return s\n\ndef _find_header_row(df_probe: pd.DataFrame) -> int:\n    \"\"\"Heuristics: pick the first row whose cells contain header-like tokens.\"\"\"\n    tokens = [\"patient\", \"visit\", \"date\", \"meals\", \"carb\", \"dose\", \"fasting\"]\n    for r in range(min(5, len(df_probe))):\n        row = df_probe.iloc[r].astype(str).str.lower()\n        hits = sum(any(t in cell for t in tokens) for cell in row)\n        if hits >= 2:\n            return r\n    # fallback: if row0 mostly numeric -> header likely row1\n    row0 = df_probe.iloc[0].astype(str)\n    numish = sum(cell.replace(\".\", \"\", 1).isdigit() for cell in row0)\n    return 1 if numish > max(2, df_probe.shape[1] // 2) else 0\n\ndef _read_visit_wide_robust(path: str) -> pd.DataFrame:\n    \"\"\"Reads visit-wide CSV when row1=IDs and row2=headers (or other noisy cases).\"\"\"\n    probe = pd.read_csv(path, header=None, nrows=5)\n    hdr_idx = _find_header_row(probe)\n    df = pd.read_csv(path, header=hdr_idx)\n    # Clean headers\n    new_cols = []\n    for c in df.columns:\n        new_cols.append(_clean_header_name(c))\n    df.columns = new_cols\n    # Drop empty header columns created from Unnamed:\n    df = df.loc[:, [c for c in df.columns if c != \"\"]]\n    return df\n\ndef _ensure_patient_column(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Normalize patient column to 'patientID'.\"\"\"\n    if \"patientID\" in df.columns:\n        return df\n    if \"PatientID (Huawei Data)\" in df.columns:\n        return df.rename(columns={\"PatientID (Huawei Data)\": \"patientID\"})\n    cands = [c for c in df.columns if \"patient\" in c.lower() or c.lower()==\"id\" or \"huawei\" in c.lower()]\n    if cands:\n        return df.rename(columns={cands[0]: \"patientID\"})\n    # last resort: first column as ID\n    return df.rename(columns={df.columns[0]: \"patientID\"})\n\n# ---------------------------\n# Helpers for Visit ranges â†’ daily dates\n# ---------------------------\ndef _expand_rows_by_visit_date_range(df, visit_col=\"Visit\"):\n    \"\"\"Expand each row with a 'Visit' label into one row per day in VISIT_SUBPERIODS (inclusive).\"\"\"\n    vr = {v[\"Visit\"]: (pd.to_datetime(v[\"start\"]), pd.to_datetime(v[\"end\"]))\n          for v in VISIT_SUBPERIODS}\n    out = []\n    for _, r in df.iterrows():\n        vname = r.get(visit_col)\n        if pd.isna(vname) or vname not in vr:\n            # Unknown visit label; skip expansion\n            continue\n        d0, d1 = vr[vname]\n        for d in pd.date_range(d0, d1, freq=\"D\"):\n            rr = r.copy()\n            rr[\"date\"] = d.normalize()\n            out.append(rr)\n    if not out:\n        return df.assign(date=pd.NaT)\n    return pd.DataFrame(out)\n\n# ---------------------------\n# Static loader (header on second row; handle Huawei ID)\n# ---------------------------\ndef load_static_df(static_csv=STATIC_CSV, needed=STATIC_COLS):\n    if not static_csv or not os.path.exists(static_csv):\n        print(\"âš ï¸ Static CSV not found; static features will be zero-filled.\")\n        return None\n    # header on second row\n    df = pd.read_csv(static_csv, header=1)\n    df = _ensure_patient_column(df)\n\n    keep = [\"patientID\"] + [c for c in needed if c in df.columns]\n    df = df[keep].drop_duplicates(subset=[\"patientID\"]).copy()\n\n    if \"Gender\" in df.columns:\n        df[\"Gender\"] = safe_encode_gender(df[\"Gender\"])\n    for c in keep:\n        if c != \"patientID\":\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n    print(\"â„¹ï¸ static: header fixed (second row) & patientID normalized.\")\n    return df\n\n# ---------------------------\n# Visit loader (row1 IDs / row2 headers; long fallback; Visitâ†’date expansion)\n# ---------------------------\ndef load_visit_df(visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV, needed=VISIT_COLS):\n    # ---------- WIDE FIRST ----------\n    if visit_wide_csv and os.path.exists(visit_wide_csv):\n        wide = _read_visit_wide_robust(visit_wide_csv)\n        wide = _ensure_patient_column(wide)\n\n        # Prefer a date column if present\n        date_cands = [c for c in wide.columns if (\"date\" in c.lower()) or (\"visit date\" in c.lower()) or (c.lower()==\"day\")]\n        if date_cands:\n            wide = wide.rename(columns={date_cands[0]: \"date\"})\n            wide[\"date\"] = pd.to_datetime(wide[\"date\"], errors=\"coerce\").dt.normalize()\n        else:\n            # If no explicit date, but there is a Visit label, expand to daily rows\n            vcol = \"Visit\" if \"Visit\" in wide.columns else None\n            if not vcol:\n                vc = [c for c in wide.columns if \"visit\" in c.lower()]\n                if vc: vcol = vc[0]\n            if vcol:\n                if vcol != \"Visit\":\n                    wide = wide.rename(columns={vcol: \"Visit\"})\n                wide = _expand_rows_by_visit_date_range(wide, visit_col=\"Visit\")\n                print(\"ðŸ—“ï¸ visit-wide: expanded daily dates from Visit subperiods.\")\n            else:\n                # No date and no Visit column: return patient-level only\n                keep = [\"patientID\"] + [c for c in needed if c in wide.columns]\n                if len(keep) > 1:\n                    print(\"âš ï¸ visit-wide: no date/Visit; returning patient-level visit vars.\")\n                    return wide[keep].copy()\n                else:\n                    print(\"âš ï¸ visit-wide: no date/Visit & no needed vars; trying LONG.\")\n                    wide = None  # fall through to long\n\n        if wide is not None:\n            # standardize column names for the 4 visit variables if present\n            ren_map = {\n                \"Carb (g/day)\": \"carb\",\n                \"Meals/day\": \"meals\",\n                \"Total Daily Dose (U)\": \"total_daily_dose_u\",\n                \"Fasting % (29)\": \"fasting_percent_29\"\n            }\n            for old, new in ren_map.items():\n                if old in wide.columns:\n                    wide = wide.rename(columns={old: new})\n\n            keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n            if len(keep) > 2:\n                print(f\"âœ… visit-wide parsed: {keep[2:]}\")\n                return wide[keep].copy()\n            else:\n                print(\"âš ï¸ visit-wide present but missing needed vars; falling back to LONG.\")\n\n    # ---------- LONG FALLBACK ----------\n    if visit_long_csv and os.path.exists(visit_long_csv):\n        long = pd.read_csv(visit_long_csv)\n        long = _ensure_patient_column(long)\n        # pick date/variable/value cols\n        date_col = [c for c in long.columns if \"date\" in c.lower() or \"visit\" in c.lower()][0]\n        var_col  = [c for c in long.columns if \"variable\" in c.lower() or \"feature\" in c.lower()][0]\n        val_col  = [c for c in long.columns if \"value\" in c.lower() or \"reading\" in c.lower() or \"amount\" in c.lower()][0]\n\n        long = long.rename(columns={date_col:\"date\", var_col:\"variable\", val_col:\"value\"})\n        long[\"date\"] = pd.to_datetime(long[\"date\"], errors=\"coerce\").dt.normalize()\n\n        # normalize visit variable names a bit\n        norm = long[\"variable\"].astype(str).str.strip().str.lower()\n        mapr = {\"carb (g/day)\":\"carb\",\"meals/day\":\"meals\",\n                \"total daily dose (u)\":\"total_daily_dose_u\",\n                \"fasting % (29)\":\"fasting_percent_29\"}\n        long[\"variable\"] = norm.map(mapr).fillna(norm)\n\n        wide = (long.pivot_table(index=[\"patientID\",\"date\"], columns=\"variable\", values=\"value\", aggfunc=\"mean\")\n                    .reset_index())\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"âœ… visit-long pivot successful: {keep[2:]}\")\n            return wide[keep].copy()\n\n    print(\"âš ï¸ No usable visit CSVs found; visit features will be zero-filled.\")\n    return None\n\n# ----------------------------------------------------------------\n# Part A â€” Build hourly Ramadan features and leakâ€‘safe transforms\n# ----------------------------------------------------------------\ndef build_hourly_features_with_leak_safe_transforms(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.3, val_size=0.1, random_state=RANDOM_STATE,\n    static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    # Load & parse intraday\n    df = pd.read_csv(in_csv)\n\n    # Robust patient column for intraday too\n    if \"patientID\" not in df.columns:\n        pid_col = _pick_patient_col(df)\n        df = df.rename(columns={pid_col: \"patientID\"})\n        print(f\"â„¹ï¸ intraday: using patientID column = '{pid_col}'\")\n\n    # timestamps (robust)\n    start_col = \"start\" if \"start\" in df.columns else _pick_date_col(df)\n    df[start_col] = to_dt(df[start_col])\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[start_col].dt.date)\n    df[\"hour\"]        = df[start_col].dt.floor(\"h\")\n    df[\"hour_of_day\"] = df[start_col].dt.hour\n\n    df = ensure_numeric(df)\n\n    # Ramadan filter (inclusive)\n    df = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n    # Require CGM\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"âŒ Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # Valid hourly windows\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # Base hourly CGM stats\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(\n                       cgm_min=(\"cgm\",\"min\"),\n                       cgm_max=(\"cgm\",\"max\"),\n                       cgm_mean=(\"cgm\",\"mean\"),\n                       cgm_std=(\"cgm\",\"std\")\n                   )\n                   .sort_values([\"patientID\",\"hour\"])\n                   .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    # Hypo label\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # Lifestyle hourly means (if present)\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean().fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # Composite CGM features\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # Patient-level split (NO LEAK)\n    pids = hourly[\"patientID\"].dropna().unique()\n    train_p, val_p, test_p = split_patients(pids, test_size=test_size, val_size=val_size, random_state=random_state)\n    hourly[\"Split\"] = np.where(hourly[\"patientID\"].isin(train_p), \"train\",\n                        np.where(hourly[\"patientID\"].isin(val_p), \"val\", \"test\"))\n\n    # CGM PCA fit on TRAIN only\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"] == \"train\"\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    def _apply_cgm_pca(df_in):\n        X = scal_cgm.transform(df_in[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df_in.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n    hourly = _apply_cgm_pca(hourly)\n\n    # Lifestyle PCA fit on TRAIN only\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        pca_life  = PCA(n_components=3, random_state=random_state).fit(\n            scal_life.transform(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        )\n        X_all = scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        Z_all = pca_life.transform(X_all)\n        hourly[\"pc1_activity_energy\"] = Z_all[:,0]\n        hourly[\"pc2_physiology\"]      = Z_all[:,1]\n        hourly[\"pc3_sleep_rest\"]      = Z_all[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    # Merge VISIT features (daily)\n    visit_df = load_visit_df(visit_wide_csv, visit_long_csv, VISIT_COLS)\n    hourly[\"date\"] = hourly[\"hour\"].dt.normalize()\n    if visit_df is not None:\n        if \"date\" in visit_df.columns:\n            visit_df[\"date\"] = pd.to_datetime(visit_df[\"date\"], errors=\"coerce\").dt.normalize()\n            # Limit to Ramadan window if dates exist\n            visit_df = visit_df[(visit_df[\"date\"] >= RAMADAN_START) & (visit_df[\"date\"] <= RAMADAN_END)].copy()\n            hourly = hourly.merge(visit_df, on=[\"patientID\",\"date\"], how=\"left\")\n        else:\n            # patient-level only: broadcast across all dates for that patient\n            hourly = hourly.merge(visit_df, on=\"patientID\", how=\"left\")\n\n    for c in VISIT_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    # Merge STATIC features (per patient)\n    static_df = load_static_df(static_csv, STATIC_COLS)\n    if static_df is not None:\n        hourly = hourly.merge(static_df, on=\"patientID\", how=\"left\")\n    for c in STATIC_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    # Save hourly table\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly.to_csv(out_csv, index=False)\n    print(f\"âœ… Saved hourly features (leak-safe) â†’ {out_csv}\")\n\n    return hourly, (train_p, val_p, test_p)\n\n# ---------------------------------------------------------------\n# Part B â€” Build sequences (optionally with per-patient static)\n# ---------------------------------------------------------------\ndef build_sequences_by_split(\n    hourly, splits, seq_len=SEQ_LEN,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    scale_features=True\n):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float).fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n                if static_mat is not None and pid in static_mat.index:\n                    X_stat.append(static_mat.loc[pid].values.astype(float))\n        X_seq = np.array(X_seq); y = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    # Scale sequence features (fit on TRAIN only), and static (fit on TRAIN only)\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size==0: return X\n            n, T, F = X.shape\n            return seq_scaler.transform(X.reshape(-1, F)).reshape(n, T, F)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size>0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size==0: return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"âœ… Sequences built | train={Xtr_s.shape}, val={Xva_s.shape}, test={Xte_s.shape}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ------------------------------------------------------\n# Balanced LSTM pipeline utilities (metrics, resampling)\n# ------------------------------------------------------\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_overall(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef _specificity_per_class(y_true, y_pred, positive_label):\n    y_true_bin = (np.asarray(y_true).ravel() == positive_label).astype(int)\n    y_pred_bin = (np.asarray(y_pred).ravel() == positive_label).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_per_class(y_true, y_pred, positive_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    metrics[\"Overall/Specificity\"]          = _specificity_overall(y_true, y_pred)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/MSE_pred\"]             = mse_pred\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/MSE_prob\"]       = mse_prob\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/MSE_prob\"] = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n    return metrics\n\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask])); idx = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\ndef augment_with_static(X_seq, X_stat, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0:\n        return X_seq, X_stat, y\n    noise = np.random.normal(0, sigma, X_seq.shape)\n    X_seq_aug = np.vstack([X_seq, X_seq + noise])\n    y_aug     = np.hstack([y, y])\n    if X_stat is not None:\n        X_stat_aug = np.vstack([X_stat, X_stat])\n    else:\n        X_stat_aug = None\n    return X_seq_aug, X_stat_aug, y_aug\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE, return_index=False, allow_smote=True):\n    \"\"\"\n    Sequence-level resampling. If return_index=True, also returns the index mapping used so\n    static inputs can be resampled consistently. For SMOTE-family, index mapping isn't\n    meaningful; we disable SMOTE if allow_smote=False.\n    \"\"\"\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n    base_idx = np.arange(n)\n\n    if method == \"none\":\n        return (X, y, base_idx) if return_index else (X, y)\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        rng = np.random.default_rng(random_state)\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0:\n            return (X, y, base_idx) if return_index else (X, y)\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = rng.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = rng.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:  # undersample\n            if n0 > n1:\n                keep0 = rng.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = rng.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        rng.shuffle(keep)\n        Xr, yr = X[keep], y[keep]\n        return (Xr, yr, keep) if return_index else (Xr, yr)\n\n    # SMOTE family\n    if not allow_smote:\n        print(f\"âš ï¸ {method} disabled when static input is used; falling back to 'none'.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    minority_n = int((y==1).sum())\n    majority_n = int((y==0).sum())\n    if minority_n < 2 or majority_n < 2:\n        print(\"âš ï¸ Not enough samples for SMOTE/SMOTEENN/SMOTETomek; skipping resampling.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    Xf = X.reshape(n, -1)\n    if method == \"smote\":\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n\n    Xr = Xr.reshape(-1, T, F)\n    return (Xr, yr, None) if return_index else (Xr, yr)\n\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state=RANDOM_STATE):\n    \"\"\"Return balanced (by label) subsets of X_test, y_test, and X_stat (if given).\"\"\"\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0: \n        return (X_test, y_test, (None if X_stat is None else X_stat))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ------------------------------------------------------\n# Model builders (supports seq-only or seq+static)\n# ------------------------------------------------------\ndef make_model(seq_len, n_seq_f, n_stat_f=0, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n        x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x)\n        x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x);                    x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L1\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l1(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l1(1e-5))(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x);                          x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(32, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(32, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(32, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# ------------------------------------------------------\n# Training runner (VAL for threshold; TEST for final)\n# ------------------------------------------------------\ndef run_balanced_lstm_pipeline(data,\n                               arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n                               resample_methods=RESAMPLE_METHODS,\n                               thr_min=THR_MIN, thr_max=THR_MAX,\n                               random_state=RANDOM_STATE,\n                               results_csv=OUT_RESULTS_CSV,\n                               plots_png=OUT_PLOTS_PNG):\n    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n    os.makedirs(os.path.dirname(plots_png), exist_ok=True)\n    os.makedirs(\"checkpoints\", exist_ok=True)\n\n    Xtr, Xtr_stat, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"], data[\"train\"][\"y\"]\n    Xva, Xva_stat, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"X_stat\"],   data[\"val\"][\"y\"]\n    Xte, Xte_stat, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"X_stat\"],  data[\"test\"][\"y\"]\n\n    # Augment train (and static if present)\n    Xtr, Xtr_stat, ytr = augment_with_static(Xtr, Xtr_stat, ytr, sigma=AUGMENT_SIGMA)\n\n    # Balanced copy of test for diagnostic plots\n    Xte_bal, yte_bal, Xte_stat_bal = make_balanced_test(Xte, yte, X_stat=Xte_stat)\n\n    results     = {}\n    roc_curves  = {}\n    pr_curves   = {}\n\n    allow_smote = (Xtr_stat is None or not USE_STATIC_INPUT)\n\n    def train_eval_one(method_name, arch_name):\n        nonlocal Xtr, ytr, Xtr_stat\n\n        # Resample TRAIN only\n        Xrs, yrs, idx_map = seq_resample(Xtr, ytr, method=method_name, random_state=random_state,\n                                         return_index=True, allow_smote=allow_smote)\n        if Xtr_stat is not None and USE_STATIC_INPUT:\n            if idx_map is None:\n                Xrs_stat = Xtr_stat\n            else:\n                Xrs_stat = Xtr_stat[idx_map]\n        else:\n            Xrs_stat = None\n\n        # Build model\n        seq_len, n_seq_f = Xrs.shape[1], Xrs.shape[2]\n        n_stat_f = 0 if (Xrs_stat is None or not USE_STATIC_INPUT) else Xrs_stat.shape[1]\n        model = make_model(seq_len, n_seq_f, n_stat_f=n_stat_f, arch=arch_name, lr=1e-3)\n\n        # Fit with VAL for early stopping (no peeking at test)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n        ckpt_path = f\"checkpoints/{method_name}__{arch_name}.h5\"\n        cp = ModelCheckpoint(ckpt_path, save_best_only=True, monitor=\"val_loss\", verbose=0)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xrs, Xrs_stat], yrs,\n                      validation_data=([Xva, Xva_stat], yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict([Xrs, Xrs_stat], verbose=0).ravel()\n            p_va  = model.predict([Xva, Xva_stat], verbose=0).ravel()\n            p_te  = model.predict([Xte, Xte_stat], verbose=0).ravel()\n            p_teB = (model.predict([Xte_bal, Xte_stat_bal], verbose=0).ravel()\n                     if Xte_stat_bal is not None else model.predict(Xte_bal, verbose=0).ravel())\n        else:\n            model.fit(Xrs, yrs,\n                      validation_data=(Xva, yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict(Xrs, verbose=0).ravel()\n            p_va  = model.predict(Xva, verbose=0).ravel()\n            p_te  = model.predict(Xte, verbose=0).ravel()\n            p_teB = model.predict(Xte_bal, verbose=0).ravel()\n\n        # ---------- choose thresholds on VALIDATION (not TEST) ----------\n        try:\n            fpr_va, tpr_va, thr_roc_va = roc_curve(yva, p_va); auc_roc = auc(fpr_va, tpr_va)\n        except ValueError:\n            fpr_va, tpr_va, thr_roc_va, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n        youden_va = tpr_va - fpr_va\n        t_roc, _ = _best_threshold_in_range(thr_roc_va, youden_va, thr_min, thr_max)\n\n        prec_va, rec_va, thr_pr_va = precision_recall_curve(yva, p_va)\n        f1s_va = 2*prec_va[:-1]*rec_va[:-1] / (prec_va[:-1]+rec_va[:-1]+1e-8)\n        t_pr, _ = _best_threshold_in_range(thr_pr_va, f1s_va, thr_min, thr_max)\n        ap_val  = average_precision_score(yva, p_va)\n\n        # Curves (validation-based AUC/AP shown)\n        roc_curves[(method_name, arch_name)] = (fpr_va, tpr_va, auc_roc)\n        pr_curves[(method_name, arch_name)]  = (rec_va, prec_va, ap_val)\n        print(f\"ðŸ“Œ [{method_name}/{arch_name}] VAL thresholds â†’ Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{thr_min},{thr_max}])\")\n\n        eval_ts = sorted(set([thr_min, 0.50, thr_max, float(t_roc), float(t_pr)]))\n\n        # Evaluate at thresholds: TRAIN / VAL / TEST / TEST (balanced)\n        for t in eval_ts:\n            yhat_tr  = (p_tr  >= t).astype(int)\n            yhat_va  = (p_va  >= t).astype(int)\n            yhat_te  = (p_te  >= t).astype(int)\n            yhat_teB = (p_teB >= t).astype(int)\n\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__train\"]        = evaluate_full_metrics(yrs,  yhat_tr,  p_tr)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__val\"]          = evaluate_full_metrics(yva,  yhat_va,  p_va)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__test\"]         = evaluate_full_metrics(yte,  yhat_te,  p_te)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__testBalanced\"] = evaluate_full_metrics(yte_bal, yhat_teB, p_teB)\n\n    # Loop: resampling methods Ã— architectures\n    for METHOD in resample_methods:\n        if METHOD in {\"smote\",\"smoteenn\",\"smotetomek\"} and (data[\"train\"][\"X_stat\"] is not None and USE_STATIC_INPUT):\n            print(f\"â­ï¸  Skipping {METHOD} (static input enabled).\")\n            continue\n        print(f\"\\nðŸ” Resampling: {METHOD} | train y-dist = {Counter(data['train']['y'])}\")\n        for ARCH in arch_list:\n            train_eval_one(METHOD, ARCH)\n\n    # --- Plots (validation curves)\n    plt.figure(figsize=(14,6))\n    # ROC\n    plt.subplot(1,2,1)\n    for (meth, arch), (fpr, tpr, auc_roc) in roc_curves.items():\n        plt.plot(fpr, tpr, label=f'{meth}/{arch} (VAL AUC={auc_roc:.3f})')\n    plt.plot([0,1],[0,1],'--',label='Random')\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (Validation)'); plt.legend(fontsize=8)\n    # PR\n    plt.subplot(1,2,2)\n    for (meth, arch), (rec, prec, ap) in pr_curves.items():\n        plt.plot(rec, prec, label=f'{meth}/{arch} (VAL AP={ap:.3f})')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR (Validation)'); plt.legend(fontsize=8)\n    plt.tight_layout(); plt.savefig(plots_png, dpi=300); plt.show()\n    print(f\"ðŸ–¼ï¸ Saved plots â†’ {plots_png}\")\n\n    # --- Results CSV\n    results_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"Key\"})\n    k = results_df[\"Key\"].str.strip()\n    results_df[\"Split\"]  = np.where(k.str.endswith(\"__train\"), \"train\",\n                             np.where(k.str.endswith(\"__val\"), \"val\",\n                             np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                             np.where(k.str.endswith(\"__test\"), \"test\", np.nan))))\n    parts = k.str.split(\"__\")\n    results_df[\"Method\"]    = parts.str[0]\n    results_df[\"Model\"]     = parts.str[1]\n    results_df[\"Threshold\"] = pd.to_numeric(parts.str[2].str.replace(\"thr_\",\"\", regex=False), errors=\"coerce\")\n    results_df.round(6).to_csv(results_csv, index=False)\n    print(f\"ðŸ“‘ Saved results â†’ {results_csv}\")\n\n    return results_df\n\n# --------------------\n# Run end-to-end\n# --------------------\nif __name__ == \"__main__\":\n    # A) Hourly features with leakâ€‘safe PCA & merges\n    hourly, splits = build_hourly_features_with_leak_safe_transforms(\n        in_csv=CSV_INTRADAY_WITH_VISITS,\n        out_csv=OUT_HOURLY_CSV,\n        min_cgm_per_hour=MIN_CGM_PER_H,\n        test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n        static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n    )\n\n    # B) Sequences with visit + static (no leakage; scalers fit on TRAIN only)\n    data = build_sequences_by_split(\n        hourly, splits,\n        seq_len=SEQ_LEN,\n        seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n        static_cols=STATIC_COLS,\n        scale_features=True\n    )\n\n    # (Optional) Save arrays for later reuse\n    np.savez_compressed(\n        OUT_SEQ_NPZ,\n        Xtr=data[\"train\"][\"X_seq\"],  Xtr_stat=(data[\"train\"][\"X_stat\"] if data[\"train\"][\"X_stat\"] is not None else np.empty((0,0))),\n        ytr=data[\"train\"][\"y\"],\n        Xva=data[\"val\"][\"X_seq\"],    Xva_stat=(data[\"val\"][\"X_stat\"] if data[\"val\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yva=data[\"val\"][\"y\"],\n        Xte=data[\"test\"][\"X_seq\"],   Xte_stat=(data[\"test\"][\"X_stat\"] if data[\"test\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yte=data[\"test\"][\"y\"],\n        seq_features_used=np.array(data[\"seq_features_used\"], dtype=object),\n        static_features_used=np.array(data[\"static_features_used\"], dtype=object)\n    )\n    print(f\"ðŸ’¾ Saved sequences â†’ {OUT_SEQ_NPZ}\")\n\n    # C) Train + evaluate (leak-safe thresholds chosen on VAL)\n    results_df = run_balanced_lstm_pipeline(\n        data,\n        arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n        resample_methods=RESAMPLE_METHODS,\n        thr_min=THR_MIN, thr_max=THR_MAX,\n        random_state=RANDOM_STATE,\n        results_csv=OUT_RESULTS_CSV,\n        plots_png=OUT_PLOTS_PNG\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hereâ€™s a concise readâ€‘out of the results you uploaded in **`/mnt/data/results_summary_all.csv`** and what they mean in practice.\n\n---\n\n## What I analyzed\n\nFrom your results file I looked at, per split:\n\n* **Resampling method**: `none`, `oversample_seq`, `undersample_seq`\n* **Model**: `LSTM_100`, `BiLSTM`, `LSTM_50`\n* **Thresholds** evaluated: 0.40, 0.50, 0.60 (and the full probability curves to compute ROCâ€‘AUC/PRâ€‘AUC)\n* **Metrics**: Overall Accuracy/F1 (macro & weighted), perâ€‘class Precision/Recall/F1/Specificity, plus ROCâ€‘AUC and PRâ€‘AUC\n\nSplits present in the file: **train**, **test**, and a **testBalanced** (your balanced diagnostic set with 1:1 class ratio).\n\n---\n\n##  Best practical setting\n\nIf the goal is **reliably catching hypoglycemia (Classâ€¯1)** with strong precision:\n\n* **Resampling**: `oversample_seq`\n* **Model**: `LSTM_100`\n* **Decision threshold**: **0.50**\n\n**On the balanced test set (diagnostic 1:1):**\n\n* **Weighted F1**: **0.801**\n* **Accuracy**: 0.801\n* **Classâ€¯1 (hypo) â€” Precision**: **0.956**\n* **Classâ€¯1 â€” Recall (Sensitivity)**: **0.642**\n* **ROCâ€‘AUC**: 0.906\n* **PRâ€‘AUC**: 0.903 *(note: PRâ€‘AUC depends on prevalence; this is the balanced view)*\n\n**Confusion matrix (balanced test, approximate counts):**\n\n|            |       Pred 0 |      Pred 1 |\n| ---------- | -----------: | ----------: |\n| **True 0** | **TNâ€¯=â€¯130** |  **FPâ€¯=â€¯4** |\n| **True 1** |  **FNâ€¯=â€¯48** | **TPâ€¯=â€¯86** |\n\nThatâ€™s **very few false alarms** (FP) while catching ~64% of hypos.\n\n**If you want a bit more sensitivity** (catch more hypos) and can tolerate a few more false positives, use **thresholdâ€¯=â€¯0.40** for the same model/setup:\n\n* Classâ€¯1 **Recall** â†‘ to **0.664**\n* Classâ€¯1 **Precision** â†’ 0.935\n* Weighted F1 â‰ˆ 0.799\n* Confusion matrix (balanced): **TNâ€¯126, FPâ€¯8, FNâ€¯45, TPâ€¯89**\n\n---\n\n## Why the â€œoriginal testâ€ looks deceptively great\n\nOn the **original (imbalanced) test**, the top rows (e.g., `none / LSTM_100 / thrâ€¯0.60`) show **very high weighted F1 (â‰ˆ0.85â€“0.96)** but **very low Classâ€¯1 recall (~0.13â€“0.37)**. Thatâ€™s because the dataset is dominated by Classâ€¯0, so a model that predicts negatives most of the time can look â€œgreatâ€ overall while **missing most hypos**. This is a classic classâ€‘imbalance effect.\n\nThatâ€™s why your **testBalanced** view is important: it reveals how well the model actually detects positives.\n\n---\n\n## Method & architecture comparison (balanced test)\n\nTop performer per **resampling method** (sorted by Weighted F1):\n\n1. **oversample_seq + LSTM_100, thrâ€¯0.50**\n\n   * Weighted F1 **0.801**, Acc 0.801, **Precisionâ‚ 0.956**, **Recallâ‚ 0.642**\n2. **undersample_seq + BiLSTM, thrâ€¯0.40**\n\n   * Weighted F1 0.792, Acc 0.792, Precisionâ‚ 0.860, **Recallâ‚ 0.687** *(best sensitivity among the top)*\n3. **none + LSTM_50, thrâ€¯0.50**\n\n   * Weighted F1 0.740, Acc 0.740, Precisionâ‚ 0.831, Recallâ‚ 0.642\n\n**Takeaway:**\n\n* **`LSTM_100`** is consistently the strongest backbone.\n* **Oversampling** improves **precision while retaining good recall**; **undersampling** nudges recall highest (but with more false alarms).\n* **No resampling** underperforms for the positive class.\n\n---\n\n## AUC perspective (thresholdâ€‘free)\n\n* For **oversample_seq + LSTM_100**:\n\n  * **ROCâ€‘AUC (original test)**: ~**0.886**\n  * **PRâ€‘AUC (original test)**: ~**0.336** (low due to class rarity; typical)\n  * **ROCâ€‘AUC (balanced test)**: ~**0.906**\n  * **PRâ€‘AUC (balanced test)**: ~**0.903** *(inflated by 50% prevalence; use for diagnostics only)*\n\nThe ROCâ€‘AUCs are stable and indicate a **strong ranking ability**. PRâ€‘AUC on the original test is more honest about the difficulty of the rare positives.\n\n---\n\n## Generalization check (same method/model/threshold across splits)\n\nFor **oversample_seq + LSTM_100 @ thrâ€¯0.50**:\n\n* **Train** Weighted F1 â‰ˆ **0.972**\n* **Val** Weighted F1 â‰ˆ **0.946**\n* **Test (original)** Weighted F1 â‰ˆ **0.950**\n* **TestBalanced** Weighted F1 â‰ˆ **0.801**\n\nThe drop on **testBalanced** is expected because the class prior is forced to 50/50; it does **not** indicate overfitting. Train/Val/Test are tightly aligned.\n\n---\n\n## Recommendations\n\n1. **Deploy default:** `oversample_seq + LSTM_100` with **thresholdâ€¯=â€¯0.50**\n\n   * Great precision on hypos (few false alarms) with reasonable sensitivity.\n\n2. **Sensitivity mode:** set **thresholdâ€¯=â€¯0.40**\n\n   * Use when **missing a hypo is costlier** than an extra false alert.\n\n3. **If you want even more recall**, consider `undersample_seq + BiLSTM @ thrâ€¯0.40` (Recallâ‚ â‰ˆ **0.687** on balanced test), but expect more false positives.\n\n4. **Calibrate to clinical preference:** You can choose threshold by optimizing **FÎ²** (e.g., Î²=2 for recallâ€‘heavy) on the **validation set**, then lock that threshold for the test/deployment.\n\n5. **Next steps to squeeze more recall without losing precision:**\n\n   * Try adding **pca_cgm2/3** and **hour_of_day** to sequence features.\n   * Small **temporal dropout/label smoothing** to stabilize.\n   * **Patientâ€‘grouped CV** to confirm robustness.\n   * **Threshold per risk period** (e.g., nocturnal vs daytime) using hourâ€‘ofâ€‘day.\n\n---\n\n### Quick reference (best configurations)\n\n* **Best balanced overall**: `oversample_seq / LSTM_100 / thr=0.50`\n  **Weighted F1 0.801 Â· Acc 0.801 Â· Precâ‚ 0.956 Â· Recâ‚ 0.642 Â· ROCâ€‘AUC 0.906 Â· PRâ€‘AUC 0.903**\n\n* **More recall**: `oversample_seq / LSTM_100 / thr=0.40`\n  **Weighted F1 0.799 Â· Acc 0.799 Â· Precâ‚ 0.935 Â· Recâ‚ 0.664**\n\n* **Highest recall among topâ€‘2**: `undersample_seq / BiLSTM / thr=0.40`\n  **Weighted F1 0.792 Â· Acc 0.792 Â· Precâ‚ 0.860 Â· Recâ‚ 0.687**\n\nIf you want, I can generate a compact leaderboard table (or plots) from this file showing the top N runs for each split and highlight the tradeâ€‘offs between precision and recall.\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# Results analysis & reporting\n# ============================\nimport os\nimport io\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom sklearn.metrics import (\n    confusion_matrix, roc_curve, precision_recall_curve, auc, average_precision_score\n)\n\nimport tensorflow as tf\n\n# ---- Rule thresholds from Table 29 (clinical screens) ----\nREQ_RECALL = 0.70    # sensitivity\nREQ_PRAUC  = 0.60    # minority-class discrimination\nMAX_BRIER  = 0.20    # probability calibration\n\ndef _round_or_none(x, nd=3):\n    import numpy as np\n    try: return round(float(x), nd)\n    except Exception: return np.nan\n\ndef _tag_range(v, cuts):\n    for thr, lab, direction in cuts:\n        if direction == \">=\" and v >= thr: return lab\n        if direction == \"<=\" and v <= thr: return lab\n    return \"â€”\"\n\ndef tag_recall(v): return _tag_range(v, [(0.60,\"High\",\">=\"),(0.40,\"Balanced\",\">=\")])\ndef tag_pr(v):     return _tag_range(v, [(0.90,\"Excellent\",\">=\"),(0.85,\"Strong\",\">=\"),(0.70,\"Acceptable\",\">=\")])\ndef tag_brier(v):  return _tag_range(v, [(0.02,\"Excellent cal\",\"<=\"),(0.05,\"Good\",\"<=\"),(0.10,\"Acceptable\",\"<=\"),(0.20,\"Clinical OK\",\"<=\")])\n\n\n# ---------- Configuration ----------\nRESULTS_CSV_CANDIDATES = [\n    \n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\nSEQ_NPZ_CANDIDATES = [\n    \"/kaggle/working/sequences_leakfree.npz\",\n    \"/mnt/data/sequences_leakfree.npz\",\n    \"sequences_leakfree.npz\"\n]\nCHECKPOINT_DIR = \"checkpoints\"  # expects files like: checkpoints/{Method}__{Model}.h5\n\n# Your canonical visit/static lists (for feature breakdown)\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n# Which methods to summarize\nMETHODS_FOR_TOP5 = [\"none\", \"oversample_seq\", \"undersample_seq\"]\n\n# ---------- Helpers ----------\ndef _first_existing(path_list):\n    for p in path_list:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"\n    Ensure df has Method/Model/Threshold/Split. If missing, try to parse from 'Key'.\n    \"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file is missing Method/Model/Threshold/Split and has no 'Key' column to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    # Threshold is typically in the 3rd token as 'thr_0.50'\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    # Split can be at the end of the Key; back off to explicit Split col if present\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"), \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", df.get(\"Split\", np.nan)))))\n    return df\n\ndef _round_or_none(x, nd=3):\n    try:\n        return round(float(x), nd)\n    except Exception:\n        return np.nan\n\ndef _safe_confmat_from_row(row: pd.Series):\n    \"\"\"\n    Reconstructs an integer confusion matrix from supports + recall/specificity metrics in the row.\n    Assumes:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N (specificity wrt positives decision among negatives)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n    sp1  = float(row.get(\"Class1/Specificity\", np.nan))\n\n    if any([not np.isfinite(v) for v in [s0, s1, rec1, sp1]]):\n        raise ValueError(\"Cannot reconstruct confusion matrix: missing supports/recall/specificity in row.\")\n\n    tp = int(round(rec1 * s1))\n    fn = max(0, s1 - tp)\n    tn = int(round(sp1 * s0))\n    fp = max(0, s0 - tn)\n\n    # Small adjustments to keep sums consistent\n    tn = max(0, min(tn, s0))\n    fp = s0 - tn\n    tp = max(0, min(tp, s1))\n    fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef focal_loss(gamma=8.0, alpha=0.25):\n    # For loading custom-loss models\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _try_load_sequences(npz_candidates):\n    p = _first_existing(npz_candidates)\n    if not p:\n        print(\"âš ï¸ sequences_leakfree.npz not found. Feature counts, shapes and ROC/PR will be limited.\")\n        return None\n    npz = np.load(p, allow_pickle=True)\n    # unpack with fallbacks\n    data = {\n        \"train\": {\"X_seq\": npz[\"Xtr\"], \"y\": npz[\"ytr\"]},\n        \"val\":   {\"X_seq\": npz[\"Xva\"], \"y\": npz[\"yva\"]},\n        \"test\":  {\"X_seq\": npz[\"Xte\"], \"y\": npz[\"yte\"]},\n        \"seq_features_used\": list(npz[\"seq_features_used\"].tolist()) if \"seq_features_used\" in npz.files else [],\n        \"static_features_used\": list(npz[\"static_features_used\"].tolist()) if \"static_features_used\" in npz.files else []\n    }\n    # Optional static inputs\n    if \"Xtr_stat\" in npz.files and npz[\"Xtr_stat\"].size > 0:\n        data[\"train\"][\"X_stat\"] = npz[\"Xtr_stat\"]\n    else:\n        data[\"train\"][\"X_stat\"] = None\n\n    if \"Xva_stat\" in npz.files and npz[\"Xva_stat\"].size > 0:\n        data[\"val\"][\"X_stat\"] = npz[\"Xva_stat\"]\n    else:\n        data[\"val\"][\"X_stat\"] = None\n\n    if \"Xte_stat\" in npz.files and npz[\"Xte_stat\"].size > 0:\n        data[\"test\"][\"X_stat\"] = npz[\"Xte_stat\"]\n    else:\n        data[\"test\"][\"X_stat\"] = None\n\n    data[\"npz_path\"] = p\n    return data\n\ndef _predict_loaded_model(model, X_seq, X_stat=None):\n    # Allow models with one or two inputs\n    try:\n        if isinstance(model.input, list) or len(model.inputs) > 1:\n            if X_stat is None:\n                raise ValueError(\"Model expects static input but none provided.\")\n            preds = model.predict([X_seq, X_stat], verbose=0).ravel()\n        else:\n            preds = model.predict(X_seq, verbose=0).ravel()\n        return preds\n    except Exception as e:\n        print(f\"âš ï¸ Prediction failed: {e}\")\n        return None\n\n# ---------- 1) Load results ----------\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results CSV. Please update RESULTS_CSV_CANDIDATES.\")\nprint(f\"ðŸ“„ Using results file: {res_path}\")\ndf = pd.read_csv(res_path)\ndf = _ensure_columns(df)\n\n# ---------- 1) Top-5 tables per method (by VAL F1_weighted) ----------\nwant_cols_map = {\n    \"Model\": \"Model\",\n    \"Split\": \"Split\",\n    \"Threshold\": \"Threshold\",\n    \"Accuracy\": \"Overall/Accuracy\",\n    \"Precision_weighted\": \"Overall/Precision_weighted\",\n    \"Recall_weighted\": \"Overall/Recall_weighted\",\n    \"F1_weighted\": \"Overall/F1_weighted\",\n    \"Precision_1\": \"Class1/Precision\",\n    \"Recall_1\": \"Class1/Recall\",\n    \"F1_1\": \"Class1/F1\",\n    \"Specificity_1\": \"Class1/Specificity\",\n    \"ROC_AUC\": \"Overall/ROC-AUC\",\n    \"PR_AUC\": \"Overall/PR-AUC\",\n    \"Brier\": \"Overall/MSE_prob\"\n}\n\ndf_val = df[df[\"Split\"].str.lower() == \"val\"].copy()\nall_top5 = []\nfor m in METHODS_FOR_TOP5:\n    sub = df_val[df_val[\"Method\"] == m].copy()\n    sub = sub.dropna(subset=[\"Overall/F1_weighted\"])\n    sub = sub.sort_values(\"Overall/F1_weighted\", ascending=False).head(5)\n    if sub.empty:\n        continue\n    out = pd.DataFrame({\n        k: sub[v].values if v in sub.columns else np.nan\n        for k, v in want_cols_map.items()\n    })\n    out.insert(0, \"Method\", m)\n    all_top5.append(out)\n\ntop5_df = pd.concat(all_top5, ignore_index=True) if all_top5 else pd.DataFrame(columns=[\"Method\"]+list(want_cols_map.keys()))\ntop5_df_rounded = top5_df.copy()\nfor c in [\"Threshold\",\"Accuracy\",\"Precision_weighted\",\"Recall_weighted\",\"F1_weighted\",\n          \"Precision_1\",\"Recall_1\",\"F1_1\",\"Specificity_1\",\"ROC_AUC\",\"PR_AUC\",\"Brier\"]:\n    if c in top5_df_rounded.columns:\n        top5_df_rounded[c] = top5_df_rounded[c].apply(lambda x: _round_or_none(x, 4))\n\nprint(\"\\n=== Top-5 by VAL F1_weighted for each method (none / oversample_seq / undersample_seq) ===\")\nprint(top5_df_rounded.to_string(index=False))\n\n# Save\ntop5_out_path = \"/kaggle/working/top5_summary_per_method.csv\" if os.path.exists(\"/kaggle/working\") else \"top5_summary_per_method.csv\"\ntop5_df_rounded.to_csv(top5_out_path, index=False)\nprint(f\"\\nðŸ’¾ Saved top-5 summary â†’ {top5_out_path}\")\n\n# ---------- 2) Confusion matrix for BEST VAL F1 model ----------\nif df_val.empty:\n    print(\"\\nâš ï¸ No validation rows found; cannot select best VAL F1 model.\")\nelse:\n    best_row = df_val.sort_values(\"Overall/F1_weighted\", ascending=False).iloc[0]\n    cm = _safe_confmat_from_row(best_row)\n    print(\"\\n=== Best VAL model (by F1_weighted) ===\")\n    print(f\"Method={best_row['Method']} | Model={best_row['Model']} | thr={best_row['Threshold']:.2f}\")\n    print(\"Confusion Matrix [VAL]:\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Plot & save PNG\n    fig, ax = plt.subplots(figsize=(4.5, 4))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(f\"Confusion Matrix (VAL)\\n{best_row['Method']} / {best_row['Model']} @ thr={best_row['Threshold']:.2f}\")\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    # text\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=12, color=(\"white\" if cm[i,j]>cm.max()/2 else \"black\"))\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    cm_png_path = \"/kaggle/working/confusion_matrix_best_val.png\" if os.path.exists(\"/kaggle/working\") else \"confusion_matrix_best_val.png\"\n    plt.savefig(cm_png_path, dpi=200)\n    plt.close(fig)\n    print(f\"ðŸ–¼ï¸ Saved confusion matrix PNG â†’ {cm_png_path}\")\n\n# ---------- 3) ROC/PR curves for best 5 (by VAL F1) ----------\n# We will try to load the checkpoints and the sequences NPZ.\nseq_data = _try_load_sequences(SEQ_NPZ_CANDIDATES)\nif seq_data is None:\n    print(\"\\nâš ï¸ Skipping ROC/PR (sequences NPZ not found).\")\nelse:\n    # pick top 5 by VAL F1 overall (unique Method/Model)\n    top5_overall = (df_val\n                    .dropna(subset=[\"Overall/F1_weighted\"])\n                    .sort_values(\"Overall/F1_weighted\", ascending=False))\n    # keep first occurrence per (Method, Model)\n    top5_overall = top5_overall.drop_duplicates(subset=[\"Method\",\"Model\"]).head(5)\n    if top5_overall.empty:\n        print(\"\\nâš ï¸ No candidates for ROC/PR plotting.\")\n    else:\n        roc_handles = []\n        pr_handles  = []\n        fig_roc, ax_roc = plt.subplots(figsize=(6.5,5))\n        fig_pr,  ax_pr  = plt.subplots(figsize=(6.5,5))\n\n        yte = seq_data[\"test\"][\"y\"].astype(int)\n        Xte = seq_data[\"test\"][\"X_seq\"]\n        Xte_stat = seq_data[\"test\"].get(\"X_stat\", None)\n\n        for _, r in top5_overall.iterrows():\n            meth, arch = r[\"Method\"], r[\"Model\"]\n            ckpt = os.path.join(CHECKPOINT_DIR, f\"{meth}__{arch}.h5\")\n            if not os.path.exists(ckpt):\n                print(f\"âš ï¸ Checkpoint not found for {meth}/{arch}: {ckpt} â€” skipping.\")\n                continue\n\n            try:\n                model = tf.keras.models.load_model(ckpt, custom_objects={\"loss\": focal_loss()}, compile=False)\n            except Exception as e:\n                print(f\"âš ï¸ Failed to load model {ckpt}: {e}\")\n                continue\n\n            y_prob = _predict_loaded_model(model, Xte, X_stat=Xte_stat)\n            if y_prob is None:\n                continue\n\n            # ROC\n            try:\n                fpr, tpr, _ = roc_curve(yte, y_prob)\n                roc_auc = auc(fpr, tpr)\n                ax_roc.plot(fpr, tpr, label=f'{meth}/{arch} (AUC={roc_auc:.3f})')\n            except Exception as e:\n                print(f\"âš ï¸ ROC failed for {meth}/{arch}: {e}\")\n\n            # PR\n            try:\n                prec, rec, _ = precision_recall_curve(yte, y_prob)\n                ap = average_precision_score(yte, y_prob)\n                ax_pr.plot(rec, prec, label=f'{meth}/{arch} (AP={ap:.3f})')\n            except Exception as e:\n                print(f\"âš ï¸ PR failed for {meth}/{arch}: {e}\")\n\n        # finalize ROC\n        ax_roc.plot([0,1],[0,1],'--', color='gray', label='Random')\n        ax_roc.set_title(\"ROC â€” Best 5 by VAL F1\")\n        ax_roc.set_xlabel(\"False Positive Rate\")\n        ax_roc.set_ylabel(\"True Positive Rate\")\n        ax_roc.legend(fontsize=8)\n        plt.tight_layout()\n        roc_png = \"/kaggle/working/best5_roc.png\" if os.path.exists(\"/kaggle/working\") else \"best5_roc.png\"\n        fig_roc.savefig(roc_png, dpi=250); plt.close(fig_roc)\n        print(f\"ðŸ–¼ï¸ Saved ROC curves â†’ {roc_png}\")\n\n        # finalize PR\n        ax_pr.set_title(\"Precisionâ€“Recall â€” Best 5 by VAL F1\")\n        ax_pr.set_xlabel(\"Recall\")\n        ax_pr.set_ylabel(\"Precision\")\n        ax_pr.legend(fontsize=8)\n        plt.tight_layout()\n        pr_png = \"/kaggle/working/best5_pr.png\" if os.path.exists(\"/kaggle/working\") else \"best5_pr.png\"\n        fig_pr.savefig(pr_png, dpi=250); plt.close(fig_pr)\n        print(f\"ðŸ–¼ï¸ Saved PR curves â†’ {pr_png}\")\n\n# ---------- 4) Feature counts (hourly vs visit vs static) ----------\nif seq_data is not None:\n    seq_feats = seq_data.get(\"seq_features_used\", []) or []\n    static_feats = seq_data.get(\"static_features_used\", []) or []\n    visit_used = [f for f in seq_feats if f in VISIT_COLS]\n    hourly_used = [f for f in seq_feats if f not in VISIT_COLS]\n\n    print(\"\\n=== Feature sets used (from NPZ) ===\")\n    print(f\"Hourly features after transform: {len(hourly_used)} â†’ {hourly_used}\")\n    print(f\"Static features after transform: {len(static_feats)} â†’ {static_feats}\")\n    print(f\"Visit features: {len(visit_used)} â†’ {visit_used}\")\nelse:\n    print(\"\\nâš ï¸ Feature counts unavailable (NPZ not found).\")\n\n# ---------- 5) Sequence shapes & class distribution ----------\ndef _shape_or_na(arr):\n    try:\n        return tuple(arr.shape)\n    except Exception:\n        return \"(NA)\"\n\ndef _dist_str(y):\n    if y is None or len(y)==0:\n        return \"{ } (pos=NA)\"\n    cnt = Counter(np.asarray(y).astype(int).tolist())\n    pos = cnt.get(1,0); tot = sum(cnt.values())\n    pct = 100.0*pos/max(1,tot)\n    return f\"{dict(cnt)} (pos={pct:.1f}%)\"\n\nif seq_data is not None:\n    trX, vaX, teX = seq_data[\"train\"][\"X_seq\"], seq_data[\"val\"][\"X_seq\"], seq_data[\"test\"][\"X_seq\"]\n    print(\"\\n=== Sequence shapes ===\")\n    print(f\"Train seq: {_shape_or_na(trX)}\")\n    print(f\"Val   seq: {_shape_or_na(vaX)}\")\n    print(f\"Test  seq: {_shape_or_na(teX)}\")\n\n    print(\"\\n=== Class distribution ===\")\n    print(f\"ðŸ”Ž Train sequences: {_dist_str(seq_data['train']['y'])}\")\n    print(f\"ðŸ”Ž Val sequences:   {_dist_str(seq_data['val']['y'])}\")\n    print(f\"ðŸ”Ž Test sequences:  {_dist_str(seq_data['test']['y'])}\")\nelse:\n    print(\"\\nâš ï¸ Sequence shapes & class distribution unavailable (NPZ not found).\")\n\n# ---------- 6) Print structure of the BEST model ----------\nif df_val.empty:\n    print(\"\\nâš ï¸ No validation rows â†’ cannot determine best model for summary.\")\nelse:\n    best_row = df_val.sort_values(\"Overall/F1_weighted\", ascending=False).iloc[0]\n    meth, arch = best_row[\"Method\"], best_row[\"Model\"]\n    ckpt = os.path.join(CHECKPOINT_DIR, f\"{meth}__{arch}.h5\")\n    if not os.path.exists(ckpt):\n        print(f\"\\nâš ï¸ Best model checkpoint not found: {ckpt}\")\n    else:\n        try:\n            model = tf.keras.models.load_model(ckpt, custom_objects={\"loss\": focal_loss()}, compile=False)\n            s = io.StringIO()\n            model.summary(print_fn=lambda x: s.write(x + \"\\n\"))\n            summary_text = s.getvalue()\n            print(\"\\n=== Best model structure (Keras summary) ===\")\n            print(summary_text)\n            # Save to file\n            summary_path = \"/kaggle/working/best_model_summary.txt\" if os.path.exists(\"/kaggle/working\") else \"best_model_summary.txt\"\n            with open(summary_path, \"w\") as f:\n                f.write(summary_text)\n            print(f\"ðŸ’¾ Saved best model summary â†’ {summary_path}\")\n        except Exception as e:\n            print(f\"\\nâš ï¸ Failed to load/print model summary: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:59:19.120945Z","iopub.execute_input":"2025-10-18T08:59:19.121502Z","iopub.status.idle":"2025-10-18T08:59:32.588905Z","shell.execute_reply.started":"2025-10-18T08:59:19.12147Z","shell.execute_reply":"2025-10-18T08:59:32.587999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================================\n# Extra: fixed-confusion-matrix PNGs for\n#  - oversample_seq + LSTM_100 @ thr 0.50\n#  - undersample_seq + BiLSTM  @ thr 0.40\n#  - none          + LSTM_50   @ thr 0.50\n# ===========================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ---- where to find the results file ----\nRESULTS_CSV_CANDIDATES = [\n    \"/mnt/data/results_summary_all (9).csv\",\n    \"/mnt/data/results_summary_all.csv\",\n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\n\ndef _first_existing(paths):\n    for p in paths:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"Make sure Method/Model/Threshold/Split are present (parse from Key if needed).\"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file missing Method/Model/Threshold/Split and no 'Key' to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"),   \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", np.nan))))\n    return df\n\ndef _confmat_from_row(row: pd.Series):\n    \"\"\"\n    Build confusion matrix from supports + recall/spec recorded in results CSV:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N  (fallback to Class0/Recall if needed)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n\n    sp1 = row.get(\"Class1/Specificity\", np.nan)\n    if not np.isfinite(sp1):\n        # fallback: recall for class 0 equals specificity wrt class 1\n        sp1 = float(row.get(\"Class0/Recall\", np.nan))\n\n    if not all(np.isfinite([s0, s1, rec1, sp1])):\n        raise ValueError(\"Row lacks required metrics to reconstruct confusion matrix.\")\n\n    tp = int(round(rec1 * s1))\n    fn = s1 - tp\n    tn = int(round(sp1 * s0))\n    fp = s0 - tn\n\n    # clamp for safety\n    tn = max(0, min(tn, s0)); fp = s0 - tn\n    tp = max(0, min(tp, s1)); fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef _plot_cm(cm: np.ndarray, title: str, out_path: str):\n    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v),\n                ha=\"center\", va=\"center\",\n                color=(\"white\" if v > cm.max()/2 else \"black\"),\n                fontsize=12)\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=220)\n    plt.close(fig)\n\n# ---- load results ----\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results_summary_all*.csv â€” update RESULTS_CSV_CANDIDATES.\")\nprint(f\"ðŸ“„ Using results file: {res_path}\")\nres = pd.read_csv(res_path)\nres = _ensure_columns(res)\n\n# We'll take the 'test' split by default (change SPLIT to \"val\" if you want VAL instead)\nSPLIT = \"test\"\n\n# ---- combos you requested ----\nrequests = [\n    (\"oversample_seq\", \"LSTM_100\", 0.50),\n    (\"undersample_seq\", \"BiLSTM\",  0.40),\n    (\"none\",           \"LSTM_50\",  0.50),\n]\n\n# ---- make outputs dir if on Kaggle ----\nout_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\"\noutputs = []\n\nfor method, model, thr in requests:\n    # filter rows (case-insensitive Split; Threshold rounded to 2dp in pipeline)\n    sub = res[\n        (res[\"Method\"] == method) &\n        (res[\"Model\"]  == model)  &\n        (res[\"Split\"].astype(str).str.lower() == SPLIT.lower())\n    ].copy()\n\n    if sub.empty:\n        print(f\"âš ï¸ No rows for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # match threshold to 2 decimals\n    # accept both exact and near-equal due to float rounding\n    def _thr_match(x):\n        try:\n            return (round(float(x), 2) == round(float(thr), 2)) or (abs(float(x) - float(thr)) < 1e-6)\n        except Exception:\n            return False\n\n    sub = sub[sub[\"Threshold\"].apply(_thr_match)]\n    if sub.empty:\n        print(f\"âš ï¸ No threshold={thr:.2f} row for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # If multiple rows (rare), take the first\n    row = sub.iloc[0]\n    cm = _confmat_from_row(row)\n\n    # Print nicely\n    print(f\"\\n=== Confusion Matrix â€” {method} + {model} @ thr={thr:.2f} [{SPLIT}] ===\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Save PNG\n    safe_method = method.replace(\"/\",\"-\")\n    safe_model  = model.replace(\"/\",\"-\")\n    out_png = os.path.join(out_dir, f\"cm_{safe_method}__{safe_model}__thr_{thr:.2f}__{SPLIT}.png\")\n    _plot_cm(cm, f\"{method} + {model} @ thr={thr:.2f} [{SPLIT}]\", out_png)\n    outputs.append(out_png)\n\nprint(\"\\nðŸ–¼ï¸ Saved confusion matrix images:\")\nfor p in outputs:\n    print(\" -\", p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T09:43:08.635081Z","iopub.execute_input":"2025-10-18T09:43:08.635547Z","iopub.status.idle":"2025-10-18T09:43:08.666415Z","shell.execute_reply.started":"2025-10-18T09:43:08.635524Z","shell.execute_reply":"2025-10-18T09:43:08.665256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  fixed-confusion-matrix PNGs","metadata":{}},{"cell_type":"code","source":"# ===========================================\n# Extra: fixed-confusion-matrix PNGs for\n#  - oversample_seq + LSTM_100 @ thr 0.50\n#  - undersample_seq + BiLSTM  @ thr 0.40\n#  - none          + LSTM_50   @ thr 0.50\n# ===========================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ---- where to find the results file ----\nRESULTS_CSV_CANDIDATES = [\n    \"/mnt/data/results_summary_all (9).csv\",\n    \"/mnt/data/results_summary_all.csv\",\n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\n\ndef _first_existing(paths):\n    for p in paths:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"Make sure Method/Model/Threshold/Split are present (parse from Key if needed).\"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file missing Method/Model/Threshold/Split and no 'Key' to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"),   \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", np.nan))))\n    return df\n\ndef _confmat_from_row(row: pd.Series):\n    \"\"\"\n    Build confusion matrix from supports + recall/spec recorded in results CSV:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N  (fallback to Class0/Recall if needed)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n\n    sp1 = row.get(\"Class1/Specificity\", np.nan)\n    if not np.isfinite(sp1):\n        # fallback: recall for class 0 equals specificity wrt class 1\n        sp1 = float(row.get(\"Class0/Recall\", np.nan))\n\n    if not all(np.isfinite([s0, s1, rec1, sp1])):\n        raise ValueError(\"Row lacks required metrics to reconstruct confusion matrix.\")\n\n    tp = int(round(rec1 * s1))\n    fn = s1 - tp\n    tn = int(round(sp1 * s0))\n    fp = s0 - tn\n\n    # clamp for safety\n    tn = max(0, min(tn, s0)); fp = s0 - tn\n    tp = max(0, min(tp, s1)); fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef _plot_cm(cm: np.ndarray, title: str, out_path: str):\n    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v),\n                ha=\"center\", va=\"center\",\n                color=(\"white\" if v > cm.max()/2 else \"black\"),\n                fontsize=12)\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=220)\n    plt.close(fig)\n\n# ---- load results ----\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results_summary_all*.csv â€” update RESULTS_CSV_CANDIDATES.\")\nprint(f\"ðŸ“„ Using results file: {res_path}\")\nres = pd.read_csv(res_path)\nres = _ensure_columns(res)\n\n# We'll take the 'test' split by default (change SPLIT to \"val\" if you want VAL instead)\nSPLIT = \"test\"\n\n# ---- combos you requested ----\nrequests = [\n    (\"oversample_seq\", \"LSTM_100\", 0.50),\n    (\"undersample_seq\", \"BiLSTM\",  0.40),\n    (\"none\",           \"LSTM_50\",  0.50),\n]\n\n# ---- make outputs dir if on Kaggle ----\nout_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\"\noutputs = []\n\nfor method, model, thr in requests:\n    # filter rows (case-insensitive Split; Threshold rounded to 2dp in pipeline)\n    sub = res[\n        (res[\"Method\"] == method) &\n        (res[\"Model\"]  == model)  &\n        (res[\"Split\"].astype(str).str.lower() == SPLIT.lower())\n    ].copy()\n\n    if sub.empty:\n        print(f\"âš ï¸ No rows for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # match threshold to 2 decimals\n    # accept both exact and near-equal due to float rounding\n    def _thr_match(x):\n        try:\n            return (round(float(x), 2) == round(float(thr), 2)) or (abs(float(x) - float(thr)) < 1e-6)\n        except Exception:\n            return False\n\n    sub = sub[sub[\"Threshold\"].apply(_thr_match)]\n    if sub.empty:\n        print(f\"âš ï¸ No threshold={thr:.2f} row for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # If multiple rows (rare), take the first\n    row = sub.iloc[0]\n    cm = _confmat_from_row(row)\n\n    # Print nicely\n    print(f\"\\n=== Confusion Matrix â€” {method} + {model} @ thr={thr:.2f} [{SPLIT}] ===\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Save PNG\n    safe_method = method.replace(\"/\",\"-\")\n    safe_model  = model.replace(\"/\",\"-\")\n    out_png = os.path.join(out_dir, f\"cm_{safe_method}__{safe_model}__thr_{thr:.2f}__{SPLIT}.png\")\n    _plot_cm(cm, f\"{method} + {model} @ thr={thr:.2f} [{SPLIT}]\", out_png)\n    outputs.append(out_png)\n\nprint(\"\\nðŸ–¼ï¸ Saved confusion matrix images:\")\nfor p in outputs:\n    print(\" -\", p)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Shape \n### Fixed analysis + sequence utilities (single-source, de-duplicated)\n### - build_sequences_by_split   (scales using actual X.shape, not SEQ_LEN)\n### - _build_sequence_index_map  (one canonical copy)\n### - compute_visit_shap         (robust, seq_len inferred if omitted)\n### - aggregate_hourly_to_daily_risk (no hidden deps; direct model.predict)\n### - make_balanced_test         (shape-safe)","metadata":{}},{"cell_type":"code","source":"# ======================================================\n# Fixed analysis + sequence utilities + RUNNER\n#  - build_sequences_by_split   (scales using actual X.shape, not SEQ_LEN)\n#  - _build_sequence_index_map  (one canonical copy)\n#  - compute_visit_shap         (robust; seq_len inferred if omitted)\n#  - aggregate_hourly_to_daily_risk (direct model.predict; no hidden deps)\n#  - make_balanced_test         (shape-safe)\n#  - run_analyses_all           (calls everything and prints outputs)\n#  - quick model fit if none    (small LSTM; early stopping)\n# ======================================================\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Optional TF imports only used in the quick-fit path\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# ---------------------------\n# Global guards / safe fallbacks\n# ---------------------------\ndef _bool_env(name, default=False):\n    v = os.environ.get(name)\n    if v is None: return default\n    return str(v).strip().lower() in {\"1\",\"true\",\"yes\",\"y\",\"on\"}\n\ntry:\n    USE_STATIC_INPUT\nexcept NameError:\n    USE_STATIC_INPUT = True\n\ntry:\n    RANDOM_STATE\nexcept NameError:\n    RANDOM_STATE = 42\n\ntry:\n    DEFAULT_SEQ_FEATURE_COLS\nexcept NameError:\n    DEFAULT_SEQ_FEATURE_COLS = (\n        \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n        \"pc1_activity_energy\",\n        \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n    )\n\ntry:\n    STATIC_COLS\nexcept NameError:\n    STATIC_COLS = [\n        \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n        \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n    ]\n\ntry:\n    VISIT_COLS\nexcept NameError:\n    VISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\n\n# Analysis defaults\nVISIT_FEATURES       = VISIT_COLS\nSHAP_BACKGROUND_SIZE = 512\nSHAP_TEST_SAMPLES    = 1024\nRISK_THRESHOLD       = 0.50\n\ndef _check_globals():\n    \"\"\"No-op guard used by helpers that previously referenced external globals.\"\"\"\n    pass\n\n# ======================================================\n# build_sequences_by_split (uses actual X.shape[1] when reshaping)\n# ======================================================\ndef build_sequences_by_split(\n    hourly: pd.DataFrame,\n    splits,\n    seq_len: int,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    scale_features: bool = True\n):\n    \"\"\"\n    Build (X_seq, X_stat, y) arrays for train/val/test given hourly data and patient splits.\n    - Scalers are fit on TRAIN only.\n    - Uses actual window length (T) from arrays when reshaping; no reliance on global SEQ_LEN.\n    \"\"\"\n    # Required columns\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly = hourly.copy()\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    # Sequence features presence\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    # Static matrix per patient (fill zeros for missing patients)\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float)\n                      .fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        sub = sub.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len])\n                y.append(labels[i+seq_len])\n                if USE_STATIC_INPUT and static_mat is not None:\n                    if pid in static_mat.index:\n                        X_stat.append(static_mat.loc[pid].values.astype(float))\n                    else:\n                        X_stat.append(np.zeros(len(static_cols_present), dtype=float))\n        X_seq = np.array(X_seq)\n        y     = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (USE_STATIC_INPUT and static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    # -------- Scale on TRAIN only (use actual T, F) --------\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s is not None and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size == 0:\n                return X\n            n, T, F = X.shape\n            return seq_scaler.transform(X.reshape(-1, F)).reshape(n, T, F)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size > 0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size == 0:\n                return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"âœ… Sequences built | train={getattr(Xtr_s,'shape',None)}, val={getattr(Xva_s,'shape',None)}, test={getattr(Xte_s,'shape',None)}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ======================================================\n# Sequence index map (single canonical copy)\n# ======================================================\ndef _build_sequence_index_map(hourly_df: pd.DataFrame, split: str, seq_len: int) -> pd.DataFrame:\n    \"\"\"\n    Recreate the exact sequence ordering used in build_sequences_by_split so that\n    index i maps to the (i+seq_len)-th hour row for each patient.\n\n    Returns: ['seq_idx','patientID','hour','date','visit_assigned','period_main','row_idx']\n    \"\"\"\n    sub = (hourly_df[hourly_df[\"Split\"].astype(str).str.lower() == split.lower()]\n           .sort_values([\"patientID\",\"hour\"])\n           .reset_index())\n    sub = sub.rename(columns={\"index\": \"row_idx\"})\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\n                \"seq_idx\": len(rows),\n                \"patientID\": pid,\n                \"hour\": pd.to_datetime(tgt[\"hour\"]),\n                \"date\": pd.to_datetime(tgt.get(\"date\", pd.NaT)),\n                \"visit_assigned\": tgt.get(\"visit_assigned\", np.nan),\n                \"period_main\": tgt.get(\"period_main\", np.nan),\n                \"row_idx\": int(tgt[\"row_idx\"]),\n            })\n    return pd.DataFrame(rows)\n\n# ======================================================\n# SHAP on visit features (robust; seq_len inference; shape checks)\n# ======================================================\ndef compute_visit_shap(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    seq_features_used,\n    visit_features=None,\n    split: str = \"test\",\n    background_size: int = SHAP_BACKGROUND_SIZE,\n    max_test_windows: int = SHAP_TEST_SAMPLES,\n    out_dir: str = \"/kaggle/working\",\n    seq_len: int = None\n):\n    \"\"\"\n    Computes global and per-visit SHAP importance for visit features that are INCLUDED\n    in the sequence feature tensor. Saves two CSVs:\n      - shap_visit_global.csv\n      - shap_visit_per_visit.csv\n    \"\"\"\n    _check_globals()\n    os.makedirs(out_dir, exist_ok=True)\n    visit_features = list(visit_features) if visit_features is not None else list(VISIT_FEATURES)\n\n    # Infer seq_len from arrays if not provided\n    Xte = data[split][\"X_seq\"]\n    if seq_len is None:\n        if Xte is None or Xte.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len from data arrays.\")\n        seq_len = Xte.shape[1]\n\n    # Map visit features -> indices in the sequence features\n    feat_to_idx = {f: i for i, f in enumerate(seq_features_used)}\n    visit_in_seq = [f for f in visit_features if f in feat_to_idx]\n    if not visit_in_seq:\n        raise ValueError(\n            f\"None of the visit features are present in seq_features_used={seq_features_used}. \"\n            f\"Ensure your DEFAULT_SEQ_FEATURE_COLS includes items from VISIT_COLS.\"\n        )\n\n    Xtr, Xtr_stat = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"]\n    Xte, Xte_stat = data[split][\"X_seq\"],  data[split][\"X_stat\"]\n\n    bg_n = min(int(background_size), len(Xtr))\n    te_n = min(int(max_test_windows), len(Xte))\n    if bg_n < 1 or te_n < 1:\n        raise RuntimeError(f\"Not enough sequences for SHAP (bg_n={bg_n}, te_n={te_n}).\")\n\n    rng   = np.random.default_rng(42)\n    bg_ix = rng.choice(np.arange(len(Xtr)), size=bg_n, replace=False)\n    te_ix = rng.choice(np.arange(len(Xte)), size=te_n, replace=False)\n\n    bg_seq, te_seq = Xtr[bg_ix], Xte[te_ix]\n    if USE_STATIC_INPUT and (Xtr_stat is not None and Xte_stat is not None and Xtr_stat.size>0 and Xte_stat.size>0):\n        bg_static, te_static = Xtr_stat[bg_ix], Xte_stat[te_ix]\n    else:\n        bg_static = te_static = None\n\n    # ---- SHAP explainer\n    try:\n        import shap\n    except Exception as e:\n        raise ImportError(\n            \"This SHAP analysis requires the 'shap' package. Install with: pip install shap\"\n        ) from e\n\n    try:\n        if bg_static is not None:\n            explainer   = shap.DeepExplainer(model, [bg_seq, bg_static])\n            shap_values = explainer.shap_values([te_seq, te_static])\n        else:\n            explainer   = shap.DeepExplainer(model, bg_seq)\n            shap_values = explainer.shap_values(te_seq)\n    except Exception as e:\n        print(f\"[WARN] DeepExplainer failed ({e}). Falling back to GradientExplainerâ€¦\")\n        if bg_static is not None:\n            explainer   = shap.GradientExplainer(model, [bg_seq, bg_static])\n            shap_values = explainer.shap_values([te_seq, te_static])\n        else:\n            explainer   = shap.GradientExplainer(model, bg_seq)\n            shap_values = explainer.shap_values(te_seq)\n\n    shap_seq = shap_values[0] if isinstance(shap_values, list) else shap_values\n    if shap_seq.ndim != 3:\n        raise RuntimeError(f\"Unexpected SHAP shape for sequence input: {shap_seq.shape}\")\n\n    # Reduce over time â†’ mean |SHAP| across the window\n    shap_abs_time = np.mean(np.abs(shap_seq), axis=1)  # [n_samples, F]\n\n    # ---- GLOBAL visit feature importance\n    rows = [{\"feature\": f, \"mean_abs_shap\": float(np.mean(shap_abs_time[:, feat_to_idx[f]]))}\n            for f in visit_in_seq]\n    global_visit_df = pd.DataFrame(rows).sort_values(\"mean_abs_shap\", ascending=False)\n    gpath = os.path.join(out_dir, \"shap_visit_global.csv\")\n    global_visit_df.to_csv(gpath, index=False)\n    print(\"âœ… Saved global visit SHAP â†’\", gpath)\n\n    # ---- PERâ€‘VISIT importance\n    seq_map = _build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(Xte):\n        raise RuntimeError(\n            f\"Mapping length {len(seq_map)} != X_{split} length {len(Xte)}. \"\n            f\"seq_len={seq_len}. Rebuild hourly/sequences consistently.\"\n        )\n    seq_map_sub = seq_map.iloc[te_ix].reset_index(drop=True)\n\n    per_rows = []\n    for i in range(len(seq_map_sub)):\n        pid = seq_map_sub.loc[i, \"patientID\"]\n        dte = pd.to_datetime(seq_map_sub.loc[i, \"date\"])\n        for f in visit_in_seq:\n            per_rows.append({\n                \"patientID\": pid,\n                \"date\": dte,\n                \"feature\": f,\n                \"mean_abs_shap\": float(shap_abs_time[i, feat_to_idx[f]])\n            })\n    per_visit_df = (pd.DataFrame(per_rows)\n                    .groupby([\"patientID\",\"date\",\"feature\"], as_index=False)[\"mean_abs_shap\"].mean()\n                    .sort_values([\"patientID\",\"date\",\"mean_abs_shap\"], ascending=[True, True, False]))\n    ppath = os.path.join(out_dir, \"shap_visit_per_visit.csv\")\n    per_visit_df.to_csv(ppath, index=False)\n    print(\"âœ… Saved perâ€‘visit SHAP â†’\", ppath)\n\n    return global_visit_df, per_visit_df\n\n# ======================================================\n# Aggregate hourly predictions to daily risk\n# ======================================================\ndef aggregate_hourly_to_daily_risk(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    split: str = \"test\",\n    threshold: float = RISK_THRESHOLD,\n    out_dir: str = \"/kaggle/working\",\n    seq_len: int = None\n):\n    \"\"\"\n    Aggregate sequenceâ€‘window predictions to daily risk summaries.\n    Saves 'daily_risk_<split>.csv' and (best effort) a small example plot.\n    \"\"\"\n    os.makedirs(out_dir, exist_ok=True)\n\n    # Predictions\n    Xs      = data[split][\"X_seq\"]\n    Xs_stat = data[split][\"X_stat\"]\n    y_true  = np.asarray(data[split][\"y\"]).astype(int).ravel()\n\n    if seq_len is None:\n        if Xs is None or Xs.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len from data arrays.\")\n        seq_len = Xs.shape[1]\n\n    if USE_STATIC_INPUT and (Xs_stat is not None and Xs_stat.size>0):\n        y_prob = model.predict([Xs, Xs_stat], verbose=0).ravel()\n    else:\n        y_prob = model.predict(Xs, verbose=0).ravel()\n    y_pred = (y_prob >= float(threshold)).astype(int)\n\n    # Map sequence rows back to hours/dates\n    seq_map = _build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(y_true):\n        raise RuntimeError(\n            f\"Sequence map length {len(seq_map)} != prediction length {len(y_true)}.\\n\"\n            f\"seq_len={seq_len}, X_{split}.shape={getattr(Xs, 'shape', None)}. \"\n            \"Use the same hourly/sequences and seq_len that were used for training.\"\n        )\n\n    pred_df = pd.DataFrame({\n        \"patientID\": seq_map[\"patientID\"].values,\n        \"hour\":      pd.to_datetime(seq_map[\"hour\"].values),\n        \"date\":      pd.to_datetime(seq_map[\"date\"].values),\n        \"visit_assigned\": seq_map.get(\"visit_assigned\", pd.Series([np.nan]*len(seq_map))).values,\n        \"period_main\":    seq_map.get(\"period_main\", pd.Series([np.nan]*len(seq_map))).values,\n        \"y_true\":    y_true,\n        \"y_prob\":    y_prob,\n        \"y_pred\":    y_pred\n    })\n\n    # Daily aggregates\n    grp = pred_df.groupby([\"patientID\",\"date\"], as_index=False)\n    daily = grp.agg(\n        n_windows=(\"y_true\",\"size\"),\n        true_positives=(\"y_true\",\"sum\"),\n        pred_positives=(\"y_pred\",\"sum\"),\n        risk_mean=(\"y_prob\",\"mean\"),\n        risk_max=(\"y_prob\",\"max\"),\n        risk_p95=(\"y_prob\", lambda x: float(np.quantile(x, 0.95))),\n        hours_above_thr=(\"y_pred\",\"sum\")\n    )\n    daily[\"prevalence\"] = daily[\"true_positives\"] / daily[\"n_windows\"].replace(0, np.nan)\n    daily_csv = os.path.join(out_dir, f\"daily_risk_{split}.csv\")\n    daily.to_csv(daily_csv, index=False)\n    print(f\"âœ… Saved daily risk aggregates â†’ {daily_csv}\")\n\n    # Optional quick plot\n    try:\n        example_pid = daily[\"patientID\"].iloc[0]\n        dsub = daily[daily[\"patientID\"] == example_pid].sort_values(\"date\")\n        plt.figure(figsize=(8,3))\n        plt.plot(dsub[\"date\"], dsub[\"risk_mean\"], label=\"Mean daily risk\")\n        plt.plot(dsub[\"date\"], dsub[\"risk_max\"],  label=\"Max daily risk\")\n        plt.axhline(threshold, linestyle=\"--\", label=f\"thr={threshold:.2f}\")\n        plt.xlabel(\"Date\"); plt.ylabel(\"Risk\"); plt.title(f\"Daily risk â€” patient {example_pid}\")\n        plt.legend(); plt.tight_layout()\n        png = os.path.join(out_dir, f\"daily_risk_trend_patient_{example_pid}.png\")\n        plt.savefig(png, dpi=200); plt.close()\n        print(f\"ðŸ–¼ï¸ Saved example daily trend â†’ {png}\")\n    except Exception as e:\n        print(f\"[WARN] Could not plot daily trend example: {e}\")\n\n    return pred_df, daily\n\n# ======================================================\n# Balanced test subset (shape-safe)\n# ======================================================\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state: int = RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0:\n        return (X_test, y_test, (None if X_stat is None else np.asarray(X_stat)))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ======================================================\n# ---------- Helper: tiny LSTM builder for quick run ----------\n# ======================================================\ndef _make_quick_model(seq_len, n_seq_f, n_stat_f=0, lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = LSTM(64, return_sequences=True)(seq_in)\n    x = Dropout(0.2)(x)\n    x = LSTM(32)(x)\n    x = Dropout(0.2)(x)\n    x = Dense(16, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(16, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(16, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(16, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# ======================================================\n# ---------- Helper: load sequences from NPZ ----------\n# ======================================================\ndef _load_sequences_npz(npz_path):\n    d = np.load(npz_path, allow_pickle=True)\n    def _arr(name):\n        return None if name not in d or d[name].size==0 else d[name]\n    data = {\n        \"train\": {\"X_seq\": _arr(\"Xtr\"), \"X_stat\": _arr(\"Xtr_stat\"), \"y\": d[\"ytr\"]},\n        \"val\":   {\"X_seq\": _arr(\"Xva\"), \"X_stat\": _arr(\"Xva_stat\"), \"y\": d[\"yva\"]},\n        \"test\":  {\"X_seq\": _arr(\"Xte\"), \"X_stat\": _arr(\"Xte_stat\"), \"y\": d[\"yte\"]},\n        \"seq_features_used\": list(d[\"seq_features_used\"]),\n        \"static_features_used\": list(d[\"static_features_used\"]) if \"static_features_used\" in d else []\n    }\n    return data\n\n# ======================================================\n# ---------- Helper: find files & build splits ----------\n# ======================================================\ndef _first_existing(paths):\n    for p in paths:\n        if p and os.path.exists(p):\n            return p\n    return None\n\ndef _splits_from_hourly(hourly: pd.DataFrame):\n    train_p = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"train\",\"patientID\"].unique()))\n    val_p   = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"val\",\"patientID\"].unique()))\n    test_p  = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"test\",\"patientID\"].unique()))\n    if len(train_p)==0 or len(val_p)==0 or len(test_p)==0:\n        raise RuntimeError(\"hourly must include a 'Split' column with 'train'/'val'/'test' assignments.\")\n    return (train_p, val_p, test_p)\n\n# ======================================================\n# --------------------- RUNNER -------------------------\n# ======================================================\ndef run_analyses_all(model=None, data=None, hourly=None,\n                     split=\"test\", out_dir=None, seq_len_default=24,\n                     do_shap=True, train_epochs=6):\n    \"\"\"\n    If model/data/hourly are not supplied, this runner tries to:\n    - load sequences from NPZ (sequences_leakfree.npz)\n    - load hourly CSV (dynamic_hourly_features_ramadan.csv)\n    - fit a small LSTM quickly\n    Then it computes:\n    - SHAP CSVs (if 'shap' is installed and do_shap=True)\n    - Daily risk CSV + a small PNG\n    \"\"\"\n    # --- output dir\n    if out_dir is None:\n        if os.path.exists(\"/kaggle/working\"): out_dir = \"/kaggle/working\"\n        else:\n            out_dir = os.path.join(\".\", \"outputs\")\n            os.makedirs(out_dir, exist_ok=True)\n\n    # --- locate files if needed\n    if hourly is None:\n        HOURLY_CANDIDATES = [\n            \"/kaggle/working/dynamic_hourly_features_ramadan.csv\",\n            \"/mnt/data/dynamic_hourly_features_ramadan.csv\",\n            \"/kaggle/input/hmc-model-static-variables/dynamic_hourly_features_ramadan.csv\"\n        ]\n        hp = _first_existing(HOURLY_CANDIDATES)\n        if not hp:\n            raise FileNotFoundError(\"Could not find hourly CSV. Please pass 'hourly' DataFrame.\")\n        hourly = pd.read_csv(hp)\n        print(f\"ðŸ“„ Loaded hourly table: {hp} | shape={hourly.shape}\")\n\n    if data is None:\n        NPZ_CANDIDATES = [\n            \"/kaggle/working/sequences_leakfree.npz\",\n            \"/kaggle/working/outputs/sequences_leakfree.npz\",\n            \"/mnt/data/sequences_leakfree.npz\"\n        ]\n        npz = _first_existing(NPZ_CANDIDATES)\n        if npz:\n            data = _load_sequences_npz(npz)\n            print(f\"ðŸ“¦ Loaded sequences NPZ: {npz}\")\n        else:\n            # rebuild sequences from hourly\n            splits = _splits_from_hourly(hourly)\n            data = build_sequences_by_split(hourly, splits, seq_len=seq_len_default,\n                                            seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n                                            static_cols=STATIC_COLS, scale_features=True)\n\n    # --- fit quick model if none provided\n    if model is None:\n        Xtr, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"y\"]\n        Xva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"y\"]\n        Xtr_stat = data[\"train\"][\"X_stat\"]; Xva_stat = data[\"val\"][\"X_stat\"]\n        seq_len  = Xtr.shape[1]; n_seq_f = Xtr.shape[2]\n        n_stat_f = 0 if (Xtr_stat is None or not USE_STATIC_INPUT) else Xtr_stat.shape[1]\n\n        model = _make_quick_model(seq_len, n_seq_f, n_stat_f=n_stat_f, lr=1e-3)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xtr, Xtr_stat], ytr, validation_data=([Xva, Xva_stat], yva),\n                      epochs=train_epochs, batch_size=64, callbacks=[es], verbose=1)\n        else:\n            model.fit(Xtr, ytr, validation_data=(Xva, yva),\n                      epochs=train_epochs, batch_size=64, callbacks=[es], verbose=1)\n        print(\"âœ… Model trained (quick fit).\")\n\n    # --- SHAP (optional)\n    if do_shap:\n        try:\n            _ = compute_visit_shap(model, data, hourly, data[\"seq_features_used\"],\n                                   visit_features=VISIT_FEATURES, split=split, out_dir=out_dir)\n        except ImportError as e:\n            print(f\"âš ï¸ SHAP not available; skipping. ({e})\")\n        except Exception as e:\n            print(f\"âš ï¸ SHAP step skipped due to error: {e}\")\n\n    # --- Daily risk aggregation\n    pred_df, daily = aggregate_hourly_to_daily_risk(model, data, hourly,\n                                                    split=split, threshold=RISK_THRESHOLD,\n                                                    out_dir=out_dir)\n    # Print small previews so you \"see output\"\n    with pd.option_context(\"display.width\", 160, \"display.max_columns\", 20):\n        print(\"\\nðŸ”Ž Predictions (head):\")\n        print(pred_df.head(8).to_string(index=False))\n        print(\"\\nðŸ“Š Daily risk (head):\")\n        print(daily.head(8).to_string(index=False))\n\n    print(\"\\nðŸŽ¯ Done. Outputs saved under:\", out_dir)\n    return model, data, hourly\n\n# ======================================================\n# Example: run automatically if this cell/file is executed\n# (You can comment this out if you already have model/data/hourly in memory.)\n# ======================================================\nif __name__ == \"__main__\":\n    _ = run_analyses_all(\n        model=None,          # set to your trained model object to skip quick fit\n        data=None,           # set to your 'data' dict to reuse existing arrays\n        hourly=None,         # set to your hourly DataFrame if already loaded\n        split=\"test\",\n        out_dir=None,        # default (/kaggle/working or ./outputs)\n        seq_len_default=24,  # used only if rebuilding sequences from hourly\n        do_shap=True,        # requires 'pip install shap'\n        train_epochs=6       # small quick fit; adjust as you like\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndr = pd.read_csv(\"/kaggle/working/daily_risk_test.csv\", parse_dates=[\"date\"])\nsummary = {\n    \"n_rows\": len(dr),\n    \"n_patients\": dr[\"patientID\"].nunique(),\n    \"date_min\": dr[\"date\"].min(),\n    \"date_max\": dr[\"date\"].max(),\n    \"n_days\": dr[\"date\"].nunique(),\n    \"risk_mean_median\": dr[\"risk_mean\"].median(),\n    \"risk_mean_IQR\": (dr[\"risk_mean\"].quantile(0.25), dr[\"risk_mean\"].quantile(0.75)),\n    \"risk_p95_median\": dr[\"risk_p95\"].median(),\n    \"prevalence_median\": dr[\"prevalence\"].median(),\n    \"corr_riskmean_prevalence\": dr[[\"risk_mean\",\"prevalence\"]].corr().iloc[0,1],\n    \"corr_riskp95_prevalence\": dr[[\"risk_p95\",\"prevalence\"]].corr().iloc[0,1],\n}\nsummary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# forecasts\n\nNotes & options\n\nChange patient: set _patient = <your id> before running.\n\nChange K: set K_HOURS to the forecast horizon you want (e.g., 12).\n\nChange threshold: set THRESH if you prefer the VALâ€‘optimized threshold you saw in your summary.\n\nBetter inputs for multiâ€‘step: replace the â€œnaive persistenceâ€ block with proper hourâ€‘ahead forecasts of your sequence features (CGM, lifestyle PCs, visit vars) for stronger multiâ€‘hour accuracy.\n\n\nNotes for stronger multiâ€‘hour forecasts\n\nYouâ€™re currently using naive persistence for the future input features when rolling forward (i.e., you reuse the last observed hourâ€™s feature vector for each next hour). Thatâ€™s OK to get started, but for better K>6 accuracy consider:\n\nCGM features (cgm_mean, cgm_std, â€¦): replace persistence with a small CGM forecaster (e.g., ARIMA/Prophet/LightGBM) to generate hourâ€‘ahead CGM summaries, then feed those into your LSTM rolling window.\n\nLifestyle PCs (pc1_activity_energy, â€¦): if you have â€œtypical daily patternsâ€, a daily seasonal baseline (e.g., by hourâ€‘ofâ€‘day) can outperform pure persistence.\n\nVisit features (carb, meals, â€¦): these are daily; step them forward from the last known day or incorporate the next known visit day if available.\n\nIf you want, tell me:\n\na different patientID (from the list above),\n\nyour preferred K_HOURS, and\n\nwhether you want a more conservative (fewer alerts) or more sensitive (more alerts) threshold,\n\nand Iâ€™ll output a tailored snippet with those values baked in","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Forecasting & time-compare (fixed + improved)\n# ==========================\nimport os, numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib.dates as mdates\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, precision_recall_curve\nimport tensorflow as tf\n\n# --------- Fallbacks for global symbols if not already defined in the notebook ---------\ntry:\n    OUT_RESULTS_CSV\nexcept NameError:\n    OUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\n\ntry:\n    USE_STATIC_INPUT\nexcept NameError:\n    USE_STATIC_INPUT = True  # your training used static inputs\n\n# focal_loss (redefine here so load_model(custom_objects=...) always works)\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\n# --------------------------\n# ðŸ”§ Parameters you can edit\n# --------------------------\nOVERRIDE_PATIENT_ID        = 69    # force this patient (else it auto-suggests)\nK_HOURS                    = 12    # forecast horizon\nAUTO_USE_VAL_OPTIMAL_THR   = True  # use best VAL threshold from your results CSV\nTHRESH_MANUAL              = 0.49  # fallback if the VAL-optimal isnâ€™t found\nFORECAST_METHOD            = \"ema\" # \"ema\" | \"linear\" | \"persistence\"\nEMA_ALPHA                  = 0.6\nLIN_STEPS                  = 6\nPATIENT_SELECTION_STRATEGY = \"positives\"\nTOP_N_PATIENTS_TO_PLOT     = 3\nSAVE_DIR                   = \"/kaggle/working\"\n\n# Improvement toggles\nUSE_HORIZON_THRESHOLDS     = True  # calibrate per-horizon thresholds on VAL (boosts recall at longer horizons)\nFBETA_FOR_CAL              = 1.5   # Î²>1 favors recall\nAPPLY_NOFM                 = True  # apply N-of-M decision smoothing\nNOFM_N, NOFM_M             = 2, 3  # 2 of last 3 positives -> positive\n\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# --- Datetime helpers: make all times tz-naive in UTC ---\ndef _to_naive_utc(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    # If tz-aware, convert to UTC then drop tz; if tz-naive, leave as is\n    if getattr(s.dt, \"tz\", None) is not None:\n        s = s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n    return s\n\n# ----------------------------------------------------\n# 0) Utilities: choose best checkpoint & VAL threshold\n# ----------------------------------------------------\ndef pick_best_checkpoint(results_csv=OUT_RESULTS_CSV, ckpt_dir=\"checkpoints\"):\n    if not os.path.exists(results_csv):\n        raise FileNotFoundError(f\"Results CSV not found: {results_csv}\")\n    df = pd.read_csv(results_csv)\n    dfv = df[df[\"Split\"].astype(str).str.lower().eq(\"val\")].copy()\n    if dfv.empty:\n        raise RuntimeError(\"No VAL rows found in results; cannot pick best model.\")\n    dfv = dfv.sort_values(\"Overall/F1_weighted\", ascending=False)\n    best = dfv.iloc[0]\n    method = str(best[\"Method\"]); model = str(best[\"Model\"])\n    ckpt_path = os.path.join(ckpt_dir, f\"{method}__{model}.h5\")\n    if not os.path.exists(ckpt_path):\n        files = [os.path.join(ckpt_dir, f) for f in os.listdir(ckpt_dir) if f.endswith(\".h5\")]\n        if not files: raise FileNotFoundError(\"No .h5 checkpoints found in 'checkpoints'.\")\n        ckpt_path = files[0]\n        print(f\"[WARN] Expected checkpoint not found; using {ckpt_path}\")\n    print(f\"âœ… Best (VAL F1) â†’ {method}/{model} | ckpt = {ckpt_path}\")\n    return ckpt_path, method, model\n\ndef get_val_optimal_threshold(results_csv, method, model):\n    try:\n        df = pd.read_csv(results_csv)\n        dfv = df[(df[\"Split\"].astype(str).str.lower()==\"val\") &\n                 (df[\"Method\"].astype(str)==method) &\n                 (df[\"Model\"].astype(str)==model)].copy()\n        if dfv.empty:\n            dfv = df[df[\"Split\"].astype(str).str.lower()==\"val\"].copy()\n        dfv = dfv.sort_values(\"Overall/F1_weighted\", ascending=False)\n        t = float(dfv.iloc[0][\"Threshold\"])\n        if np.isfinite(t): return t\n    except Exception as e:\n        print(f\"[WARN] Could not read VAL-optimal threshold: {e}\")\n    return None\n\n# ------------------------------------------------\n# 1) Align predictions to hours for a given split\n# ------------------------------------------------\ndef predict_split_prob_df(model, data, hourly, split=\"test\", threshold=0.50):\n    Xs = data[split][\"X_seq\"]; Xstat = data[split][\"X_stat\"]\n    ytrue = data[split][\"y\"].astype(int).ravel()\n    if Xs is None or Xs.ndim != 3:\n        raise ValueError(f\"No sequences for split={split}.\")\n    yprob = model.predict([Xs, Xstat], verbose=0).ravel() if (Xstat is not None and Xstat.size>0) else model.predict(Xs, verbose=0).ravel()\n    ypred = (yprob >= float(threshold)).astype(int)\n    seq_len = Xs.shape[1]\n\n    sub = (hourly[hourly[\"Split\"].astype(str).str.lower() == split.lower()]\n           .sort_values([\"patientID\",\"hour\"]).reset_index())\n    sub = sub.rename(columns={\"index\":\"row_idx\"})\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\"seq_idx\":len(rows), \"patientID\":pid,\n                         \"hour\":pd.to_datetime(tgt[\"hour\"]),\n                         \"date\":pd.to_datetime(tgt.get(\"date\", pd.NaT))})\n    idx_map = pd.DataFrame(rows)\n    if len(idx_map) != len(ytrue):\n        raise RuntimeError(f\"Mapping length {len(idx_map)} != predictions length {len(ytrue)}.\")\n    out = (pd.DataFrame({\n                \"patientID\": idx_map[\"patientID\"].values,\n                \"hour\":      pd.to_datetime(idx_map[\"hour\"].values),\n                \"date\":      pd.to_datetime(idx_map[\"date\"].values),\n                \"y_true\":    ytrue,\n                \"y_prob\":    yprob,\n                \"y_pred\":    ypred\n            })\n            .sort_values([\"patientID\",\"hour\"]).reset_index(drop=True))\n    out[\"hour\"] = _to_naive_utc(out[\"hour\"])  # belt-and-suspenders\n    return out\n\n# ------------------------------------------------------------\n# 2) Mini feature forecaster for multi-step rolling predictions\n# ------------------------------------------------------------\ndef _ema_next(v, alpha=0.6):\n    s = v[0]\n    for x in v[1:]:\n        s = alpha*x + (1-alpha)*s\n    return float(s)\n\ndef _linear_next(v):\n    n = len(v)\n    if n < 2: return float(v[-1])\n    x = np.arange(n, dtype=float)\n    try:\n        b1, b0 = np.polyfit(x, v.astype(float), 1)  # y = b1*x + b0\n        return float(b1*(n) + b0)\n    except Exception:\n        return float(v[-1])\n\ndef next_feature_vector(hist_raw, feat_names, method=\"ema\", ema_alpha=0.6, lin_steps=6):\n    T, F = hist_raw.shape\n    out = np.zeros(F, dtype=float)\n    for j, name in enumerate(feat_names):\n        col = hist_raw[:, j]\n        if method == \"persistence\":\n            out[j] = float(col[-1])\n        elif method == \"linear\":\n            w = min(len(col), max(2, lin_steps))\n            out[j] = _linear_next(col[-w:])\n        else:  # \"ema\" default for dynamic signals\n            if any(k in str(name).lower() for k in [\"cgm\",\"pca\",\"pc1\",\"pc2\",\"pc3\",\"steps\",\"calories\",\"heart\"]):\n                w = min(len(col), max(2, lin_steps))\n                out[j] = _ema_next(col[-w:], alpha=ema_alpha)\n            else:\n                out[j] = float(col[-1])\n    return out\n\n# -----------------------------------------------------------------\n# 3) Prepare one anchor window & rolling multi-step patient forecast\n# -----------------------------------------------------------------\ndef _prepare_window_for_patient_index(hourly, data, patient_id, idx, split=\"test\"):\n    seq_feats = list(data[\"seq_features_used\"])\n    seq_len   = int(data[\"train\"][\"X_seq\"].shape[1])\n    sub = hourly[(hourly[\"Split\"].astype(str).str.lower()==split.lower()) &\n                 (hourly[\"patientID\"]==patient_id)].sort_values(\"hour\").reset_index(drop=True)\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    if idx < seq_len: raise ValueError(\"idx must be >= seq_len\")\n    hist_raw   = sub.loc[idx-seq_len:idx-1, seq_feats].astype(float).values\n    seq_scaler = data[\"scalers\"][\"seq\"]\n    hist_scaled= seq_scaler.transform(hist_raw) if seq_scaler is not None else hist_raw\n    # static\n    if USE_STATIC_INPUT and data[\"train\"][\"X_stat\"] is not None:\n        stat_feats = list(data.get(\"static_features_used\", []))\n        srow = (hourly[[\"patientID\"]+stat_feats].drop_duplicates(subset=[\"patientID\"]).set_index(\"patientID\"))\n        s = (srow.loc[patient_id].astype(float).values if patient_id in srow.index else np.zeros(len(stat_feats), dtype=float))\n        stat_scaler = data[\"scalers\"][\"stat\"]\n        if stat_scaler is not None and s.size>0:\n            s = stat_scaler.transform(s.reshape(1,-1)).ravel()\n    else:\n        s = None\n    last_hour  = pd.to_datetime(sub.loc[idx-1, \"hour\"])\n    return hist_scaled, s, hist_raw, last_hour, sub\n\ndef rolling_forecast_patient(model, data, hourly, patient_id, k=6, split=\"test\", threshold=0.50,\n                             method=FORECAST_METHOD, ema_alpha=EMA_ALPHA, lin_steps=LIN_STEPS):\n    seq_feats   = list(data[\"seq_features_used\"])\n    seq_len     = int(data[\"train\"][\"X_seq\"].shape[1])\n    seq_scaler  = data[\"scalers\"][\"seq\"]\n\n    sub = hourly[(hourly[\"Split\"].astype(str).str.lower()==split.lower()) &\n                 (hourly[\"patientID\"]==patient_id)].sort_values(\"hour\").reset_index(drop=True)\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    rows = []\n    for anchor_idx in range(seq_len, len(sub)-1):  # each anchor predicts +1..+k\n        last_needed = anchor_idx + k\n        if last_needed >= len(sub): break\n        Xwin_scaled, svec, raw_hist, _, _ = _prepare_window_for_patient_index(hourly, data, patient_id, anchor_idx, split=split)\n        cur_scaled = Xwin_scaled.copy()\n        cur_raw    = raw_hist.copy()\n        F = cur_scaled.shape[1]\n        for h in range(1, k+1):\n            xin = cur_scaled.reshape(1, seq_len, F)\n            prob = (model.predict([xin, svec.reshape(1,-1)], verbose=0).ravel()[0]\n                    if (svec is not None and USE_STATIC_INPUT) else\n                    model.predict(xin, verbose=0).ravel()[0])\n            tgt_idx  = anchor_idx + h\n            tgt_hour = pd.to_datetime(sub.loc[tgt_idx, \"hour\"])\n            y_true   = int(sub.loc[tgt_idx, \"hypo_label\"])\n            # horizon-specific threshold support\n            thr_h = (threshold.get(h, float(threshold.get(1, 0.50)))\n                     if isinstance(threshold, dict) else float(threshold))\n            rows.append({\n                \"patientID\": patient_id,\n                \"anchor_hour\": pd.to_datetime(sub.loc[anchor_idx, \"hour\"]),\n                \"forecast_hour\": tgt_hour,\n                \"horizon\": h,\n                \"y_prob\": float(prob),\n                \"y_pred\": int(prob >= thr_h),\n                \"y_true\": y_true\n            })\n            # roll features using mini forecaster\n            next_raw    = next_feature_vector(cur_raw, seq_feats, method=method, ema_alpha=ema_alpha, lin_steps=lin_steps)\n            next_scaled = seq_scaler.transform(next_raw.reshape(1,-1)).ravel() if seq_scaler is not None else next_raw\n            cur_scaled  = np.vstack([cur_scaled[1:], next_scaled])\n            cur_raw     = np.vstack([cur_raw[1:], next_raw])\n    out = (pd.DataFrame(rows).sort_values([\"forecast_hour\",\"horizon\"]).reset_index(drop=True))\n    out[\"forecast_hour\"] = _to_naive_utc(out[\"forecast_hour\"])  # <<< tz-fix\n    out[\"anchor_hour\"]   = _to_naive_utc(out[\"anchor_hour\"])\n    return out\n\n# -----------------------------------------\n# 4) Metrics by horizon + patient suggestion\n# -----------------------------------------\ndef metrics_by_horizon(df_forecast):\n    out = []\n    for h, g in df_forecast.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        p = g[\"y_prob\"].astype(float).values\n        yhat = g[\"y_pred\"].astype(int).values\n        try: roc = roc_auc_score(y, p)\n        except: roc = np.nan\n        try: pr  = average_precision_score(y, p)\n        except: pr  = np.nan\n        out.append({\n            \"horizon\": h, \"n\": len(g),\n            \"Accuracy\": accuracy_score(y, yhat),\n            \"Precision\": precision_score(y, yhat, zero_division=0),\n            \"Recall\": recall_score(y, yhat, zero_division=0),\n            \"F1\": f1_score(y, yhat, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr\n        })\n    return pd.DataFrame(out).sort_values(\"horizon\")\n\ndef metrics_by_horizon_on_col(df_forecast, col=\"y_pred\"):\n    rows = []\n    for h, g in df_forecast.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        yhat = g[col].astype(int).values\n        try: roc = roc_auc_score(y, g[\"y_prob\"].astype(float).values)\n        except: roc = np.nan\n        try: pr  = average_precision_score(y, g[\"y_prob\"].astype(float).values)\n        except: pr  = np.nan\n        rows.append({\n            \"horizon\": h, \"n\": len(g),\n            \"Accuracy\": accuracy_score(y, yhat),\n            \"Precision\": precision_score(y, yhat, zero_division=0),\n            \"Recall\": recall_score(y, yhat, zero_division=0),\n            \"F1\": f1_score(y, yhat, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr,\n            \"decision\": col\n        })\n    return pd.DataFrame(rows).sort_values(\"horizon\")\n\ndef suggest_patients_for_plots(df_nowcast, strategy=\"positives\", top_n=3):\n    g = df_nowcast.groupby(\"patientID\", as_index=False)\n    if strategy == \"coverage\":\n        s = g.size().rename(columns={\"size\":\"n\"}).sort_values(\"n\", ascending=False)\n        ids = list(s[\"patientID\"].head(top_n))\n    elif strategy == \"auc\":\n        rows = []\n        for pid, grp in df_nowcast.groupby(\"patientID\"):\n            y, p = grp[\"y_true\"].values, grp[\"y_prob\"].values\n            try: pr = average_precision_score(y, p)\n            except: pr = np.nan\n            rows.append({\"patientID\":pid, \"PR_AUC\":pr, \"n\":len(grp)})\n        s = (pd.DataFrame(rows).dropna(subset=[\"PR_AUC\"])\n             .sort_values([\"PR_AUC\",\"n\"], ascending=[False, False]))\n        ids = list(s[\"patientID\"].head(top_n)) if not s.empty else list(df_nowcast[\"patientID\"].value_counts().head(top_n).index)\n    else:  # \"positives\" default\n        pos = g[\"y_true\"].sum().rename(columns={\"y_true\":\"positives\"}) \\\n               .sort_values(\"positives\", ascending=False)\n        ids = list(pos[\"patientID\"].head(top_n))\n        if pos[\"positives\"].max() <= 0:\n            ids = list(df_nowcast[\"patientID\"].value_counts().head(top_n).index)\n    return ids\n\n# -----------------------\n# 5) Post-processing (N-of-M)\n# -----------------------\ndef apply_n_of_m_rule(df, n=2, m=3):\n    \"\"\"df must be one patient; columns: forecast_hour, y_pred.\"\"\"\n    df = df.sort_values(\"forecast_hour\").copy()\n    flags = df[\"y_pred\"].astype(int).values\n    out = np.zeros_like(flags)\n    for i in range(len(flags)):\n        win = flags[max(0, i-m+1):i+1]\n        out[i] = 1 if win.sum() >= n else 0\n    df[\"y_pred_nofm\"] = out\n    return df\n\n# -----------------------\n# 6) Plotting functions\n# -----------------------\ndef plot_nowcast_and_forecast_timeline(df_nowcast, df_forecast, patient_id,\n                                       hours_back=72, out_png=None, threshold=0.50):\n    past = df_nowcast[df_nowcast[\"patientID\"]==patient_id].sort_values(\"hour\").copy()\n    past[\"hour\"] = _to_naive_utc(past[\"hour\"])  # <<< tz-fix\n    if not past.empty and hours_back and hours_back>0:\n        cutoff = past[\"hour\"].max() - pd.Timedelta(hours=hours_back)\n        past = past[past[\"hour\"] >= cutoff]\n    if df_forecast.empty: raise ValueError(\"df_forecast is empty.\")\n    last_anchor = df_forecast[df_forecast[\"patientID\"]==patient_id][\"anchor_hour\"].max()\n    fut = (df_forecast[(df_forecast[\"patientID\"]==patient_id) &\n                       (df_forecast[\"anchor_hour\"]==last_anchor)]\n           .sort_values(\"forecast_hour\").copy())\n    fut[\"forecast_hour\"] = _to_naive_utc(fut[\"forecast_hour\"])  # <<< tz-fix\n\n    fig, ax = plt.subplots(figsize=(10,4))\n    if not past.empty:\n        ax.plot(past[\"hour\"], past[\"y_prob\"], lw=2, label=\"Nowcast (test) prob\")\n        ax.step(past[\"hour\"], past[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label (past)\")\n    ax.plot(fut[\"forecast_hour\"], fut[\"y_prob\"], lw=2, marker=\"o\",\n            label=f\"Forecast next {int(fut['horizon'].max())}h prob\")\n    if \"y_true\" in fut.columns and fut[\"y_true\"].notna().any():\n        ax.step(fut[\"forecast_hour\"], fut[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label (future)\")\n    ax.axhline(float(threshold) if not isinstance(threshold, dict) else float(threshold.get(1, 0.50)),\n               ls=\"--\", label=\"Decision threshold\")\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(\"Hour\"); ax.set_ylabel(\"Risk / Label\")\n    ax.set_title(f\"Patient {patient_id} â€” past nowcast + future forecast\")\n    ax.legend(loc=\"upper left\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d %H:%M\"))\n    fig.autofmt_xdate(); fig.tight_layout()\n    if out_png:\n        fig.savefig(out_png, dpi=200); print(f\"ðŸ–¼ï¸ Saved timeline â†’ {out_png}\")\n    plt.show()\n\ndef plot_nowcast_vs_forecast_h1(df_nowcast, df_forecast, patient_id, out_png=None):\n    f1 = df_forecast[(df_forecast[\"patientID\"]==patient_id) & (df_forecast[\"horizon\"]==1)].copy()\n    f1 = f1.rename(columns={\"forecast_hour\":\"hour\", \"y_prob\":\"y_prob_forecast_h1\"})\n    past = df_nowcast[df_nowcast[\"patientID\"]==patient_id][[\"hour\",\"y_prob\",\"y_true\"]].copy()\n\n    # --- force both keys to the same dtype/timezone ---\n    past[\"hour\"] = _to_naive_utc(past[\"hour\"])\n    f1[\"hour\"]   = _to_naive_utc(f1[\"hour\"])\n\n    j = past.merge(f1[[\"hour\",\"y_prob_forecast_h1\"]], on=\"hour\", how=\"inner\").sort_values(\"hour\")\n    if j.empty:\n        print(\"[INFO] No overlap between nowcast timeline and horizon-1 forecasts for this patient.\"); return\n    fig, ax = plt.subplots(figsize=(10,4))\n    ax.plot(j[\"hour\"], j[\"y_prob\"], lw=2, label=\"Nowcast prob (test)\")\n    ax.plot(j[\"hour\"], j[\"y_prob_forecast_h1\"], lw=2, linestyle=\"--\", label=\"Forecast@+1h prob\")\n    ax.step(j[\"hour\"], j[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label\")\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(\"Hour\"); ax.set_ylabel(\"Risk / Label\")\n    ax.set_title(f\"Patient {patient_id} â€” nowcast vs forecast@+1h\")\n    ax.legend(loc=\"upper left\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d %H:%M\"))\n    fig.autofmt_xdate(); fig.tight_layout()\n    if out_png:\n        fig.savefig(out_png, dpi=200); print(f\"ðŸ–¼ï¸ Saved compare@+1h â†’ {out_png}\")\n    plt.show()\n\n# -------------------------------------------------\n# 7) Threshold calibration on VAL (per horizon)\n# -------------------------------------------------\ndef _best_thr_by_fbeta(y, p, beta=1.0, thr_min=0.10, thr_max=0.70):\n    prec, rec, thr = precision_recall_curve(y, p)\n    prec, rec, thr = prec[:-1], rec[:-1], thr\n    mask = np.isfinite(thr) & (thr >= thr_min) & (thr <= thr_max)\n    if not mask.any():\n        return 0.50\n    fb = (1+beta**2) * (prec[mask]*rec[mask]) / (beta**2*prec[mask] + rec[mask] + 1e-9)\n    return float(thr[mask][np.nanargmax(fb)])\n\ndef calibrate_horizon_thresholds_on_val(model, data, hourly, k=12, beta=1.5, method=\"ema\"):\n    vals = []\n    val_mask = hourly[\"Split\"].astype(str).str.lower()==\"val\"\n    val_ids = sorted(hourly.loc[val_mask, \"patientID\"].unique())\n    for pid in val_ids:\n        df_fc_val = rolling_forecast_patient(\n            model, data, hourly, patient_id=pid, k=k, split=\"val\",\n            threshold=0.50, method=method  # temporary; only need probs\n        )\n        if not df_fc_val.empty:\n            vals.append(df_fc_val)\n    if not vals:\n        return {}\n    allv = pd.concat(vals, ignore_index=True)\n    out = {}\n    for h, g in allv.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        p = g[\"y_prob\"].astype(float).values\n        if len(np.unique(y)) < 2:\n            continue\n        out[h] = _best_thr_by_fbeta(y, p, beta=beta, thr_min=0.10, thr_max=0.70)\n    return out\n\n# ======================\n# 8) Run: load, forecast, plot\n# ======================\n# Preconditions\nif \"hourly\" not in globals() or \"data\" not in globals():\n    raise RuntimeError(\"hourly/data not found. Please run your feature builder and sequence builder cells first.\")\n\nCKPT_PATH, _best_method, _best_model = pick_best_checkpoint(OUT_RESULTS_CSV, \"checkpoints\")\nmodel = tf.keras.models.load_model(CKPT_PATH, custom_objects={\"loss\": focal_loss, \"focal_loss\": focal_loss})\n\n# (b) Threshold: VAL-optimal or manual\nif AUTO_USE_VAL_OPTIMAL_THR:\n    thr_val = get_val_optimal_threshold(OUT_RESULTS_CSV, _best_method, _best_model)\n    THRESH   = float(thr_val) if (thr_val is not None) else float(THRESH_MANUAL)\n    if thr_val is None:\n        print(f\"[INFO] Using manual THRESH={THRESH_MANUAL:.2f} (VAL-optimal not found).\")\n    else:\n        print(f\"âœ… Using VAL-optimal THRESH={THRESH:.2f}\")\nelse:\n    THRESH = float(THRESH_MANUAL)\n    print(f\"â„¹ï¸ Using manual THRESH={THRESH:.2f}\")\n\n# (c) Nowcast on TEST + suggestions\ndf_test_nowcast = predict_split_prob_df(model, data, hourly, split=\"test\", threshold=THRESH)\nif OVERRIDE_PATIENT_ID is not None and OVERRIDE_PATIENT_ID in set(df_test_nowcast[\"patientID\"].unique()):\n    suggested = [OVERRIDE_PATIENT_ID]\nelse:\n    suggested = suggest_patients_for_plots(df_test_nowcast, strategy=PATIENT_SELECTION_STRATEGY,\n                                           top_n=TOP_N_PATIENTS_TO_PLOT)\nprint(\"ðŸ“Œ Suggested patientID(s) to plot:\", suggested)\n\n# (d) Optional: learn per-horizon thresholds on VAL\nHORIZON_THR = None\nif USE_HORIZON_THRESHOLDS:\n    HORIZON_THR = calibrate_horizon_thresholds_on_val(\n        model, data, hourly, k=K_HOURS, beta=FBETA_FOR_CAL, method=FORECAST_METHOD\n    )\n    print(\"Horizon thresholds (VAL):\", HORIZON_THR)\n\n# (e) Forecast & plot\nfor pid in suggested:\n    print(f\"\\n=== Forecasting patient {pid} | K={K_HOURS}, method={FORECAST_METHOD} ===\")\n    thr_to_use = (HORIZON_THR if (isinstance(HORIZON_THR, dict) and len(HORIZON_THR)>0) else THRESH)\n    df_fc = rolling_forecast_patient(model, data, hourly, patient_id=pid, k=K_HOURS,\n                                     split=\"test\", threshold=thr_to_use,\n                                     method=FORECAST_METHOD, ema_alpha=EMA_ALPHA, lin_steps=LIN_STEPS)\n    print(\"Forecast rows:\", len(df_fc))\n\n    # Optional N-of-M smoothing\n    if APPLY_NOFM and not df_fc.empty:\n        df_fc = df_fc.groupby(\"patientID\", group_keys=False).apply(apply_n_of_m_rule, n=NOFM_N, m=NOFM_M)\n\n    # Metrics\n    hz_base = metrics_by_horizon(df_fc)\n    print(\"Horizon metrics (base decision):\\n\", hz_base)\n    if APPLY_NOFM and \"y_pred_nofm\" in df_fc.columns:\n        hz_nofm = metrics_by_horizon_on_col(df_fc, col=\"y_pred_nofm\")\n        print(\"Horizon metrics (N-of-M decision):\\n\", hz_nofm)\n\n    # Plots\n    outA = os.path.join(SAVE_DIR, f\"nowcast_plus_forecast_patient_{pid}.png\")\n    plot_nowcast_and_forecast_timeline(df_test_nowcast, df_fc, patient_id=pid,\n                                       hours_back=72, threshold=thr_to_use, out_png=outA)\n    outB = os.path.join(SAVE_DIR, f\"nowcast_vs_forecast_h1_patient_{pid}.png\")\n    plot_nowcast_vs_forecast_h1(df_test_nowcast, df_fc, patient_id=pid, out_png=outB)\n\nprint(\"âœ… Done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================\n# Leak-safe Ramadan features + Balanced LSTM\n# (hourly builder + sequences + training utilities + significance testing)\n# ==============================================\nimport os, time, warnings, random, re\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\nfrom scipy.stats import norm  # <-- added\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nSTATIC_CSV      = \"/kaggle/input/hmc-model-static-variables/outcome_static.csv\"\nVISIT_WIDE_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_wide_by_variable.csv\"\nVISIT_LONG_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_long.csv\"\n\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree.npz\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\nOUT_PLOTS_PNG   = \"/kaggle/working/combined_roc_pr_curves.png\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\nHYPO_CUTOFF   = 70.0\nMIN_CGM_PER_H = 4\nSEQ_LEN       = 24\n\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n    \"pc1_activity_energy\",\n    \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n)\n\nRANDOM_STATE     = 42\nTHR_MIN, THR_MAX = 0.40, 0.60\nAUGMENT_SIGMA    = 0.01\nRESAMPLE_METHODS = [\"none\",\"oversample_seq\",\"undersample_seq\",\"smote\",\"smoteenn\",\"smotetomek\"]\nUSE_STATIC_INPUT = True\n\n# --------------------\n# Utilities (robust column picking)\n# --------------------\ndef set_global_seeds(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_global_seeds(RANDOM_STATE)\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef safe_encode_gender(series):\n    if series.dtype == \"object\":\n        return (series.str.strip().str.lower().map({\"male\":1, \"m\":1, \"female\":0, \"f\":0}))\n    return pd.to_numeric(series, errors=\"coerce\")\n\ndef split_patients(unique_pids, test_size=0.2, val_size=0.1, random_state=RANDOM_STATE):\n    train_pids, test_pids = train_test_split(unique_pids, test_size=test_size, random_state=random_state)\n    val_fraction = val_size / max(1e-9, (1.0 - test_size))\n    train_pids, val_pids = train_test_split(train_pids, test_size=val_fraction, random_state=random_state)\n    return np.array(train_pids), np.array(val_pids), np.array(test_pids)\n\ndef _normalize_date(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    return s.dt.normalize()\n\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef _pick_col_flex(df: pd.DataFrame, preferred=None, required=False, name=\"\", must_contain_all=None, any_contains=None):\n    cols = list(df.columns)\n    norm_map = {c: _norm_col(c) for c in cols}\n    if preferred:\n        lower_pref = {str(p).strip().lower(): p for p in preferred}\n        for c in cols:\n            if str(c).strip().lower() in lower_pref:\n                return c\n    if preferred:\n        pref_norm = {_norm_col(p): p for p in preferred}\n        for c, n in norm_map.items():\n            if n in pref_norm:\n                return c\n    cands = []\n    for c, n in norm_map.items():\n        ok = True\n        if must_contain_all:\n            for tok in must_contain_all:\n                if _norm_col(tok) not in n:\n                    ok = False; break\n        if ok and any_contains:\n            if not any(_norm_col(tok) in n for tok in any_contains):\n                ok = False\n        if ok: cands.append(c)\n    if cands:\n        def _priority(col: str):\n            n = norm_map[col]\n            starts_pid = n.startswith(\"patientid\")\n            has_pid    = \"patientid\" in n\n            return (-(starts_pid or has_pid), len(n))\n        cands.sort(key=_priority)\n        return cands[0]\n    if required:\n        raise KeyError(f\"Required column not found for {name}. preferred={preferred} | must_contain_all={must_contain_all} | any_contains={any_contains}. Available: {cols}\")\n    return None\n\ndef _pick_patient_col(df: pd.DataFrame) -> str:\n    preferred = [\"patientID\",\"patientId\",\"PatientID (Huawei Data)\",\"subject_id\",\"patid\",\"pid\",\"id\",\"huaweiid\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"patientID\",\n                          must_contain_all=[\"id\"], any_contains=[\"patient\",\"subject\",\"pat\",\"huawei\"])\n\ndef _pick_date_col(df: pd.DataFrame) -> str:\n    preferred = [\"date\",\"visit_date\",\"Date\",\"day\",\"timestamp\",\"Visit Date\",\"date_of_visit\",\"start\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"date\",\n                          any_contains=[\"date\",\"visit\",\"day\",\"timestamp\",\"start\"])\n\ndef _pick_variable_col(df: pd.DataFrame) -> str:\n    preferred = [\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"variable\",\n                          any_contains=[\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"])\n\ndef _pick_value_col(df: pd.DataFrame) -> str:\n    preferred = [\"value\",\"val\",\"measure_value\",\"reading\",\"amount\",\"score\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"value\",\n                          any_contains=[\"value\",\"val\",\"measurevalue\",\"reading\",\"amount\",\"score\"])\n\n# ---------------------------\n# Loaders for external files\n# ---------------------------\ndef load_static_df(static_csv=STATIC_CSV, needed=STATIC_COLS):\n    if not static_csv or not os.path.exists(static_csv):\n        print(\"âš ï¸ Static CSV not found; static features will be zero-filled.\")\n        return None\n    df = pd.read_csv(static_csv)\n    pid_col = _pick_patient_col(df)\n    df = df.rename(columns={pid_col:\"patientID\"})\n    keep = [\"patientID\"] + [c for c in needed if c in df.columns]\n    df = df[keep].drop_duplicates(subset=[\"patientID\"]).copy()\n    if \"Gender\" in df.columns:\n        df[\"Gender\"] = safe_encode_gender(df[\"Gender\"])\n    for c in keep:\n        if c != \"patientID\":\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    print(f\"â„¹ï¸ static: using patientID column = '{pid_col}'\")\n    return df\n\ndef load_visit_df(visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV, needed=VISIT_COLS):\n    if visit_wide_csv and os.path.exists(visit_wide_csv):\n        wide = pd.read_csv(visit_wide_csv)\n        pid_col  = _pick_patient_col(wide)\n        date_col = _pick_date_col(wide)\n        wide = wide.rename(columns={pid_col:\"patientID\", date_col:\"date\"})\n        wide[\"date\"] = _normalize_date(wide[\"date\"])\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"â„¹ï¸ visit-wide: patientID='{pid_col}', date='{date_col}', kept={keep[2:]}\")\n            return wide[keep].copy()\n        else:\n            print(\"âš ï¸ VISIT_WIDE_CSV found but none of the needed visit columns present; will try LONG if available.\")\n    if visit_long_csv and os.path.exists(visit_long_csv):\n        long = pd.read_csv(visit_long_csv)\n        pid_col   = _pick_patient_col(long)\n        date_col  = _pick_date_col(long)\n        var_col   = _pick_variable_col(long)\n        value_col = _pick_value_col(long)\n        long = long.rename(columns={pid_col:\"patientID\", date_col:\"date\", var_col:\"variable\", value_col:\"value\"})\n        long[\"date\"] = _normalize_date(long[\"date\"])\n        wide = (long\n                .pivot_table(index=[\"patientID\",\"date\"], columns=\"variable\", values=\"value\", aggfunc=\"mean\")\n                .reset_index())\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"â„¹ï¸ visit-long: patientID='{pid_col}', date='{date_col}', variables matched={keep[2:]}\")\n            return wide[keep].copy()\n        print(\"âš ï¸ VISIT_LONG_CSV present but none of the needed variables were found in the pivot.\")\n    print(\"âš ï¸ No usable visit CSVs found; visit features will be zero-filled.\")\n    return None\n\n# ----------------------------------------------------------------\n# Part A â€” Build hourly Ramadan features and leak-safe transforms\n# ----------------------------------------------------------------\ndef build_hourly_features_with_leak_safe_transforms(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n    static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n    df = pd.read_csv(in_csv)\n    if \"patientID\" not in df.columns:\n        pid_col = _pick_patient_col(df)\n        df = df.rename(columns={pid_col: \"patientID\"})\n        print(f\"â„¹ï¸ intraday: using patientID column = '{pid_col}'\")\n\n    start_col = \"start\" if \"start\" in df.columns else _pick_date_col(df)\n    df[start_col] = to_dt(df[start_col])\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[start_col].dt.date)\n    df[\"hour\"]       = df[start_col].dt.floor(\"h\")\n    df[\"hour_of_day\"]= df[\"hour\"].dt.hour\n\n    df = ensure_numeric(df)\n    df = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"âŒ Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(\n                       cgm_min=(\"cgm\",\"min\"),\n                       cgm_max=(\"cgm\",\"max\"),\n                       cgm_mean=(\"cgm\",\"mean\"),\n                       cgm_std=(\"cgm\",\"std\")\n                   )\n                   .sort_values([\"patientID\",\"hour\"])\n                   .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean().fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    pids = hourly[\"patientID\"].dropna().unique()\n    train_p, val_p, test_p = split_patients(pids, test_size=test_size, val_size=val_size, random_state=random_state)\n    hourly[\"Split\"] = np.where(hourly[\"patientID\"].isin(train_p), \"train\",\n                        np.where(hourly[\"patientID\"].isin(val_p), \"val\", \"test\"))\n\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"] == \"train\"\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    def _apply_cgm_pca(df_in):\n        X = scal_cgm.transform(df_in[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df_in.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n    hourly = _apply_cgm_pca(hourly)\n\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        pca_life  = PCA(n_components=3, random_state=random_state).fit(\n            scal_life.transform(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        )\n        X_all = scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        Z_all = pca_life.transform(X_all)\n        hourly[\"pc1_activity_energy\"] = Z_all[:,0]\n        hourly[\"pc2_physiology\"]      = Z_all[:,1]\n        hourly[\"pc3_sleep_rest\"]      = Z_all[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    visit_df = load_visit_df(visit_wide_csv, visit_long_csv, VISIT_COLS)\n    hourly[\"date\"] = hourly[\"hour\"].dt.normalize()\n    if visit_df is not None:\n        visit_df[\"date\"] = pd.to_datetime(visit_df[\"date\"], errors=\"coerce\").dt.normalize()\n        visit_df = visit_df[(visit_df[\"date\"] >= RAMADAN_START) & (visit_df[\"date\"] <= RAMADAN_END)].copy()\n        hourly = hourly.merge(visit_df, on=[\"patientID\",\"date\"], how=\"left\")\n    for c in VISIT_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    static_df = load_static_df(static_csv, STATIC_COLS)\n    if static_df is not None:\n        hourly = hourly.merge(static_df, on=\"patientID\", how=\"left\")\n    for c in STATIC_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(cast := hourly[c], errors=\"coerce\").fillna(0.0)\n\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly.to_csv(out_csv, index=False)\n    print(f\"âœ… Saved hourly features (leak-safe) â†’ {out_csv}\")\n\n    return hourly, (train_p, val_p, test_p)\n\n# ---------------------------------------------------------------\n# Part B â€” Build sequences (optionally with per-patient static)\n# ---------------------------------------------------------------\ndef build_sequences_by_split(hourly, splits, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n                             static_cols=STATIC_COLS, scale_features=True):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float).fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n                if static_mat is not None and pid in static_mat.index:\n                    X_stat.append(static_mat.loc[pid].values.astype(float))\n        X_seq = np.array(X_seq); y = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size==0: return X\n            n = X.shape[0]; return seq_scaler.transform(X.reshape(-1, n_f)).reshape(n, SEQ_LEN, n_f)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size>0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size==0: return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"âœ… Sequences built | train={Xtr_s.shape}, val={Xva_s.shape}, test={Xte_s.shape}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ------------------------------------------------------\n# Balanced LSTM pipeline utilities (metrics, resampling)\n# ------------------------------------------------------\nTHR_MIN, THR_MAX = THR_MIN, THR_MAX\n\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask])); idx = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_overall(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef _specificity_per_class(y_true, y_pred, positive_label):\n    y_true_bin = (np.asarray(y_true).ravel() == positive_label).astype(int)\n    y_pred_bin = (np.asarray(y_pred).ravel() == positive_label).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_per_class(y_true, y_pred, positive_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    metrics[\"Overall/Specificity\"]          = _specificity_overall(y_true, y_pred)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/MSE_pred\"]             = mse_pred\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/MSE_prob\"]       = mse_prob\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/MSE_prob\"] = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n    return metrics\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\ndef augment_with_static(X_seq, X_stat, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0:\n        return X_seq, X_stat, y\n    noise = np.random.normal(0, sigma, X_seq.shape)\n    X_seq_aug = np.vstack([X_seq, X_seq + noise])\n    y_aug     = np.hstack([y, y])\n    if X_stat is not None:\n        X_stat_aug = np.vstack([X_stat, X_stat])\n    else:\n        X_stat_aug = None\n    return X_seq_aug, X_stat_aug, y_aug\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE, return_index=False, allow_smote=True):\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n    base_idx = np.arange(n)\n\n    if method == \"none\":\n        return (X, y, base_idx) if return_index else (X, y)\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        rng = np.random.default_rng(random_state)\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0:\n            return (X, y, base_idx) if return_index else (X, y)\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = rng.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = rng.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:\n            if n0 > n1:\n                keep0 = rng.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = rng.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        rng.shuffle(keep)\n        Xr, yr = X[keep], y[keep]\n        return (Xr, yr, keep) if return_index else (Xr, yr)\n\n    if not allow_smote:\n        print(f\"âš ï¸ {method} disabled when static input is used; falling back to 'none'.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    minority_n = int((y==1).sum())\n    majority_n = int((y==0).sum())\n    if minority_n < 2 or majority_n < 2:\n        print(\"âš ï¸ Not enough samples for SMOTE/SMOTEENN/SMOTETomek; skipping resampling.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    Xf = X.reshape(n, -1)\n    if method == \"smote\":\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n\n    Xr = Xr.reshape(-1, T, F)\n    return (Xr, yr, None) if return_index else (Xr, yr)\n\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state=RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0:\n        return (X_test, y_test, (None if X_stat is None else X_stat))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ------------------------------------------------------\n# Model builders (supports seq-only or seq+static)\n# ------------------------------------------------------\ndef make_model(seq_len, n_seq_f, n_stat_f=0, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n        x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x)\n        x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x);                    x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L1\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l1(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l1(1e-5))(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x);                          x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(32, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(32, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(32, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# ==========================\n# Significance / Uncertainty\n# ==========================\ndef delong_bootstrap_auc(y_true, p1, p2, n_boot=2000, random_state=42):\n    \"\"\"\n    Bootstrap test for Î”AUC = AUC(p1) - AUC(p2).\n    Returns (delta_auc, se, z, p_value_two_sided).\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    y_true = np.asarray(y_true).astype(int).ravel()\n    p1 = np.asarray(p1).astype(float).ravel()\n    p2 = np.asarray(p2).astype(float).ravel()\n\n    diffs = []\n    n = len(y_true)\n    for _ in range(n_boot):\n        idx = rng.integers(0, n, n)\n        yb, p1b, p2b = y_true[idx], p1[idx], p2[idx]\n        try:\n            diffs.append(roc_auc_score(yb, p1b) - roc_auc_score(yb, p2b))\n        except ValueError:\n            continue\n    diffs = np.array(diffs, dtype=float)\n    delta = float(np.nanmean(diffs))\n    se    = float(np.nanstd(diffs, ddof=1))\n    z     = 0.0 if se == 0.0 else delta / se\n    pval  = 2.0 * (1.0 - norm.cdf(abs(z)))\n    return delta, se, z, pval\n\ndef bootstrap_ci_auc(y_true, p, n_boot=2000, alpha=0.05, random_state=42):\n    \"\"\"\n    Percentile bootstrap CI for ROC-AUC. Returns (auc_hat, [lo, hi]).\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    y_true = np.asarray(y_true).astype(int).ravel()\n    p = np.asarray(p).astype(float).ravel()\n\n    stats = []\n    n = len(y_true)\n    for _ in range(n_boot):\n        idx = rng.integers(0, n, n)\n        yb, pb = y_true[idx], p[idx]\n        try:\n            stats.append(roc_auc_score(yb, pb))\n        except ValueError:\n            continue\n    stats = np.array(stats, dtype=float)\n    auc_hat = float(roc_auc_score(y_true, p))\n    lo = float(np.nanpercentile(stats, 2.5))\n    hi = float(np.nanpercentile(stats, 97.5))\n    return auc_hat, [lo, hi]\n\n# ------------------------------------------------------\n# Training runner (VAL for threshold; TEST for final)\n# ------------------------------------------------------\ndef run_balanced_lstm_pipeline(data,\n                               arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n                               resample_methods=RESAMPLE_METHODS,\n                               thr_min=THR_MIN, thr_max=THR_MAX,\n                               random_state=RANDOM_STATE,\n                               results_csv=OUT_RESULTS_CSV,\n                               plots_png=OUT_PLOTS_PNG):\n\n    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n    os.makedirs(os.path.dirname(plots_png), exist_ok=True)\n    os.makedirs(\"checkpoints\", exist_ok=True)\n\n    Xtr, Xtr_stat, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"], data[\"train\"][\"y\"]\n    Xva, Xva_stat, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"X_stat\"],   data[\"val\"][\"y\"]\n    Xte, Xte_stat, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"X_stat\"],  data[\"test\"][\"y\"]\n\n    Xtr, Xtr_stat, ytr = augment_with_static(Xtr, Xtr_stat, ytr, sigma=AUGMENT_SIGMA)\n\n    Xte_bal, yte_bal, Xte_stat_bal = make_balanced_test(Xte, yte, X_stat=Xte_stat)\n\n    results     = {}\n    roc_curves  = {}\n    pr_curves   = {}\n\n    # For post-hoc significance tests\n    prob_store = {}  # (method, arch, split) -> (y_true, y_prob)\n\n    allow_smote = (Xtr_stat is None or not USE_STATIC_INPUT)\n\n    def train_eval_one(method_name, arch_name):\n        nonlocal Xtr, ytr, Xtr_stat\n\n        Xrs, yrs, idx_map = seq_resample(Xtr, ytr, method=method_name, random_state=random_state,\n                                         return_index=True, allow_smote=allow_smote)\n        if Xtr_stat is not None and USE_STATIC_INPUT:\n            Xrs_stat = Xtr_stat if idx_map is None else Xtr_stat[idx_map]\n        else:\n            Xrs_stat = None\n\n        seq_len, n_seq_f = Xrs.shape[1], Xrs.shape[2]\n        n_stat_f = 0 if (Xrs_stat is None or not USE_STATIC_INPUT) else Xrs_stat.shape[1]\n        model = make_model(seq_len, n_seq_f, n_stat_f=n_stat_f, arch=arch_name, lr=1e-3)\n\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n        ckpt_path = f\"checkpoints/{method_name}__{arch_name}.h5\"\n        cp = ModelCheckpoint(ckpt_path, save_best_only=True, monitor=\"val_loss\", verbose=0)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xrs, Xrs_stat], yrs,\n                      validation_data=([Xva, Xva_stat], yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict([Xrs, Xrs_stat], verbose=0).ravel()\n            p_va  = model.predict([Xva, Xva_stat], verbose=0).ravel()\n            p_te  = model.predict([Xte, Xte_stat], verbose=0).ravel()\n            p_teB = model.predict([Xte_bal, Xte_stat_bal], verbose=0).ravel() if Xte_stat_bal is not None else model.predict(Xte_bal, verbose=0).ravel()\n        else:\n            model.fit(Xrs, yrs,\n                      validation_data=(Xva, yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict(Xrs, verbose=0).ravel()\n            p_va  = model.predict(Xva, verbose=0).ravel()\n            p_te  = model.predict(Xte, verbose=0).ravel()\n            p_teB = model.predict(Xte_bal, verbose=0).ravel()\n\n        # thresholds on VAL\n        try:\n            fpr_va, tpr_va, thr_roc_va = roc_curve(yva, p_va); auc_roc = auc(fpr_va, tpr_va)\n        except ValueError:\n            fpr_va, tpr_va, thr_roc_va, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n        youden_va = tpr_va - fpr_va\n        t_roc, _ = _best_threshold_in_range(thr_roc_va, youden_va, thr_min, thr_max)\n\n        prec_va, rec_va, thr_pr_va = precision_recall_curve(yva, p_va)\n        f1s_va = 2*prec_va[:-1]*rec_va[:-1] / (prec_va[:-1]+rec_va[:-1]+1e-8)\n        t_pr, _ = _best_threshold_in_range(thr_pr_va, f1s_va, thr_min, thr_max)\n        ap_val  = average_precision_score(yva, p_va)\n\n        roc_curves[(method_name, arch_name)] = (fpr_va, tpr_va, auc_roc)\n        pr_curves[(method_name, arch_name)]  = (rec_va, prec_va, ap_val)\n        print(f\"ðŸ“Œ [{method_name}/{arch_name}] VAL thresholds â†’ Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{thr_min},{thr_max}])\")\n\n        eval_ts = sorted(set([thr_min, 0.50, thr_max, float(t_roc), float(t_pr)]))\n\n        for t in eval_ts:\n            yhat_tr  = (p_tr  >= t).astype(int)\n            yhat_va  = (p_va  >= t).astype(int)\n            yhat_te  = (p_te  >= t).astype(int)\n            yhat_teB = (p_teB >= t).astype(int)\n\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__train\"]        = evaluate_full_metrics(yrs,     yhat_tr,  p_tr)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__val\"]          = evaluate_full_metrics(yva,     yhat_va,  p_va)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__test\"]         = evaluate_full_metrics(yte,     yhat_te,  p_te)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__testBalanced\"] = evaluate_full_metrics(yte_bal, yhat_teB, p_teB)\n\n        # --- store probabilities for A/B significance (threshold-free AUC) ---\n        prob_store[(method_name, arch_name, \"test\")]         = (yte,     p_te)\n        prob_store[(method_name, arch_name, \"testBalanced\")] = (yte_bal, p_teB)\n\n    # Loop: resampling methods Ã— architectures\n    for METHOD in resample_methods:\n        if METHOD in {\"smote\",\"smoteenn\",\"smotetomek\"} and (data[\"train\"][\"X_stat\"] is not None and USE_STATIC_INPUT):\n            print(f\"â­ï¸  Skipping {METHOD} (static input enabled).\")\n            continue\n        print(f\"\\nðŸ” Resampling: {METHOD} | train y-dist = {Counter(data['train']['y'])}\")\n        for ARCH in arch_list:\n            train_eval_one(METHOD, ARCH)\n\n    # --- Plots (validation curves)\n    plt.figure(figsize=(14,6))\n    plt.subplot(1,2,1)\n    for (meth, arch), (fpr, tpr, auc_roc) in roc_curves.items():\n        plt.plot(fpr, tpr, label=f'{meth}/{arch} (VAL AUC={auc_roc:.3f})')\n    plt.plot([0,1],[0,1],'--',label='Random')\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (Validation)'); plt.legend(fontsize=8)\n    plt.subplot(1,2,2)\n    for (meth, arch), (rec, prec, ap) in pr_curves.items():\n        plt.plot(rec, prec, label=f'{meth}/{arch} (VAL AP={ap:.3f})')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR (Validation)'); plt.legend(fontsize=8)\n    plt.tight_layout(); plt.savefig(plots_png, dpi=300); plt.show()\n    print(f\"ðŸ–¼ï¸ Saved plots â†’ {plots_png}\")\n\n    # --- Results CSV\n    results_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"Key\"})\n    k = results_df[\"Key\"].str.strip()\n    results_df[\"Split\"]  = np.where(k.str.endswith(\"__train\"), \"train\",\n                             np.where(k.str.endswith(\"__val\"), \"val\",\n                             np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                             np.where(k.str.endswith(\"__test\"), \"test\", np.nan))))\n    parts = k.str.split(\"__\")\n    results_df[\"Method\"]    = parts.str[0]\n    results_df[\"Model\"]     = parts.str[1]\n    results_df[\"Threshold\"] = pd.to_numeric(parts.str[2].str.replace(\"thr_\",\"\", regex=False), errors=\"coerce\")\n    results_df.round(6).to_csv(results_csv, index=False)\n    print(f\"ðŸ“‘ Saved results â†’ {results_csv}\")\n\n    # ================================\n    # Pairwise AUC significance (A/B)\n    # ================================\n    pairs_to_compare = [\n        ((\"none\",\"LSTM_100\"), (\"oversample_seq\",\"LSTM_100\")),\n        ((\"none\",\"LSTM_100\"), (\"undersample_seq\",\"BiLSTM\")),\n        ((\"oversample_seq\",\"LSTM_100\"), (\"undersample_seq\",\"BiLSTM\")),\n    ]\n    splits_to_use = [\"test\", \"testBalanced\"]\n\n    sig_rows = []\n    for split_name in splits_to_use:\n        for (A, B) in pairs_to_compare:\n            methA, archA = A\n            methB, archB = B\n            keyA = (methA, archA, split_name)\n            keyB = (methB, archB, split_name)\n            if keyA not in prob_store or keyB not in prob_store:\n                continue\n            (yA, pA) = prob_store[keyA]\n            (yB, pB) = prob_store[keyB]\n            y_true = yA  # same split -> same ground truth\n            delta, se, z, p = delong_bootstrap_auc(y_true, pA, pB, n_boot=2000, random_state=random_state)\n            aucA, ciA = bootstrap_ci_auc(y_true, pA, n_boot=2000, alpha=0.05, random_state=random_state)\n            aucB, ciB = bootstrap_ci_auc(y_true, pB, n_boot=2000, alpha=0.05, random_state=random_state)\n            sig_rows.append({\n                \"Split\": split_name,\n                \"ModelA\": f\"{methA}/{archA}\",\n                \"ModelB\": f\"{methB}/{archB}\",\n                \"AUC_A\": aucA, \"AUC_A_CI95_L\": ciA[0], \"AUC_A_CI95_U\": ciA[1],\n                \"AUC_B\": aucB, \"AUC_B_CI95_L\": ciB[0], \"AUC_B_CI95_U\": ciB[1],\n                \"Delta_AUC\": delta, \"SE_Delta\": se, \"Z\": z, \"P_value\": p\n            })\n\n    sig_df = pd.DataFrame(sig_rows)\n    out_sig_csv = (os.path.join(os.path.dirname(results_csv), \"auc_significance.csv\")\n                   if os.path.dirname(results_csv) else \"auc_significance.csv\")\n    sig_df.to_csv(out_sig_csv, index=False)\n    print(f\"ðŸ“‘ Saved AUC significance table â†’ {out_sig_csv}\")\n    if not sig_df.empty:\n        print(sig_df.head(10).to_string(index=False))\n\n    return results_df\n\n# --------------------\n# Run end-to-end\n# --------------------\nif __name__ == \"__main__\":\n    hourly, splits = build_hourly_features_with_leak_safe_transforms(\n        in_csv=CSV_INTRADAY_WITH_VISITS,\n        out_csv=OUT_HOURLY_CSV,\n        min_cgm_per_hour=MIN_CGM_PER_H,\n        test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n        static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n    )\n\n    data = build_sequences_by_split(\n        hourly, splits,\n        seq_len=SEQ_LEN,\n        seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n        static_cols=STATIC_COLS,\n        scale_features=True\n    )\n\n    np.savez_compressed(\n        OUT_SEQ_NPZ,\n        Xtr=data[\"train\"][\"X_seq\"],  Xtr_stat=(data[\"train\"][\"X_stat\"] if data[\"train\"][\"X_stat\"] is not None else np.empty((0,0))),\n        ytr=data[\"train\"][\"y\"],\n        Xva=data[\"val\"][\"X_seq\"],    Xva_stat=(data[\"val\"][\"X_stat\"] if data[\"val\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yva=data[\"val\"][\"y\"],\n        Xte=data[\"test\"][\"X_seq\"],   Xte_stat=(data[\"test\"][\"X_stat\"] if data[\"test\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yte=data[\"test\"][\"y\"],\n        seq_features_used=np.array(data[\"seq_features_used\"], dtype=object),\n        static_features_used=np.array(data[\"static_features_used\"], dtype=object)\n    )\n    print(f\"ðŸ’¾ Saved sequences â†’ {OUT_SEQ_NPZ}\")\n\n    results_df = run_balanced_lstm_pipeline(\n        data,\n        arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n        resample_methods=RESAMPLE_METHODS,\n        thr_min=THR_MIN, thr_max=THR_MAX,\n        random_state=RANDOM_STATE,\n        results_csv=OUT_RESULTS_CSV,\n        plots_png=OUT_PLOTS_PNG\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Cross-Phase Validation (Leak-safe)\n#   Train: Ramadan  â†’ Test: Shawwal\n#   Train: Shawwal  â†’ Test: Ramadan\n# Models: LSTM_100, BiLSTM, LSTM_50, LSTM_25_L2\n# ============================================================\n\nimport os, re, random, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, mean_squared_error\n)\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\n\n# --------------------------\n# CONFIG\n# --------------------------\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE); tf.random.set_seed(RANDOM_STATE)\n\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\n# If you don't have static/visit files ready, leave them None\nSTATIC_CSV      = None\nVISIT_WIDE_CSV  = None\nVISIT_LONG_CSV  = None\n\nRAMADAN_START  = pd.to_datetime(\"2023-03-22\"); RAMADAN_END  = pd.to_datetime(\"2023-04-19\")\nSHAWWAL_START  = pd.to_datetime(\"2023-04-20\"); SHAWWAL_END  = pd.to_datetime(\"2023-05-19\")\n\nSEQ_LEN       = 24\nHYPO_CUTOFF   = 70.0\nMIN_CGM_PER_H = 4\nVAL_FRACTION  = 0.15  # within *training phase* patients\n\nUSE_STATIC_INPUT = False  # this script runs seq-only (set True if you extend with static branch)\nARCH_LIST = (\"LSTM_100\",\"BiLSTM\",\"LSTM_50\",\"LSTM_25_L2\")\nTHR_MIN, THR_MAX = 0.40, 0.60\n\n# Features (sequence)\nLIFESTYLE_COLS_CANDIDATES = [\"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"]\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\"pc1_activity_energy\"\n)\n\n# --------------------------\n# UTILS\n# --------------------------\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    return {0: float(N/(2.0*n0)), 1: float(N/(2.0*n1))}\n\n# --------------------------\n# DATA: build hourly per window\n# --------------------------\ndef build_hourly_for_window(in_csv, start_date, end_date, min_cgm_per_hour=MIN_CGM_PER_H):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(in_csv)\n    df = pd.read_csv(in_csv)\n\n    # normalize patient column\n    if \"patientID\" not in df.columns:\n        for c in df.columns:\n            if _norm_col(c).startswith(\"patientid\") or _norm_col(c) in {\"patientid\",\"id\",\"huaweiid\"}:\n                df = df.rename(columns={c:\"patientID\"})\n                break\n        if \"patientID\" not in df.columns:\n            raise KeyError(\"patientID column not found.\")\n\n    # time columns\n    time_col = \"start\" if \"start\" in df.columns else (\"timestamp\" if \"timestamp\" in df.columns else None)\n    if time_col is None and \"date\" not in df.columns:\n        raise KeyError(\"No start/timestamp/date column found.\")\n    if time_col is not None:\n        df[time_col] = to_dt(df[time_col])\n        df[\"date\"] = pd.to_datetime(df[time_col].dt.date)\n        df[\"hour\"] = df[time_col].dt.floor(\"h\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n        df[\"hour\"] = df[\"date\"].dt.floor(\"h\")\n\n    df[\"hour_of_day\"] = pd.to_datetime(df[\"hour\"]).dt.hour\n    df = ensure_numeric(df)\n\n    # filter window\n    df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)].copy()\n\n    # CGM present?\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # valid hours\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # hourly summaries\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(cgm_min=(\"cgm\",\"min\"),\n                        cgm_max=(\"cgm\",\"max\"),\n                        cgm_mean=(\"cgm\",\"mean\"),\n                        cgm_std=(\"cgm\",\"std\"))\n                   .sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = pd.to_datetime(hourly[\"hour\"]).dt.hour\n\n    # labels\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # composites\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # lifestyle means (if present)\n    life_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if life_cols:\n        life_hourly = (df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[life_cols].mean().fillna(0.0))\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n    else:\n        for c in [\"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"]:\n            if c not in hourly.columns:\n                hourly[c] = 0.0\n\n    hourly[\"date\"] = pd.to_datetime(pd.to_datetime(hourly[\"hour\"]).dt.date)\n    return hourly\n\n# --------------------------\n# Fit leak-safe PCA (train phase only) and add PCA cols\n# --------------------------\ndef apply_leak_safe_pca(hourly, lifestyle_cols=None):\n    hourly = hourly.copy()\n    assert \"Split\" in hourly.columns, \"hourly must have Split\"\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"].astype(str).str.lower().eq(\"train\")\n\n    # CGM PCA\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=RANDOM_STATE).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    X_all = scal_cgm.transform(hourly[cgm_cols].fillna(0.0))\n    Z_all = pca_cgm.transform(X_all)\n    hourly[\"pca_cgm1\"], hourly[\"pca_cgm2\"], hourly[\"pca_cgm3\"] = Z_all[:,0], Z_all[:,1], Z_all[:,2]\n\n    # Lifestyle PCA (if present)\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        ZL = PCA(n_components=3, random_state=RANDOM_STATE).fit_transform(\n            scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        )\n        hourly[\"pc1_activity_energy\"], hourly[\"pc2_physiology\"], hourly[\"pc3_sleep_rest\"] = ZL[:,0], ZL[:,1], ZL[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n\n    return hourly\n\n# --------------------------\n# Build sequences by split\n# --------------------------\ndef build_sequences(hourly, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing {c}\")\n\n    hourly = hourly.copy()\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    missing = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing:\n        raise KeyError(f\"Missing seq features: {missing}\")\n\n    train_p = sorted(hourly.loc[hourly[\"Split\"].eq(\"train\"), \"patientID\"].unique())\n    val_p   = sorted(hourly.loc[hourly[\"Split\"].eq(\"val\"),   \"patientID\"].unique())\n    test_p  = sorted(hourly.loc[hourly[\"Split\"].eq(\"test\"),  \"patientID\"].unique())\n\n    def _build_for(pid_list):\n        sub = hourly[hourly[\"patientID\"].isin(pid_list)].sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n        X, y = [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len: continue\n            feats  = grp[list(seq_feature_cols)].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp)-seq_len):\n                X.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n        return (np.array(X), np.array(y).astype(int))\n\n    Xtr, ytr = _build_for(train_p)\n    Xva, yva = _build_for(val_p)\n    Xte, yte = _build_for(test_p)\n\n    # scaler on TRAIN only (sequence features)\n    seq_scaler = None\n    if Xtr.size > 0:\n        F = Xtr.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr.reshape(-1, F))\n        def _scale(X):\n            if X is None or X.size==0: return X\n            n, T, F = X.shape\n            return seq_scaler.transform(X.reshape(-1, F)).reshape(n, T, F)\n        Xtr = _scale(Xtr); Xva = _scale(Xva); Xte = _scale(Xte)\n\n    return {\n        \"train\": {\"X_seq\": Xtr, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte, \"y\": yte},\n        \"seq_features_used\": list(seq_feature_cols),\n        \"scalers\": {\"seq\": seq_scaler}\n    }\n\n# --------------------------\n# Models\n# --------------------------\ndef make_model(seq_len, n_seq_f, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x); x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x); x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x); x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x); x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    out = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=seq_in, outputs=out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# --------------------------\n# Threshold selection & evaluation\n# --------------------------\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx = np.where(mask)[0][int(np.nanargmax(scores[mask]))]\n        return float(thresholds[idx])\n    # fallback: best overall, then clamp\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max))\n\ndef pick_val_thresholds(y_val, p_val, thr_min=THR_MIN, thr_max=THR_MAX):\n    # Youden's J\n    try:\n        fpr, tpr, thr_roc = roc_curve(y_val, p_val)\n        youden = tpr - fpr\n        t_roc = _best_threshold_in_range(thr_roc, youden, thr_min, thr_max)\n    except Exception:\n        t_roc = 0.50\n    # PR-F1\n    try:\n        prec, rec, thr_pr = precision_recall_curve(y_val, p_val)\n        f1s = 2*prec[:-1]*rec[:-1] / (prec[:-1]+rec[:-1]+1e-9)\n        t_pr = _best_threshold_in_range(thr_pr, f1s, thr_min, thr_max)\n    except Exception:\n        t_pr = 0.50\n    return t_roc, t_pr\n\ndef eval_probs(y_true, y_prob, thresholds=(0.40, 0.50, 0.60)):\n    \"\"\"\n    Evaluate thresholded metrics at specified operating points, plus AUC/PR-AUC/Brier (threshold-free).\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_prob = np.asarray(y_prob).astype(float).ravel()\n\n    try:\n        roc = roc_auc_score(y_true, y_prob)\n    except Exception:\n        roc = np.nan\n    try:\n        pr  = average_precision_score(y_true, y_prob)\n    except Exception:\n        pr  = np.nan\n    brier = mean_squared_error(y_true, y_prob)\n\n    rows = []\n    for t in thresholds:\n        yhat = (y_prob >= float(t)).astype(int)\n        rows.append({\n            \"Threshold\": round(float(t), 2),\n            \"Accuracy\": accuracy_score(y_true, yhat),\n            \"F1_weighted\": f1_score(y_true, yhat, average=\"weighted\", zero_division=0),\n            \"Prec_1\": precision_score(y_true, yhat, pos_label=1, zero_division=0),\n            \"Recall_1\": recall_score(y_true, yhat, pos_label=1, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr, \"Brier\": brier\n        })\n    return pd.DataFrame(rows)\n\n# --------------------------\n# Core runner (one direction)\n# --------------------------\ndef run_cross_phase(train_window, test_window, out_prefix=\"ram_to_sha\"):\n    \"\"\"\n    train_window/test_window: (start_date, end_date)\n    \"\"\"\n    # A) Build hourly tables for both phases\n    hourly_train = build_hourly_for_window(CSV_INTRADAY_WITH_VISITS, *train_window)\n    hourly_test  = build_hourly_for_window(CSV_INTRADAY_WITH_VISITS, *test_window)\n\n    # B) Create patient splits: TRAIN/VAL within train phase; TEST from test phase (no overlap)\n    train_patients_all = sorted(hourly_train[\"patientID\"].unique().tolist())\n    test_patients_all  = sorted(hourly_test[\"patientID\"].unique().tolist())\n\n    # hold-out VAL from train phase (patient-wise)\n    rng = np.random.default_rng(RANDOM_STATE)\n    rng.shuffle(train_patients_all)\n    n_val = max(1, int(len(train_patients_all) * VAL_FRACTION))\n    val_patients = sorted(train_patients_all[:n_val])\n    tr_patients  = sorted(train_patients_all[n_val:])\n\n    # ensure cross-phase TEST patients don't overlap training phase patients\n    test_only = sorted(list(set(test_patients_all) - set(tr_patients) - set(val_patients)))\n    if len(test_only) == 0:\n        # fallback: allow all test phase patients (still leak-free wrt transforms, but same subjects across phases)\n        test_only = test_patients_all\n\n    hourly_train[\"Split\"] = np.where(hourly_train[\"patientID\"].isin(tr_patients), \"train\",\n                              np.where(hourly_train[\"patientID\"].isin(val_patients), \"val\", \"drop\"))\n    hourly_train = hourly_train[hourly_train[\"Split\"] != \"drop\"].copy()\n\n    hourly_test[\"Split\"]  = np.where(hourly_test[\"patientID\"].isin(test_only), \"test\", \"drop\")\n    hourly_test = hourly_test[hourly_test[\"Split\"] != \"drop\"].copy()\n\n    # C) Merge to single hourly dataframe with Split: train/val/test\n    hourly = pd.concat([hourly_train, hourly_test], ignore_index=True)\n\n    # D) Leak-safe PCA (fit on TRAIN only)\n    life_cols_present = [c for c in LIFESTYLE_COLS_CANDIDATES if c in hourly.columns]\n    hourly = apply_leak_safe_pca(hourly, lifestyle_cols=life_cols_present)\n\n    # E) Build sequences\n    data = build_sequences(hourly, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS)\n    Xtr, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"y\"]\n    Xva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"y\"]\n    Xte, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"y\"]\n\n    if any(arr is None or arr.size == 0 for arr in [Xtr, ytr, Xva, yva, Xte, yte]):\n        raise RuntimeError(\"Insufficient sequences in one of the splits. Check coverage or windows.\")\n\n    results_rows = []\n\n    # F) Train/evaluate per architecture\n    for arch in ARCH_LIST:\n        model = make_model(seq_len=Xtr.shape[1], n_seq_f=Xtr.shape[2], arch=arch, lr=1e-3)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=0)\n\n        hist = model.fit(Xtr, ytr,\n                         validation_data=(Xva, yva),\n                         epochs=12, batch_size=64,\n                         callbacks=[es],\n                         class_weight=make_class_weight(ytr),\n                         verbose=0)\n\n        # Predict\n        p_tr = model.predict(Xtr, verbose=0).ravel()\n        p_va = model.predict(Xva, verbose=0).ravel()\n        p_te = model.predict(Xte, verbose=0).ravel()\n\n        # Pick thresholds on VAL\n        t_roc, t_pr = pick_val_thresholds(yva, p_va, thr_min=THR_MIN, thr_max=THR_MAX)\n        eval_df = eval_probs(yte, p_te, thresholds=(THR_MIN, 0.50, THR_MAX, t_roc, t_pr))\n        eval_df.insert(0, \"Model\", arch)\n        eval_df.insert(1, \"Direction\", out_prefix)\n        results_rows.append(eval_df)\n\n    results = pd.concat(results_rows, ignore_index=True)\n    os.makedirs(\"/kaggle/working\", exist_ok=True)\n    out_csv = f\"/kaggle/working/crossphase_{out_prefix}.csv\"\n    results.to_csv(out_csv, index=False)\n    print(f\"âœ… Saved â†’ {out_csv}\")\n    return results\n\n# --------------------------\n# RUN BOTH DIRECTIONS\n# --------------------------\nramadan = (RAMADAN_START, RAMADAN_END)\nshawwal  = (SHAWWAL_START, SHAWWAL_END)\n\nres_ram_to_sha = run_cross_phase(ramadan, shawwal, out_prefix=\"Ramadan_to_Shawwal\")\nres_sha_to_ram = run_cross_phase(shawwal, ramadan, out_prefix=\"Shawwal_to_Ramadan\")\n\n# Combine + pivot for quick view\ncombined = pd.concat([res_ram_to_sha, res_sha_to_ram], ignore_index=True)\ncombined.to_csv(\"/kaggle/working/crossphase_combined.csv\", index=False)\n\n# Produce a compact comparison at VAL-optimized PR-F1 threshold (we pick the row with Threshold == t_pr ~= within [0.40,0.60])\ndef _pick_best_rows(df):\n    # We approximated two VAL-optimal thresholds (t_roc, t_pr) and included both in eval table.\n    # Here we pick the row with the highest F1_weighted per Model/Direction.\n    key_cols = [\"Direction\",\"Model\"]\n    best = (df.sort_values([\"Direction\",\"Model\",\"F1_weighted\"], ascending=[True, True, False])\n              .groupby(key_cols, as_index=False)\n              .head(1)\n              .reset_index(drop=True))\n    return best[[\"Direction\",\"Model\",\"Threshold\",\"ROC_AUC\",\"PR_AUC\",\"F1_weighted\",\"Recall_1\",\"Prec_1\",\"Brier\"]]\n\nsummary = _pick_best_rows(combined)\nsummary.to_csv(\"/kaggle/working/crossphase_summary_best.csv\", index=False)\nprint(\"ðŸ§¾ Summary (best per Model/Direction):\")\nprint(summary.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T08:12:22.334284Z","iopub.execute_input":"2025-10-29T08:12:22.334997Z","iopub.status.idle":"2025-10-29T08:21:08.911634Z","shell.execute_reply.started":"2025-10-29T08:12:22.334973Z","shell.execute_reply":"2025-10-29T08:21:08.910816Z"}},"outputs":[],"execution_count":null}]}