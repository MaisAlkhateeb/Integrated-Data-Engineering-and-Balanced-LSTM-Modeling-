{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13047461,"sourceType":"datasetVersion","datasetId":7421066},{"sourceId":268813348,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ‚úÖ 1. Define the libraries and upload the dataset","metadata":{}},{"cell_type":"code","source":"# Step 1: Create a new environment\n!python -m venv cleanenv\n\n# Step 2: Activate it\n# On Windows:\n!cleanenv\\Scripts\\activate\n# On Mac/Linux:\n#source cleanenv/bin/activate\n\n# Step 3: Install only what you need\n!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.0 imbalanced-learn==0.13.0 tensorflow==2.18.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:53:21.276016Z","iopub.execute_input":"2025-10-31T19:53:21.276712Z","iopub.status.idle":"2025-10-31T19:53:34.691167Z","shell.execute_reply.started":"2025-10-31T19:53:21.276687Z","shell.execute_reply":"2025-10-31T19:53:34.690091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GOOD (pick one)\nimport torch                         # PyTorch only\n# OR\nimport tensorflow as tf              # TensorFlow only\n# OR\nimport jax                           # JAX only","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:53:38.614675Z","iopub.execute_input":"2025-10-31T19:53:38.614967Z","iopub.status.idle":"2025-10-31T19:53:53.930148Z","shell.execute_reply.started":"2025-10-31T19:53:38.614943Z","shell.execute_reply":"2025-10-31T19:53:53.929553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import TF first so cuDNN is registered once\nimport tensorflow as tf\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:54:10.962130Z","iopub.execute_input":"2025-10-31T19:54:10.962725Z","iopub.status.idle":"2025-10-31T19:54:10.966192Z","shell.execute_reply.started":"2025-10-31T19:54:10.962700Z","shell.execute_reply":"2025-10-31T19:54:10.965278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy, scipy, sklearn, imblearn, tensorflow as tf\n\nprint(\"numpy:\", numpy.__version__)\nprint(\"scipy:\", scipy.__version__)\nprint(\"scikit-learn:\", sklearn.__version__)\nprint(\"imbalanced-learn:\", imblearn.__version__)\nprint(\"tensorflow:\", tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:54:16.669976Z","iopub.execute_input":"2025-10-31T19:54:16.670253Z","iopub.status.idle":"2025-10-31T19:54:16.881802Z","shell.execute_reply.started":"2025-10-31T19:54:16.670233Z","shell.execute_reply":"2025-10-31T19:54:16.880604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom sklearn.utils import resample\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:54:43.013453Z","iopub.execute_input":"2025-10-31T19:54:43.014150Z","iopub.status.idle":"2025-10-31T19:54:43.035234Z","shell.execute_reply.started":"2025-10-31T19:54:43.014124Z","shell.execute_reply":"2025-10-31T19:54:43.034395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This script creates **hourly-level dynamic features** for each patient during **Ramadan** using continuous glucose monitoring (CGM) data and lifestyle metrics (activity, sleep, physiology) from wearable devices.\nIt‚Äôs part of a preprocessing pipeline for modeling glucose behavior and hypoglycemia risk.\n\nHere‚Äôs a complete explanation üëá\n\n---\n\n## üß© **Overall Goal**\n\nTo transform raw timestamped CGM and wearable data into **hourly summarized features** that represent glucose dynamics, lifestyle behavior, and physiological activity during Ramadan ‚Äî ready for statistical or machine-learning analysis.\n\n---\n\n## üß≠ **1Ô∏è‚É£ Load and Parse Data**\n\n* Loads the file:\n\n  ```\n  intraday_with_visits.csv\n  ```\n\n  which includes per-minute or per-sample CGM and Huawei sensor data.\n* Converts all timestamps (`start`, `date`) to datetime format.\n* Extracts:\n\n  * `hour` ‚Üí the nearest hour (e.g., 14:00, 15:00).\n  * `hour_of_day` ‚Üí the hour index (0‚Äì23).\n\nüëâ *Purpose:* Prepare a unified hourly timeline for every patient.\n\n---\n\n## üìÜ **2Ô∏è‚É£ Filter for Ramadan Period**\n\n* Keeps only data between **March 22 ‚Äì April 19, 2023**.\n* Ensures the dataset includes `cgm` readings (continuous glucose values).\n* Adds a **binary flag `hypo`** = `True` when CGM ‚â§ 70 mg/dL (hypoglycemia reading).\n\nüëâ *Purpose:* Focus analysis strictly on the fasting month, removing other phases.\n\n---\n\n## ‚è± **3Ô∏è‚É£ Validate Hourly Windows**\n\n* Keeps only hours with **‚â•4 CGM readings** to ensure data quality.\n* This removes incomplete or sparse hours.\n\nüëâ *Purpose:* Guarantee each hourly feature represents stable glucose behavior.\n\n---\n\n## üìä **4Ô∏è‚É£ Compute Hourly CGM Statistics**\n\nFor each patient and hour:\n\n* `cgm_min` ‚Üí minimum glucose value\n* `cgm_max` ‚Üí maximum glucose value\n* `cgm_mean` ‚Üí mean glucose level\n* `cgm_std` ‚Üí standard deviation (glucose variability)\n\nAlso adds:\n\n* `hypo_label` ‚Üí `1` if any CGM reading in that hour was ‚â§70 mg/dL.\n\nüëâ *Purpose:* Capture both variability and hypoglycemia presence within each hour.\n\n---\n\n## üßÆ **5Ô∏è‚É£ Composite Glucose Features**\n\nCreates two derived indicators:\n\n* `cgm_mean_plus_std`  ‚Üí average + variability\n* `cgm_mean_minus_std` ‚Üí average ‚Äì variability\n\nüëâ *Purpose:* Encode range boundaries for dynamic glucose variation.\n\n---\n\n## üß† **6Ô∏è‚É£ PCA on CGM Variables**\n\n* Runs **Principal Component Analysis (PCA)** on `[cgm_min, cgm_max, cgm_mean, cgm_std]`.\n* Extracts **3 principal components** (`pca_cgm1`, `pca_cgm2`, `pca_cgm3`).\n* Reports explained variance (usually >95%).\n\nüëâ *Purpose:* Compress CGM dynamics into orthogonal, interpretable axes ‚Äî summarizing glucose pattern, amplitude, and variability.\n\n---\n\n## üèÉ‚Äç‚ôÄÔ∏è **7Ô∏è‚É£ PCA on Lifestyle / Activity / Sleep Features**\n\n* Selects available columns:\n\n  ```\n  steps, distance, calories, heart_rate, spo2, deep, light, rem, nap, awake\n  ```\n* Averages these per hour per patient.\n* Runs PCA ‚Üí extracts **3 lifestyle components**:\n\n  * `pc1_activity_energy` ‚Üí overall activity/energy output\n  * `pc2_physiology` ‚Üí physiological or heart-rate‚Äìrelated factors\n  * `pc3_sleep_rest` ‚Üí rest and sleep quality indices\n* Reports explained variance ratio.\n\nüëâ *Purpose:* Reduce multiple wearable signals into interpretable latent factors.\n\n---\n\n## üìë **8Ô∏è‚É£ Finalize and Sort**\n\n* Orders the dataset by patient and hour.\n* Keeps only relevant feature columns:\n\n  ```\n  cgm_min, cgm_max, cgm_mean, cgm_std,\n  cgm_mean_plus_std, cgm_mean_minus_std,\n  pca_cgm1‚Äì3, pc1_activity_energy, pc2_physiology, pc3_sleep_rest, hypo_label\n  ```\n* Prints a preview of the final dataset.\n\n---\n\n## üíæ **9Ô∏è‚É£ Save Hourly Feature File**\n\nExports the final hourly-level dataset to:\n\n```\n/kaggle/working/dynamic_hourly_features_ramadan.csv\n```\n\nEach row now represents **one patient-hour** with fully engineered glucose and lifestyle features.\n\n---\n\n## ‚úÖ **Summary in One Line**\n\n> This code aggregates intraday CGM and wearable sensor data into **hourly-level Ramadan features**, computing glucose statistics, detecting hypoglycemia, and summarizing glucose and lifestyle variability using **PCA-derived composite components** ‚Äî producing a clean, feature-rich dataset for modeling hourly glucose dynamics during fasting.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# =========================\n# CONFIG\n# =========================\nCSV_PATH = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"  # ‚úÖ update path if needed\nOUT_HOURLY_CSV = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\n\n# =========================\n# STEP 0: Load & prepare data\n# =========================\ndf = pd.read_csv(CSV_PATH)\n\n# Parse timestamps\ndf[\"start\"] = pd.to_datetime(df[\"start\"], errors=\"coerce\")\ndf[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\ndf[\"hour\"] = df[\"start\"].dt.floor(\"h\")\ndf[\"hour_of_day\"] = df[\"start\"].dt.hour\n\n# Numeric conversion\nfor col in df.columns:\n    if col not in [\"patientID\", \"huaweiID\", \"visit_assigned\", \"period_main\", \"start\", \"date\", \"hour\", \"hour_of_day\"]:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# =========================\n# STEP 0.1: Ramadan filter\n# =========================\ndf = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n# Ensure CGM exists\nif \"cgm\" not in df.columns:\n    raise ValueError(\"‚ùå Dataset must include 'cgm' column.\")\ndf_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n# Hypo reading flag (<= 70 mg/dL)\ndf_cgm[\"hypo\"] = df_cgm[\"cgm\"] <= 70\n\n# =========================\n# STEP 1: Filter valid hours (‚â•4 CGM readings)\n# =========================\nvalid_hours = (\n    df_cgm.groupby([\"patientID\", \"hour\"])\n    .filter(lambda g: g[\"cgm\"].notna().sum() >= 4)\n)\n\n# =========================\n# STEP 2: Compute hourly CGM statistics\n# =========================\nhourly_features = (\n    valid_hours\n    .groupby([\"patientID\", \"hour_of_day\", \"hour\"], as_index=False)\n    .agg(\n        cgm_min=(\"cgm\", \"min\"),\n        cgm_max=(\"cgm\", \"max\"),\n        cgm_mean=(\"cgm\", \"mean\"),\n        cgm_std=(\"cgm\", \"std\")\n    )\n)\n\n# Hypoglycemia label per hour\nhypo_per_hour = (\n    valid_hours.groupby([\"patientID\", \"hour\"])[\"cgm\"]\n    .apply(lambda x: (x < 70).any())\n    .reset_index(name=\"hypo_label\")\n)\nhourly_features = hourly_features.merge(hypo_per_hour, on=[\"patientID\", \"hour\"], how=\"left\")\n\n# =========================\n# STEP 3: Composite CGM features\n# =========================\nhourly_features[\"cgm_mean_plus_std\"] = hourly_features[\"cgm_mean\"] + hourly_features[\"cgm_std\"]\nhourly_features[\"cgm_mean_minus_std\"] = hourly_features[\"cgm_mean\"] - hourly_features[\"cgm_std\"]\n\n# =========================\n# STEP 4: PCA on CGM stats ‚Üí 3 components\n# =========================\npca_input_cgm = hourly_features[[\"cgm_min\", \"cgm_max\", \"cgm_mean\", \"cgm_std\"]].fillna(0)\npca_cgm = PCA(n_components=3, random_state=42)\ncgm_components = pca_cgm.fit_transform(pca_input_cgm)\n\nhourly_features[\"pca_cgm1\"] = cgm_components[:, 0]\nhourly_features[\"pca_cgm2\"] = cgm_components[:, 1]\nhourly_features[\"pca_cgm3\"] = cgm_components[:, 2]\n\nprint(\"CGM PCA explained variance:\", pca_cgm.explained_variance_ratio_.round(3))\n\n# =========================\n# STEP 5: PCA on lifestyle/activity/sleep features\n# =========================\nlifestyle_cols = [\"steps\", \"distance\", \"calories\", \"heart_rate\", \"spo2\",\n                  \"deep\", \"light\", \"rem\", \"nap\", \"awake\"]\nlifestyle_cols = [c for c in lifestyle_cols if c in df_cgm.columns]\n\nif lifestyle_cols:\n    lifestyle_hourly = (\n        df_cgm.groupby([\"patientID\", \"hour\"], as_index=False)[lifestyle_cols]\n        .mean()\n        .fillna(0)\n    )\n\n    # Merge lifestyle into hourly_features\n    hourly_features = hourly_features.merge(\n        lifestyle_hourly, on=[\"patientID\", \"hour\"], how=\"left\"\n    ).fillna(0)\n\n    # Run PCA\n    pca_life = PCA(n_components=3, random_state=42)\n    life_components = pca_life.fit_transform(hourly_features[lifestyle_cols])\n\n    hourly_features[\"pc1_activity_energy\"] = life_components[:, 0]\n    hourly_features[\"pc2_physiology\"] = life_components[:, 1]\n    hourly_features[\"pc3_sleep_rest\"] = life_components[:, 2]\n\n    print(\"Lifestyle PCA explained variance:\", pca_life.explained_variance_ratio_.round(3))\n\n# =========================\n# STEP 6: Finalize dataset\n# =========================\nhourly_features = hourly_features.sort_values([\"patientID\", \"hour\"]).reset_index(drop=True)\n\nDYNAMIC_FEATURES = [\n    \"cgm_min\", \"cgm_max\", \"cgm_mean\", \"cgm_std\",\n    \"cgm_mean_plus_std\", \"cgm_mean_minus_std\",\n    \"pca_cgm1\", \"pca_cgm2\", \"pca_cgm3\",\n    \"pc1_activity_energy\", \"pc2_physiology\", \"pc3_sleep_rest\"\n]\n\nprint(hourly_features[[\"patientID\", \"hour\"] + DYNAMIC_FEATURES + [\"hypo_label\"]].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T00:14:52.948272Z","iopub.execute_input":"2025-10-18T00:14:52.948568Z","iopub.status.idle":"2025-10-18T00:15:06.23163Z","shell.execute_reply.started":"2025-10-18T00:14:52.948549Z","shell.execute_reply":"2025-10-18T00:15:06.230766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Leak-safe All static visist and hourly Ramadan features + Balanced LSTM\n(hourly builder + sequences + training utilities)Below is a single, leak‚Äësafe end‚Äëto‚Äëend script that:\n\nRobustly detects patient/date/variable/value columns (handles headers like PatientID (Huawei Data)).\n\nSplits by patient first, then fits PCA & scalers on TRAIN only.\n\nBuilds sequences with optional per‚Äëpatient static features.\n\nTrains LSTM variants with class‚Äëweighted focal loss and optional resampling.\n\nChooses decision thresholds on the VALIDATION set (not the test set) to avoid peeking.\n\nEvaluates on test and writes plots + a summary CSV.\n\nWhat changed vs your last version\n\nAdded flexible column pickers (_pick_patient_col, _pick_date_col, ‚Ä¶) and used them everywhere (intraday, visit, static).\n\nThresholds now picked on VAL (Youden and PR‚ÄëF1) ‚Üí no test leakage.\n\nBalanced test creation returns X_stat_bal too (keeps seq+static aligned).\n\nResampling with SMOTE is skipped automatically when static input is enabled (can‚Äôt synthesize static safely).","metadata":{}},{"cell_type":"code","source":"# ==============================================\n# Leak-safe Ramadan features + Balanced LSTM\n# (hourly builder + sequences + training utilities)\n# ==============================================\nimport os, time, warnings, random, re\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\n# Paths (update for your environment)\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nSTATIC_CSV      = \"/kaggle/input/hmc-model-static-variables/outcome_static.csv\"\nVISIT_WIDE_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_wide_by_variable.csv\"\nVISIT_LONG_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_long.csv\"\n\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree.npz\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\nOUT_PLOTS_PNG   = \"/kaggle/working/combined_roc_pr_curves.png\"\n\n# Ramadan window and label definition\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\nHYPO_CUTOFF   = 70.0   # mg/dL\nMIN_CGM_PER_H = 4      # minimum CGM points within an hour to keep that hour\nSEQ_LEN       = 36     # sliding window length (hours)\n\n# Lifestyle candidates (if present in intraday_with_visits)\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\n# \"final master\" feature lists\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\n# Sequence features used for models (you can edit)\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",      # CGM core + CGM PCA#1\n    \"pc1_activity_energy\",                # lifestyle PCA#1 (0 if lifestyle missing)\n    \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"  # visit features\n)\n\n# Training config\nRANDOM_STATE     = 42\nTHR_MIN, THR_MAX = 0.40, 0.60\nAUGMENT_SIGMA    = 0.01   # small Gaussian jitter on train (set None to disable)\nRESAMPLE_METHODS = [\n    \"none\",            # baseline (class_weight + focal)\n    \"oversample_seq\",  # duplicate minority sequences\n    \"undersample_seq\", # downsample majority sequences\n    # SMOTE-family below only when no static input is used\n    \"smote\", \"smoteenn\", \"smotetomek\"\n]\nUSE_STATIC_INPUT = True  # set False to ignore static input entirely (enables SMOTE variants safely)\n\n# --------------------\n# Utilities (robust column picking)\n# --------------------\ndef set_global_seeds(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_global_seeds(RANDOM_STATE)\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef safe_encode_gender(series):\n    if series.dtype == \"object\":\n        return (series.str.strip().str.lower().map({\"male\":1, \"m\":1, \"female\":0, \"f\":0}))\n    return pd.to_numeric(series, errors=\"coerce\")\n\ndef split_patients(unique_pids, test_size=0.3, val_size=0.1, random_state=RANDOM_STATE):\n    train_pids, test_pids = train_test_split(unique_pids, test_size=test_size, random_state=random_state)\n    val_fraction = val_size / max(1e-9, (1.0 - test_size))\n    train_pids, val_pids = train_test_split(train_pids, test_size=val_fraction, random_state=random_state)\n    return np.array(train_pids), np.array(val_pids), np.array(test_pids)\n\ndef _normalize_date(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    return s.dt.normalize()\n\n# ---- robust column pickers ----\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef _pick_col_flex(\n    df: pd.DataFrame,\n    preferred=None,\n    required=False,\n    name=\"\",\n    must_contain_all=None,\n    any_contains=None,\n):\n    cols = list(df.columns)\n    norm_map = {c: _norm_col(c) for c in cols}\n\n    # (1) exact by case-insensitive preferred\n    if preferred:\n        lower_pref = {str(p).strip().lower(): p for p in preferred}\n        for c in cols:\n            if str(c).strip().lower() in lower_pref:\n                return c\n\n    # (2) exact by normalized preferred\n    if preferred:\n        pref_norm = {_norm_col(p): p for p in preferred}\n        for c, n in norm_map.items():\n            if n in pref_norm:\n                return c\n\n    # (3) heuristics on normalized names\n    cands = []\n    for c, n in norm_map.items():\n        ok = True\n        if must_contain_all:\n            for tok in must_contain_all:\n                if _norm_col(tok) not in n:\n                    ok = False\n                    break\n        if ok and any_contains:\n            if not any(_norm_col(tok) in n for tok in any_contains):\n                ok = False\n        if ok:\n            cands.append(c)\n    if cands:\n        # prioritize names starting with 'patientid' when looking for patient column\n        def _priority(col: str):\n            n = norm_map[col]\n            starts_pid = n.startswith(\"patientid\")\n            has_pid    = \"patientid\" in n\n            return (-(starts_pid or has_pid), len(n))\n        cands.sort(key=_priority)\n        return cands[0]\n\n    if required:\n        raise KeyError(\n            f\"Required column not found for {name}. \"\n            f\"preferred={preferred} | must_contain_all={must_contain_all} | any_contains={any_contains}. \"\n            f\"Available: {cols}\"\n        )\n    return None\n\ndef _pick_patient_col(df: pd.DataFrame) -> str:\n    preferred = [\"patientID\", \"patientId\", \"PatientID (Huawei Data)\", \"subject_id\", \"patid\", \"pid\", \"id\", \"huaweiid\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"patientID\",\n                          must_contain_all=[\"id\"], any_contains=[\"patient\",\"subject\",\"pat\",\"huawei\"])\n\ndef _pick_date_col(df: pd.DataFrame) -> str:\n    preferred = [\"date\", \"visit_date\", \"Date\", \"day\", \"timestamp\", \"Visit Date\", \"date_of_visit\", \"start\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"date\",\n                          any_contains=[\"date\",\"visit\",\"day\",\"timestamp\",\"start\"])\n\ndef _pick_variable_col(df: pd.DataFrame) -> str:\n    preferred = [\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"variable\",\n                          any_contains=[\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"])\n\ndef _pick_value_col(df: pd.DataFrame) -> str:\n    preferred = [\"value\",\"val\",\"measure_value\",\"reading\",\"amount\",\"score\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"value\",\n                          any_contains=[\"value\",\"val\",\"measurevalue\",\"reading\",\"amount\",\"score\"])\n\n# ---------------------------\n# Loaders for external files\n# ---------------------------\ndef load_static_df(static_csv=STATIC_CSV, needed=STATIC_COLS):\n    if not static_csv or not os.path.exists(static_csv):\n        print(\"‚ö†Ô∏è Static CSV not found; static features will be zero-filled.\")\n        return None\n    df = pd.read_csv(static_csv)\n    pid_col = _pick_patient_col(df)\n    df = df.rename(columns={pid_col:\"patientID\"})\n    keep = [\"patientID\"] + [c for c in needed if c in df.columns]\n    df = df[keep].drop_duplicates(subset=[\"patientID\"]).copy()\n    if \"Gender\" in df.columns:\n        df[\"Gender\"] = safe_encode_gender(df[\"Gender\"])\n    for c in keep:\n        if c != \"patientID\":\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    print(f\"‚ÑπÔ∏è static: using patientID column = '{pid_col}'\")\n    return df\n\ndef load_visit_df(visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV, needed=VISIT_COLS):\n    # Try wide first\n    if visit_wide_csv and os.path.exists(visit_wide_csv):\n        wide = pd.read_csv(visit_wide_csv)\n        pid_col  = _pick_patient_col(wide)\n        date_col = _pick_date_col(wide)\n        wide = wide.rename(columns={pid_col:\"patientID\", date_col:\"date\"})\n        wide[\"date\"] = _normalize_date(wide[\"date\"])\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"‚ÑπÔ∏è visit-wide: patientID='{pid_col}', date='{date_col}', kept={keep[2:]}\")\n            return wide[keep].copy()\n        else:\n            print(\"‚ö†Ô∏è VISIT_WIDE_CSV found but none of the needed visit columns present; will try LONG if available.\")\n\n    # Fallback: long -> pivot\n    if visit_long_csv and os.path.exists(visit_long_csv):\n        long = pd.read_csv(visit_long_csv)\n        pid_col   = _pick_patient_col(long)\n        date_col  = _pick_date_col(long)\n        var_col   = _pick_variable_col(long)\n        value_col = _pick_value_col(long)\n        long = long.rename(columns={pid_col:\"patientID\", date_col:\"date\", var_col:\"variable\", value_col:\"value\"})\n        long[\"date\"] = _normalize_date(long[\"date\"])\n        wide = (long\n                .pivot_table(index=[\"patientID\",\"date\"], columns=\"variable\", values=\"value\", aggfunc=\"mean\")\n                .reset_index())\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"‚ÑπÔ∏è visit-long: patientID='{pid_col}', date='{date_col}', variables matched={keep[2:]}\")\n            return wide[keep].copy()\n        print(\"‚ö†Ô∏è VISIT_LONG_CSV present but none of the needed variables were found in the pivot.\")\n\n    print(\"‚ö†Ô∏è No usable visit CSVs found; visit features will be zero-filled.\")\n    return None\n\n# ----------------------------------------------------------------\n# Part A ‚Äî Build hourly Ramadan features and leak‚Äësafe transforms\n# ----------------------------------------------------------------\ndef build_hourly_features_with_leak_safe_transforms(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.3, val_size=0.1, random_state=RANDOM_STATE,\n    static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    # Load & parse intraday\n    df = pd.read_csv(in_csv)\n\n    # Robust patient column for intraday too\n    if \"patientID\" not in df.columns:\n        pid_col = _pick_patient_col(df)\n        df = df.rename(columns={pid_col: \"patientID\"})\n        print(f\"‚ÑπÔ∏è intraday: using patientID column = '{pid_col}'\")\n\n    # timestamps\n    start_col = \"start\" if \"start\" in df.columns else _pick_date_col(df)\n    df[start_col] = to_dt(df[start_col])\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[start_col].dt.date)\n    df[\"hour\"]       = df[start_col].dt.floor(\"h\")\n    df[\"hour_of_day\"]= df[\"hour\"].dt.hour\n\n    df = ensure_numeric(df)\n\n    # Ramadan filter\n    df = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n    # Require CGM\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"‚ùå Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # Valid hourly windows\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # Base hourly CGM stats\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(\n                       cgm_min=(\"cgm\",\"min\"),\n                       cgm_max=(\"cgm\",\"max\"),\n                       cgm_mean=(\"cgm\",\"mean\"),\n                       cgm_std=(\"cgm\",\"std\")\n                   )\n                   .sort_values([\"patientID\",\"hour\"])\n                   .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    # Hypo label\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # Lifestyle hourly means (if present)\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean().fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # Composite CGM features\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # Patient-level split (NO LEAK)\n    pids = hourly[\"patientID\"].dropna().unique()\n    train_p, val_p, test_p = split_patients(pids, test_size=test_size, val_size=val_size, random_state=random_state)\n    hourly[\"Split\"] = np.where(hourly[\"patientID\"].isin(train_p), \"train\",\n                        np.where(hourly[\"patientID\"].isin(val_p), \"val\", \"test\"))\n\n    # CGM PCA fit on TRAIN only\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"] == \"train\"\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    def _apply_cgm_pca(df_in):\n        X = scal_cgm.transform(df_in[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df_in.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n    hourly = _apply_cgm_pca(hourly)\n\n    # Lifestyle PCA fit on TRAIN only\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        pca_life  = PCA(n_components=3, random_state=random_state).fit(\n            scal_life.transform(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        )\n        X_all = scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        Z_all = pca_life.transform(X_all)\n        hourly[\"pc1_activity_energy\"] = Z_all[:,0]\n        hourly[\"pc2_physiology\"]      = Z_all[:,1]\n        hourly[\"pc3_sleep_rest\"]      = Z_all[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    # Merge VISIT features (daily)\n    visit_df = load_visit_df(visit_wide_csv, visit_long_csv, VISIT_COLS)\n    hourly[\"date\"] = hourly[\"hour\"].dt.normalize()\n    if visit_df is not None:\n        visit_df[\"date\"] = pd.to_datetime(visit_df[\"date\"], errors=\"coerce\").dt.normalize()\n        visit_df = visit_df[(visit_df[\"date\"] >= RAMADAN_START) & (visit_df[\"date\"] <= RAMADAN_END)].copy()\n        hourly = hourly.merge(visit_df, on=[\"patientID\",\"date\"], how=\"left\")\n    for c in VISIT_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    # Merge STATIC features (per patient)\n    static_df = load_static_df(static_csv, STATIC_COLS)\n    if static_df is not None:\n        hourly = hourly.merge(static_df, on=\"patientID\", how=\"left\")\n    for c in STATIC_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    # Save hourly table\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly.to_csv(out_csv, index=False)\n    print(f\"‚úÖ Saved hourly features (leak-safe) ‚Üí {out_csv}\")\n\n    return hourly, (train_p, val_p, test_p)\n\n# ---------------------------------------------------------------\n# Part B ‚Äî Build sequences (optionally with per-patient static)\n# ---------------------------------------------------------------\ndef build_sequences_by_split(\n    hourly, splits, seq_len=SEQ_LEN,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    scale_features=True\n):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float).fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n                if static_mat is not None and pid in static_mat.index:\n                    X_stat.append(static_mat.loc[pid].values.astype(float))\n        X_seq = np.array(X_seq); y = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    # Scale sequence features (fit on TRAIN only), and static (fit on TRAIN only)\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size==0: return X\n            n = X.shape[0]; return seq_scaler.transform(X.reshape(-1, n_f)).reshape(n, SEQ_LEN, n_f)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size>0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size==0: return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"‚úÖ Sequences built | train={Xtr_s.shape}, val={Xva_s.shape}, test={Xte_s.shape}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ------------------------------------------------------\n# Balanced LSTM pipeline utilities (metrics, resampling)\n# ------------------------------------------------------\nTHR_MIN, THR_MAX = THR_MIN, THR_MAX  # keep constants visible here\n\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask])); idx = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.8):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_overall(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef _specificity_per_class(y_true, y_pred, positive_label):\n    y_true_bin = (np.asarray(y_true).ravel() == positive_label).astype(int)\n    y_pred_bin = (np.asarray(y_pred).ravel() == positive_label).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_per_class(y_true, y_pred, positive_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    metrics[\"Overall/Specificity\"]          = _specificity_overall(y_true, y_pred)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/MSE_pred\"]             = mse_pred\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/MSE_prob\"]       = mse_prob\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/MSE_prob\"] = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n    return metrics\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\ndef augment_with_static(X_seq, X_stat, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0:\n        return X_seq, X_stat, y\n    noise = np.random.normal(0, sigma, X_seq.shape)\n    X_seq_aug = np.vstack([X_seq, X_seq + noise])\n    y_aug     = np.hstack([y, y])\n    if X_stat is not None:\n        X_stat_aug = np.vstack([X_stat, X_stat])\n    else:\n        X_stat_aug = None\n    return X_seq_aug, X_stat_aug, y_aug\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE, return_index=False, allow_smote=True):\n    \"\"\"\n    Sequence-level resampling. If return_index=True, also returns the index mapping used so\n    static inputs can be resampled consistently. For SMOTE-family, index mapping isn't\n    meaningful; we disable SMOTE if allow_smote=False.\n    \"\"\"\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n    base_idx = np.arange(n)\n\n    if method == \"none\":\n        return (X, y, base_idx) if return_index else (X, y)\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        rng = np.random.default_rng(random_state)\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0:\n            return (X, y, base_idx) if return_index else (X, y)\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = rng.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = rng.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:  # undersample\n            if n0 > n1:\n                keep0 = rng.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = rng.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        rng.shuffle(keep)\n        Xr, yr = X[keep], y[keep]\n        return (Xr, yr, keep) if return_index else (Xr, yr)\n\n    # SMOTE family\n    if not allow_smote:\n        print(f\"‚ö†Ô∏è {method} disabled when static input is used; falling back to 'none'.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    minority_n = int((y==1).sum())\n    majority_n = int((y==0).sum())\n    if minority_n < 2 or majority_n < 2:\n        print(\"‚ö†Ô∏è Not enough samples for SMOTE/SMOTEENN/SMOTETomek; skipping resampling.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    Xf = X.reshape(n, -1)\n    if method == \"smote\":\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n\n    Xr = Xr.reshape(-1, T, F)\n    return (Xr, yr, None) if return_index else (Xr, yr)\n\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state=RANDOM_STATE):\n    \"\"\"Return balanced (by label) subsets of X_test, y_test, and X_stat (if given).\"\"\"\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0: \n        return (X_test, y_test, (None if X_stat is None else X_stat))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ------------------------------------------------------\n# Model builders (supports seq-only or seq+static)\n# ------------------------------------------------------\ndef make_model(seq_len, n_seq_f, n_stat_f=0, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n        x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x)\n        x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x);                    x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L1\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l1(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l1(1e-5))(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x);                          x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(32, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(32, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(32, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# ------------------------------------------------------\n# Training runner (VAL for threshold; TEST for final)\n# ------------------------------------------------------\ndef run_balanced_lstm_pipeline(data,\n                               arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n                               resample_methods=RESAMPLE_METHODS,\n                               thr_min=THR_MIN, thr_max=THR_MAX,\n                               random_state=RANDOM_STATE,\n                               results_csv=OUT_RESULTS_CSV,\n                               plots_png=OUT_PLOTS_PNG):\n    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n    os.makedirs(os.path.dirname(plots_png), exist_ok=True)\n    os.makedirs(\"checkpoints\", exist_ok=True)\n\n    Xtr, Xtr_stat, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"], data[\"train\"][\"y\"]\n    Xva, Xva_stat, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"X_stat\"],   data[\"val\"][\"y\"]\n    Xte, Xte_stat, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"X_stat\"],  data[\"test\"][\"y\"]\n\n    # Augment train (and static if present)\n    Xtr, Xtr_stat, ytr = augment_with_static(Xtr, Xtr_stat, ytr, sigma=AUGMENT_SIGMA)\n\n    # Balanced copy of test for diagnostic plots\n    Xte_bal, yte_bal, Xte_stat_bal = make_balanced_test(Xte, yte, X_stat=Xte_stat)\n\n    results     = {}\n    roc_curves  = {}\n    pr_curves   = {}\n\n    allow_smote = (Xtr_stat is None or not USE_STATIC_INPUT)\n\n    def train_eval_one(method_name, arch_name):\n        nonlocal Xtr, ytr, Xtr_stat\n\n        # Resample TRAIN only\n        Xrs, yrs, idx_map = seq_resample(Xtr, ytr, method=method_name, random_state=random_state,\n                                         return_index=True, allow_smote=allow_smote)\n        if Xtr_stat is not None and USE_STATIC_INPUT:\n            if idx_map is None:\n                # SMOTE chosen while static present ‚Üí already disabled in seq_resample\n                Xrs_stat = Xtr_stat\n            else:\n                Xrs_stat = Xtr_stat[idx_map]\n        else:\n            Xrs_stat = None\n\n        # Build model\n        seq_len, n_seq_f = Xrs.shape[1], Xrs.shape[2]\n        n_stat_f = 0 if (Xrs_stat is None or not USE_STATIC_INPUT) else Xrs_stat.shape[1]\n        model = make_model(seq_len, n_seq_f, n_stat_f=n_stat_f, arch=arch_name, lr=1e-3)\n\n        # Fit with VAL for early stopping (no peeking at test)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n        ckpt_path = f\"checkpoints/{method_name}__{arch_name}.h5\"\n        cp = ModelCheckpoint(ckpt_path, save_best_only=True, monitor=\"val_loss\", verbose=0)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xrs, Xrs_stat], yrs,\n                      validation_data=([Xva, Xva_stat], yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict([Xrs, Xrs_stat], verbose=0).ravel()\n            p_va  = model.predict([Xva, Xva_stat], verbose=0).ravel()\n            p_te  = model.predict([Xte, Xte_stat], verbose=0).ravel()\n            p_teB = model.predict([Xte_bal, Xte_stat_bal], verbose=0).ravel() if Xte_stat_bal is not None else model.predict(Xte_bal, verbose=0).ravel()\n        else:\n            model.fit(Xrs, yrs,\n                      validation_data=(Xva, yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict(Xrs, verbose=0).ravel()\n            p_va  = model.predict(Xva, verbose=0).ravel()\n            p_te  = model.predict(Xte, verbose=0).ravel()\n            p_teB = model.predict(Xte_bal, verbose=0).ravel()\n\n        # ---------- choose thresholds on VALIDATION (not TEST) ----------\n        try:\n            fpr_va, tpr_va, thr_roc_va = roc_curve(yva, p_va); auc_roc = auc(fpr_va, tpr_va)\n        except ValueError:\n            fpr_va, tpr_va, thr_roc_va, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n        youden_va = tpr_va - fpr_va\n        t_roc, _ = _best_threshold_in_range(thr_roc_va, youden_va, thr_min, thr_max)\n\n        prec_va, rec_va, thr_pr_va = precision_recall_curve(yva, p_va)\n        f1s_va = 2*prec_va[:-1]*rec_va[:-1] / (prec_va[:-1]+rec_va[:-1]+1e-8)\n        t_pr, _ = _best_threshold_in_range(thr_pr_va, f1s_va, thr_min, thr_max)\n        ap_val  = average_precision_score(yva, p_va)\n\n        # Curves (validation-based AUC/AP shown)\n        roc_curves[(method_name, arch_name)] = (fpr_va, tpr_va, auc_roc)\n        pr_curves[(method_name, arch_name)]  = (rec_va, prec_va, ap_val)\n        print(f\"üìå [{method_name}/{arch_name}] VAL thresholds ‚Üí Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{thr_min},{thr_max}])\")\n\n        eval_ts = sorted(set([thr_min, 0.50, thr_max, float(t_roc), float(t_pr)]))\n\n        # Evaluate at thresholds: TRAIN / VAL / TEST / TEST (balanced)\n        for t in eval_ts:\n            yhat_tr  = (p_tr  >= t).astype(int)\n            yhat_va  = (p_va  >= t).astype(int)\n            yhat_te  = (p_te  >= t).astype(int)\n            yhat_teB = (p_teB >= t).astype(int)\n\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__train\"]        = evaluate_full_metrics(yrs,  yhat_tr,  p_tr)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__val\"]          = evaluate_full_metrics(yva,  yhat_va,  p_va)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__test\"]         = evaluate_full_metrics(yte,  yhat_te,  p_te)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__testBalanced\"] = evaluate_full_metrics(yte_bal, yhat_teB, p_teB)\n\n    # Loop: resampling methods √ó architectures\n    for METHOD in resample_methods:\n        if METHOD in {\"smote\",\"smoteenn\",\"smotetomek\"} and (data[\"train\"][\"X_stat\"] is not None and USE_STATIC_INPUT):\n            print(f\"‚è≠Ô∏è  Skipping {METHOD} (static input enabled).\")\n            continue\n        print(f\"\\nüîÅ Resampling: {METHOD} | train y-dist = {Counter(data['train']['y'])}\")\n        for ARCH in arch_list:\n            train_eval_one(METHOD, ARCH)\n\n    # --- Plots (validation curves)\n    plt.figure(figsize=(14,6))\n    # ROC\n    plt.subplot(1,2,1)\n    for (meth, arch), (fpr, tpr, auc_roc) in roc_curves.items():\n        plt.plot(fpr, tpr, label=f'{meth}/{arch} (VAL AUC={auc_roc:.3f})')\n    plt.plot([0,1],[0,1],'--',label='Random')\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (Validation)'); plt.legend(fontsize=8)\n    # PR\n    plt.subplot(1,2,2)\n    for (meth, arch), (rec, prec, ap) in pr_curves.items():\n        plt.plot(rec, prec, label=f'{meth}/{arch} (VAL AP={ap:.3f})')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR (Validation)'); plt.legend(fontsize=8)\n    plt.tight_layout(); plt.savefig(plots_png, dpi=300); plt.show()\n    print(f\"üñºÔ∏è Saved plots ‚Üí {plots_png}\")\n\n    # --- Results CSV\n    results_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"Key\"})\n    k = results_df[\"Key\"].str.strip()\n    results_df[\"Split\"]  = np.where(k.str.endswith(\"__train\"), \"train\",\n                             np.where(k.str.endswith(\"__val\"), \"val\",\n                             np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                             np.where(k.str.endswith(\"__test\"), \"test\", np.nan))))\n    parts = k.str.split(\"__\")\n    results_df[\"Method\"]    = parts.str[0]\n    results_df[\"Model\"]     = parts.str[1]\n    results_df[\"Threshold\"] = pd.to_numeric(parts.str[2].str.replace(\"thr_\",\"\", regex=False), errors=\"coerce\")\n    results_df.round(6).to_csv(results_csv, index=False)\n    print(f\"üìë Saved results ‚Üí {results_csv}\")\n\n    return results_df\n\n# --------------------\n# Run end-to-end\n# --------------------\nif __name__ == \"__main__\":\n    # A) Hourly features with leak‚Äësafe PCA & merges\n    hourly, splits = build_hourly_features_with_leak_safe_transforms(\n        in_csv=CSV_INTRADAY_WITH_VISITS,\n        out_csv=OUT_HOURLY_CSV,\n        min_cgm_per_hour=MIN_CGM_PER_H,\n        test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n        static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n    )\n\n    # B) Sequences with visit + static (no leakage; scalers fit on TRAIN only)\n    data = build_sequences_by_split(\n        hourly, splits,\n        seq_len=SEQ_LEN,\n        seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n        static_cols=STATIC_COLS,\n        scale_features=True\n    )\n\n    # (Optional) Save arrays for later reuse\n    np.savez_compressed(\n        OUT_SEQ_NPZ,\n        Xtr=data[\"train\"][\"X_seq\"],  Xtr_stat=(data[\"train\"][\"X_stat\"] if data[\"train\"][\"X_stat\"] is not None else np.empty((0,0))),\n        ytr=data[\"train\"][\"y\"],\n        Xva=data[\"val\"][\"X_seq\"],    Xva_stat=(data[\"val\"][\"X_stat\"] if data[\"val\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yva=data[\"val\"][\"y\"],\n        Xte=data[\"test\"][\"X_seq\"],   Xte_stat=(data[\"test\"][\"X_stat\"] if data[\"test\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yte=data[\"test\"][\"y\"],\n        seq_features_used=np.array(data[\"seq_features_used\"], dtype=object),\n        static_features_used=np.array(data[\"static_features_used\"], dtype=object)\n    )\n    print(f\"üíæ Saved sequences ‚Üí {OUT_SEQ_NPZ}\")\n\n    # C) Train + evaluate (leak-safe thresholds chosen on VAL)\n    results_df = run_balanced_lstm_pipeline(\n        data,\n        arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n        resample_methods=RESAMPLE_METHODS,\n        thr_min=THR_MIN, thr_max=THR_MAX,\n        random_state=RANDOM_STATE,\n        results_csv=OUT_RESULTS_CSV,\n        plots_png=OUT_PLOTS_PNG\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T19:44:25.008078Z","iopub.execute_input":"2025-10-31T19:44:25.008813Z","iopub.status.idle":"2025-10-31T19:44:25.108157Z","shell.execute_reply.started":"2025-10-31T19:44:25.008790Z","shell.execute_reply":"2025-10-31T19:44:25.106960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here‚Äôs a concise read‚Äëout of the results you uploaded in **`/mnt/data/results_summary_all.csv`** and what they mean in practice.\n\n---\n\n## What I analyzed\n\nFrom your results file I looked at, per split:\n\n* **Resampling method**: `none`, `oversample_seq`, `undersample_seq`\n* **Model**: `LSTM_100`, `BiLSTM`, `LSTM_50`\n* **Thresholds** evaluated: 0.40, 0.50, 0.60 (and the full probability curves to compute ROC‚ÄëAUC/PR‚ÄëAUC)\n* **Metrics**: Overall Accuracy/F1 (macro & weighted), per‚Äëclass Precision/Recall/F1/Specificity, plus ROC‚ÄëAUC and PR‚ÄëAUC\n\nSplits present in the file: **train**, **test**, and a **testBalanced** (your balanced diagnostic set with 1:1 class ratio).\n\n---\n\n##  Best practical setting\n\nIf the goal is **reliably catching hypoglycemia (Class‚ÄØ1)** with strong precision:\n\n* **Resampling**: `oversample_seq`\n* **Model**: `LSTM_100`\n* **Decision threshold**: **0.50**\n\n**On the balanced test set (diagnostic 1:1):**\n\n* **Weighted F1**: **0.801**\n* **Accuracy**: 0.801\n* **Class‚ÄØ1 (hypo) ‚Äî Precision**: **0.956**\n* **Class‚ÄØ1 ‚Äî Recall (Sensitivity)**: **0.642**\n* **ROC‚ÄëAUC**: 0.906\n* **PR‚ÄëAUC**: 0.903 *(note: PR‚ÄëAUC depends on prevalence; this is the balanced view)*\n\n**Confusion matrix (balanced test, approximate counts):**\n\n|            |       Pred 0 |      Pred 1 |\n| ---------- | -----------: | ----------: |\n| **True 0** | **TN‚ÄØ=‚ÄØ130** |  **FP‚ÄØ=‚ÄØ4** |\n| **True 1** |  **FN‚ÄØ=‚ÄØ48** | **TP‚ÄØ=‚ÄØ86** |\n\nThat‚Äôs **very few false alarms** (FP) while catching ~64% of hypos.\n\n**If you want a bit more sensitivity** (catch more hypos) and can tolerate a few more false positives, use **threshold‚ÄØ=‚ÄØ0.40** for the same model/setup:\n\n* Class‚ÄØ1 **Recall** ‚Üë to **0.664**\n* Class‚ÄØ1 **Precision** ‚Üí 0.935\n* Weighted F1 ‚âà 0.799\n* Confusion matrix (balanced): **TN‚ÄØ126, FP‚ÄØ8, FN‚ÄØ45, TP‚ÄØ89**\n\n---\n\n## Why the ‚Äúoriginal test‚Äù looks deceptively great\n\nOn the **original (imbalanced) test**, the top rows (e.g., `none / LSTM_100 / thr‚ÄØ0.60`) show **very high weighted F1 (‚âà0.85‚Äì0.96)** but **very low Class‚ÄØ1 recall (~0.13‚Äì0.37)**. That‚Äôs because the dataset is dominated by Class‚ÄØ0, so a model that predicts negatives most of the time can look ‚Äúgreat‚Äù overall while **missing most hypos**. This is a classic class‚Äëimbalance effect.\n\nThat‚Äôs why your **testBalanced** view is important: it reveals how well the model actually detects positives.\n\n---\n\n## Method & architecture comparison (balanced test)\n\nTop performer per **resampling method** (sorted by Weighted F1):\n\n1. **oversample_seq + LSTM_100, thr‚ÄØ0.50**\n\n   * Weighted F1 **0.801**, Acc 0.801, **Precision‚ÇÅ 0.956**, **Recall‚ÇÅ 0.642**\n2. **undersample_seq + BiLSTM, thr‚ÄØ0.40**\n\n   * Weighted F1 0.792, Acc 0.792, Precision‚ÇÅ 0.860, **Recall‚ÇÅ 0.687** *(best sensitivity among the top)*\n3. **none + LSTM_50, thr‚ÄØ0.50**\n\n   * Weighted F1 0.740, Acc 0.740, Precision‚ÇÅ 0.831, Recall‚ÇÅ 0.642\n\n**Takeaway:**\n\n* **`LSTM_100`** is consistently the strongest backbone.\n* **Oversampling** improves **precision while retaining good recall**; **undersampling** nudges recall highest (but with more false alarms).\n* **No resampling** underperforms for the positive class.\n\n---\n\n## AUC perspective (threshold‚Äëfree)\n\n* For **oversample_seq + LSTM_100**:\n\n  * **ROC‚ÄëAUC (original test)**: ~**0.886**\n  * **PR‚ÄëAUC (original test)**: ~**0.336** (low due to class rarity; typical)\n  * **ROC‚ÄëAUC (balanced test)**: ~**0.906**\n  * **PR‚ÄëAUC (balanced test)**: ~**0.903** *(inflated by 50% prevalence; use for diagnostics only)*\n\nThe ROC‚ÄëAUCs are stable and indicate a **strong ranking ability**. PR‚ÄëAUC on the original test is more honest about the difficulty of the rare positives.\n\n---\n\n## Generalization check (same method/model/threshold across splits)\n\nFor **oversample_seq + LSTM_100 @ thr‚ÄØ0.50**:\n\n* **Train** Weighted F1 ‚âà **0.972**\n* **Val** Weighted F1 ‚âà **0.946**\n* **Test (original)** Weighted F1 ‚âà **0.950**\n* **TestBalanced** Weighted F1 ‚âà **0.801**\n\nThe drop on **testBalanced** is expected because the class prior is forced to 50/50; it does **not** indicate overfitting. Train/Val/Test are tightly aligned.\n\n---\n\n## Recommendations\n\n1. **Deploy default:** `oversample_seq + LSTM_100` with **threshold‚ÄØ=‚ÄØ0.50**\n\n   * Great precision on hypos (few false alarms) with reasonable sensitivity.\n\n2. **Sensitivity mode:** set **threshold‚ÄØ=‚ÄØ0.40**\n\n   * Use when **missing a hypo is costlier** than an extra false alert.\n\n3. **If you want even more recall**, consider `undersample_seq + BiLSTM @ thr‚ÄØ0.40` (Recall‚ÇÅ ‚âà **0.687** on balanced test), but expect more false positives.\n\n4. **Calibrate to clinical preference:** You can choose threshold by optimizing **FŒ≤** (e.g., Œ≤=2 for recall‚Äëheavy) on the **validation set**, then lock that threshold for the test/deployment.\n\n5. **Next steps to squeeze more recall without losing precision:**\n\n   * Try adding **pca_cgm2/3** and **hour_of_day** to sequence features.\n   * Small **temporal dropout/label smoothing** to stabilize.\n   * **Patient‚Äëgrouped CV** to confirm robustness.\n   * **Threshold per risk period** (e.g., nocturnal vs daytime) using hour‚Äëof‚Äëday.\n\n---\n\n### Quick reference (best configurations)\n\n* **Best balanced overall**: `oversample_seq / LSTM_100 / thr=0.50`\n  **Weighted F1 0.801 ¬∑ Acc 0.801 ¬∑ Prec‚ÇÅ 0.956 ¬∑ Rec‚ÇÅ 0.642 ¬∑ ROC‚ÄëAUC 0.906 ¬∑ PR‚ÄëAUC 0.903**\n\n* **More recall**: `oversample_seq / LSTM_100 / thr=0.40`\n  **Weighted F1 0.799 ¬∑ Acc 0.799 ¬∑ Prec‚ÇÅ 0.935 ¬∑ Rec‚ÇÅ 0.664**\n\n* **Highest recall among top‚Äë2**: `undersample_seq / BiLSTM / thr=0.40`\n  **Weighted F1 0.792 ¬∑ Acc 0.792 ¬∑ Prec‚ÇÅ 0.860 ¬∑ Rec‚ÇÅ 0.687**\n\nIf you want, I can generate a compact leaderboard table (or plots) from this file showing the top N runs for each split and highlight the trade‚Äëoffs between precision and recall.\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# Results analysis & reporting\n# ============================\nimport os\nimport io\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom sklearn.metrics import (\n    confusion_matrix, roc_curve, precision_recall_curve, auc, average_precision_score\n)\n\nimport tensorflow as tf\n\n# ---------- Configuration ----------\nRESULTS_CSV_CANDIDATES = [\n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\nSEQ_NPZ_CANDIDATES = [\n    \"/kaggle/working/sequences_leakfree.npz\",\n    \"sequences_leakfree.npz\"\n]\nCHECKPOINT_DIR = \"checkpoints\"  # expects files like: checkpoints/{Method}__{Model}.h5\n\n# Your canonical visit/static lists (for feature breakdown)\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n# Which methods to summarize\nMETHODS_FOR_TOP5 = [\"none\", \"oversample_seq\", \"undersample_seq\"]\n\n# ---------- Helpers ----------\ndef _first_existing(path_list):\n    for p in path_list:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"\n    Ensure df has Method/Model/Threshold/Split. If missing, try to parse from 'Key'.\n    \"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file is missing Method/Model/Threshold/Split and has no 'Key' column to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    # Threshold is typically in the 3rd token as 'thr_0.50'\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    # Split can be at the end of the Key; back off to explicit Split col if present\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"), \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", df.get(\"Split\", np.nan)))))\n    return df\n\ndef _round_or_none(x, nd=3):\n    try:\n        return round(float(x), nd)\n    except Exception:\n        return np.nan\n\ndef _safe_confmat_from_row(row: pd.Series):\n    \"\"\"\n    Reconstructs an integer confusion matrix from supports + recall/specificity metrics in the row.\n    Assumes:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N (specificity wrt positives decision among negatives)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n    sp1  = float(row.get(\"Class1/Specificity\", np.nan))\n\n    if any([not np.isfinite(v) for v in [s0, s1, rec1, sp1]]):\n        raise ValueError(\"Cannot reconstruct confusion matrix: missing supports/recall/specificity in row.\")\n\n    tp = int(round(rec1 * s1))\n    fn = max(0, s1 - tp)\n    tn = int(round(sp1 * s0))\n    fp = max(0, s0 - tn)\n\n    # Small adjustments to keep sums consistent\n    tn = max(0, min(tn, s0))\n    fp = s0 - tn\n    tp = max(0, min(tp, s1))\n    fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    # For loading custom-loss models\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _try_load_sequences(npz_candidates):\n    p = _first_existing(npz_candidates)\n    if not p:\n        print(\"‚ö†Ô∏è sequences_leakfree.npz not found. Feature counts, shapes and ROC/PR will be limited.\")\n        return None\n    npz = np.load(p, allow_pickle=True)\n    # unpack with fallbacks\n    data = {\n        \"train\": {\"X_seq\": npz[\"Xtr\"], \"y\": npz[\"ytr\"]},\n        \"val\":   {\"X_seq\": npz[\"Xva\"], \"y\": npz[\"yva\"]},\n        \"test\":  {\"X_seq\": npz[\"Xte\"], \"y\": npz[\"yte\"]},\n        \"seq_features_used\": list(npz[\"seq_features_used\"].tolist()) if \"seq_features_used\" in npz.files else [],\n        \"static_features_used\": list(npz[\"static_features_used\"].tolist()) if \"static_features_used\" in npz.files else []\n    }\n    # Optional static inputs\n    if \"Xtr_stat\" in npz.files and npz[\"Xtr_stat\"].size > 0:\n        data[\"train\"][\"X_stat\"] = npz[\"Xtr_stat\"]\n    else:\n        data[\"train\"][\"X_stat\"] = None\n\n    if \"Xva_stat\" in npz.files and npz[\"Xva_stat\"].size > 0:\n        data[\"val\"][\"X_stat\"] = npz[\"Xva_stat\"]\n    else:\n        data[\"val\"][\"X_stat\"] = None\n\n    if \"Xte_stat\" in npz.files and npz[\"Xte_stat\"].size > 0:\n        data[\"test\"][\"X_stat\"] = npz[\"Xte_stat\"]\n    else:\n        data[\"test\"][\"X_stat\"] = None\n\n    data[\"npz_path\"] = p\n    return data\n\ndef _predict_loaded_model(model, X_seq, X_stat=None):\n    # Allow models with one or two inputs\n    try:\n        if isinstance(model.input, list) or len(model.inputs) > 1:\n            if X_stat is None:\n                raise ValueError(\"Model expects static input but none provided.\")\n            preds = model.predict([X_seq, X_stat], verbose=0).ravel()\n        else:\n            preds = model.predict(X_seq, verbose=0).ravel()\n        return preds\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Prediction failed: {e}\")\n        return None\n\n# ---------- 1) Load results ----------\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results CSV. Please update RESULTS_CSV_CANDIDATES.\")\nprint(f\"üìÑ Using results file: {res_path}\")\ndf = pd.read_csv(res_path)\ndf = _ensure_columns(df)\n\n# ---------- 1) Top-5 tables per method (by VAL F1_weighted) ----------\nwant_cols_map = {\n    \"Model\": \"Model\",\n    \"Split\": \"Split\",\n    \"Threshold\": \"Threshold\",\n    \"Accuracy\": \"Overall/Accuracy\",\n    \"Precision_weighted\": \"Overall/Precision_weighted\",\n    \"Recall_weighted\": \"Overall/Recall_weighted\",\n    \"F1_weighted\": \"Overall/F1_weighted\",\n    \"Precision_1\": \"Class1/Precision\",\n    \"Recall_1\": \"Class1/Recall\",\n    \"F1_1\": \"Class1/F1\",\n    \"Specificity_1\": \"Class1/Specificity\",\n    \"ROC_AUC\": \"Overall/ROC-AUC\",\n    \"PR_AUC\": \"Overall/PR-AUC\",\n    \"Brier\": \"Overall/MSE_prob\"\n}\n\ndf_val = df[df[\"Split\"].str.lower() == \"val\"].copy()\nall_top5 = []\nfor m in METHODS_FOR_TOP5:\n    sub = df_val[df_val[\"Method\"] == m].copy()\n    sub = sub.dropna(subset=[\"Overall/F1_weighted\"])\n    sub = sub.sort_values(\"Overall/F1_weighted\", ascending=False).head(5)\n    if sub.empty:\n        continue\n    out = pd.DataFrame({\n        k: sub[v].values if v in sub.columns else np.nan\n        for k, v in want_cols_map.items()\n    })\n    out.insert(0, \"Method\", m)\n    all_top5.append(out)\n\ntop5_df = pd.concat(all_top5, ignore_index=True) if all_top5 else pd.DataFrame(columns=[\"Method\"]+list(want_cols_map.keys()))\ntop5_df_rounded = top5_df.copy()\nfor c in [\"Threshold\",\"Accuracy\",\"Precision_weighted\",\"Recall_weighted\",\"F1_weighted\",\n          \"Precision_1\",\"Recall_1\",\"F1_1\",\"Specificity_1\",\"ROC_AUC\",\"PR_AUC\",\"Brier\"]:\n    if c in top5_df_rounded.columns:\n        top5_df_rounded[c] = top5_df_rounded[c].apply(lambda x: _round_or_none(x, 4))\n\nprint(\"\\n=== Top-5 by VAL F1_weighted for each method (none / oversample_seq / undersample_seq) ===\")\nprint(top5_df_rounded.to_string(index=False))\n\n# Save\ntop5_out_path = \"/kaggle/working/top5_summary_per_method.csv\" if os.path.exists(\"/kaggle/working\") else \"top5_summary_per_method.csv\"\ntop5_df_rounded.to_csv(top5_out_path, index=False)\nprint(f\"\\nüíæ Saved top-5 summary ‚Üí {top5_out_path}\")\n\n# ---------- 2) Confusion matrix for BEST VAL F1 model ----------\nif df_val.empty:\n    print(\"\\n‚ö†Ô∏è No validation rows found; cannot select best VAL F1 model.\")\nelse:\n    best_row = df_val.sort_values(\"Overall/F1_weighted\", ascending=False).iloc[0]\n    cm = _safe_confmat_from_row(best_row)\n    print(\"\\n=== Best VAL model (by F1_weighted) ===\")\n    print(f\"Method={best_row['Method']} | Model={best_row['Model']} | thr={best_row['Threshold']:.2f}\")\n    print(\"Confusion Matrix [VAL]:\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Plot & save PNG\n    fig, ax = plt.subplots(figsize=(4.5, 4))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(f\"Confusion Matrix (VAL)\\n{best_row['Method']} / {best_row['Model']} @ thr={best_row['Threshold']:.2f}\")\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    # text\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=12, color=(\"white\" if cm[i,j]>cm.max()/2 else \"black\"))\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    cm_png_path = \"/kaggle/working/confusion_matrix_best_val.png\" if os.path.exists(\"/kaggle/working\") else \"confusion_matrix_best_val.png\"\n    plt.savefig(cm_png_path, dpi=200)\n    plt.close(fig)\n    print(f\"üñºÔ∏è Saved confusion matrix PNG ‚Üí {cm_png_path}\")\n\n# ---------- 3) ROC/PR curves for best 5 (by VAL F1) ----------\n# We will try to load the checkpoints and the sequences NPZ.\nseq_data = _try_load_sequences(SEQ_NPZ_CANDIDATES)\nif seq_data is None:\n    print(\"\\n‚ö†Ô∏è Skipping ROC/PR (sequences NPZ not found).\")\nelse:\n    # pick top 5 by VAL F1 overall (unique Method/Model)\n    top5_overall = (df_val\n                    .dropna(subset=[\"Overall/F1_weighted\"])\n                    .sort_values(\"Overall/F1_weighted\", ascending=False))\n    # keep first occurrence per (Method, Model)\n    top5_overall = top5_overall.drop_duplicates(subset=[\"Method\",\"Model\"]).head(5)\n    if top5_overall.empty:\n        print(\"\\n‚ö†Ô∏è No candidates for ROC/PR plotting.\")\n    else:\n        roc_handles = []\n        pr_handles  = []\n        fig_roc, ax_roc = plt.subplots(figsize=(6.5,5))\n        fig_pr,  ax_pr  = plt.subplots(figsize=(6.5,5))\n\n        yte = seq_data[\"test\"][\"y\"].astype(int)\n        Xte = seq_data[\"test\"][\"X_seq\"]\n        Xte_stat = seq_data[\"test\"].get(\"X_stat\", None)\n\n        for _, r in top5_overall.iterrows():\n            meth, arch = r[\"Method\"], r[\"Model\"]\n            ckpt = os.path.join(CHECKPOINT_DIR, f\"{meth}__{arch}.h5\")\n            if not os.path.exists(ckpt):\n                print(f\"‚ö†Ô∏è Checkpoint not found for {meth}/{arch}: {ckpt} ‚Äî skipping.\")\n                continue\n\n            try:\n                model = tf.keras.models.load_model(ckpt, custom_objects={\"loss\": focal_loss()}, compile=False)\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Failed to load model {ckpt}: {e}\")\n                continue\n\n            y_prob = _predict_loaded_model(model, Xte, X_stat=Xte_stat)\n            if y_prob is None:\n                continue\n\n            # ROC\n            try:\n                fpr, tpr, _ = roc_curve(yte, y_prob)\n                roc_auc = auc(fpr, tpr)\n                ax_roc.plot(fpr, tpr, label=f'{meth}/{arch} (AUC={roc_auc:.3f})')\n            except Exception as e:\n                print(f\"‚ö†Ô∏è ROC failed for {meth}/{arch}: {e}\")\n\n            # PR\n            try:\n                prec, rec, _ = precision_recall_curve(yte, y_prob)\n                ap = average_precision_score(yte, y_prob)\n                ax_pr.plot(rec, prec, label=f'{meth}/{arch} (AP={ap:.3f})')\n            except Exception as e:\n                print(f\"‚ö†Ô∏è PR failed for {meth}/{arch}: {e}\")\n\n        # finalize ROC\n        ax_roc.plot([0,1],[0,1],'--', color='gray', label='Random')\n        ax_roc.set_title(\"ROC ‚Äî Best 5 by VAL F1\")\n        ax_roc.set_xlabel(\"False Positive Rate\")\n        ax_roc.set_ylabel(\"True Positive Rate\")\n        ax_roc.legend(fontsize=8)\n        plt.tight_layout()\n        roc_png = \"/kaggle/working/best5_roc.png\" if os.path.exists(\"/kaggle/working\") else \"best5_roc.png\"\n        fig_roc.savefig(roc_png, dpi=250); plt.close(fig_roc)\n        print(f\"üñºÔ∏è Saved ROC curves ‚Üí {roc_png}\")\n\n        # finalize PR\n        ax_pr.set_title(\"Precision‚ÄìRecall ‚Äî Best 5 by VAL F1\")\n        ax_pr.set_xlabel(\"Recall\")\n        ax_pr.set_ylabel(\"Precision\")\n        ax_pr.legend(fontsize=8)\n        plt.tight_layout()\n        pr_png = \"/kaggle/working/best5_pr.png\" if os.path.exists(\"/kaggle/working\") else \"best5_pr.png\"\n        fig_pr.savefig(pr_png, dpi=250); plt.close(fig_pr)\n        print(f\"üñºÔ∏è Saved PR curves ‚Üí {pr_png}\")\n\n# ---------- 4) Feature counts (hourly vs visit vs static) ----------\nif seq_data is not None:\n    seq_feats = seq_data.get(\"seq_features_used\", []) or []\n    static_feats = seq_data.get(\"static_features_used\", []) or []\n    visit_used = [f for f in seq_feats if f in VISIT_COLS]\n    hourly_used = [f for f in seq_feats if f not in VISIT_COLS]\n\n    print(\"\\n=== Feature sets used (from NPZ) ===\")\n    print(f\"Hourly features after transform: {len(hourly_used)} ‚Üí {hourly_used}\")\n    print(f\"Static features after transform: {len(static_feats)} ‚Üí {static_feats}\")\n    print(f\"Visit features: {len(visit_used)} ‚Üí {visit_used}\")\nelse:\n    print(\"\\n‚ö†Ô∏è Feature counts unavailable (NPZ not found).\")\n\n# ---------- 5) Sequence shapes & class distribution ----------\ndef _shape_or_na(arr):\n    try:\n        return tuple(arr.shape)\n    except Exception:\n        return \"(NA)\"\n\ndef _dist_str(y):\n    if y is None or len(y)==0:\n        return \"{ } (pos=NA)\"\n    cnt = Counter(np.asarray(y).astype(int).tolist())\n    pos = cnt.get(1,0); tot = sum(cnt.values())\n    pct = 100.0*pos/max(1,tot)\n    return f\"{dict(cnt)} (pos={pct:.1f}%)\"\n\nif seq_data is not None:\n    trX, vaX, teX = seq_data[\"train\"][\"X_seq\"], seq_data[\"val\"][\"X_seq\"], seq_data[\"test\"][\"X_seq\"]\n    print(\"\\n=== Sequence shapes ===\")\n    print(f\"Train seq: {_shape_or_na(trX)}\")\n    print(f\"Val   seq: {_shape_or_na(vaX)}\")\n    print(f\"Test  seq: {_shape_or_na(teX)}\")\n\n    print(\"\\n=== Class distribution ===\")\n    print(f\"üîé Train sequences: {_dist_str(seq_data['train']['y'])}\")\n    print(f\"üîé Val sequences:   {_dist_str(seq_data['val']['y'])}\")\n    print(f\"üîé Test sequences:  {_dist_str(seq_data['test']['y'])}\")\nelse:\n    print(\"\\n‚ö†Ô∏è Sequence shapes & class distribution unavailable (NPZ not found).\")\n\n# ---------- 6) Print structure of the BEST model ----------\nif df_val.empty:\n    print(\"\\n‚ö†Ô∏è No validation rows ‚Üí cannot determine best model for summary.\")\nelse:\n    best_row = df_val.sort_values(\"Overall/F1_weighted\", ascending=False).iloc[0]\n    meth, arch = best_row[\"Method\"], best_row[\"Model\"]\n    ckpt = os.path.join(CHECKPOINT_DIR, f\"{meth}__{arch}.h5\")\n    if not os.path.exists(ckpt):\n        print(f\"\\n‚ö†Ô∏è Best model checkpoint not found: {ckpt}\")\n    else:\n        try:\n            model = tf.keras.models.load_model(ckpt, custom_objects={\"loss\": focal_loss()}, compile=False)\n            s = io.StringIO()\n            model.summary(print_fn=lambda x: s.write(x + \"\\n\"))\n            summary_text = s.getvalue()\n            print(\"\\n=== Best model structure (Keras summary) ===\")\n            print(summary_text)\n            # Save to file\n            summary_path = \"/kaggle/working/best_model_summary.txt\" if os.path.exists(\"/kaggle/working\") else \"best_model_summary.txt\"\n            with open(summary_path, \"w\") as f:\n                f.write(summary_text)\n            print(f\"üíæ Saved best model summary ‚Üí {summary_path}\")\n        except Exception as e:\n            print(f\"\\n‚ö†Ô∏è Failed to load/print model summary: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:59:19.120945Z","iopub.execute_input":"2025-10-18T08:59:19.121502Z","iopub.status.idle":"2025-10-18T08:59:32.588905Z","shell.execute_reply.started":"2025-10-18T08:59:19.121470Z","shell.execute_reply":"2025-10-18T08:59:32.587999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================================\n# Extra: fixed-confusion-matrix PNGs for\n#  - oversample_seq + LSTM_100 @ thr 0.50\n#  - undersample_seq + BiLSTM  @ thr 0.40\n#  - none          + LSTM_50   @ thr 0.50\n# ===========================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ---- where to find the results file ----\nRESULTS_CSV_CANDIDATES = [\n    \"/mnt/data/results_summary_all (9).csv\",\n    \"/mnt/data/results_summary_all.csv\",\n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\n\ndef _first_existing(paths):\n    for p in paths:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"Make sure Method/Model/Threshold/Split are present (parse from Key if needed).\"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file missing Method/Model/Threshold/Split and no 'Key' to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"),   \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", np.nan))))\n    return df\n\ndef _confmat_from_row(row: pd.Series):\n    \"\"\"\n    Build confusion matrix from supports + recall/spec recorded in results CSV:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N  (fallback to Class0/Recall if needed)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n\n    sp1 = row.get(\"Class1/Specificity\", np.nan)\n    if not np.isfinite(sp1):\n        # fallback: recall for class 0 equals specificity wrt class 1\n        sp1 = float(row.get(\"Class0/Recall\", np.nan))\n\n    if not all(np.isfinite([s0, s1, rec1, sp1])):\n        raise ValueError(\"Row lacks required metrics to reconstruct confusion matrix.\")\n\n    tp = int(round(rec1 * s1))\n    fn = s1 - tp\n    tn = int(round(sp1 * s0))\n    fp = s0 - tn\n\n    # clamp for safety\n    tn = max(0, min(tn, s0)); fp = s0 - tn\n    tp = max(0, min(tp, s1)); fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef _plot_cm(cm: np.ndarray, title: str, out_path: str):\n    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v),\n                ha=\"center\", va=\"center\",\n                color=(\"white\" if v > cm.max()/2 else \"black\"),\n                fontsize=12)\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=220)\n    plt.close(fig)\n\n# ---- load results ----\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results_summary_all*.csv ‚Äî update RESULTS_CSV_CANDIDATES.\")\nprint(f\"üìÑ Using results file: {res_path}\")\nres = pd.read_csv(res_path)\nres = _ensure_columns(res)\n\n# We'll take the 'test' split by default (change SPLIT to \"val\" if you want VAL instead)\nSPLIT = \"test\"\n\n# ---- combos you requested ----\nrequests = [\n    (\"oversample_seq\", \"LSTM_100\", 0.50),\n    (\"undersample_seq\", \"BiLSTM\",  0.40),\n    (\"none\",           \"LSTM_50\",  0.50),\n]\n\n# ---- make outputs dir if on Kaggle ----\nout_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\"\noutputs = []\n\nfor method, model, thr in requests:\n    # filter rows (case-insensitive Split; Threshold rounded to 2dp in pipeline)\n    sub = res[\n        (res[\"Method\"] == method) &\n        (res[\"Model\"]  == model)  &\n        (res[\"Split\"].astype(str).str.lower() == SPLIT.lower())\n    ].copy()\n\n    if sub.empty:\n        print(f\"‚ö†Ô∏è No rows for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # match threshold to 2 decimals\n    # accept both exact and near-equal due to float rounding\n    def _thr_match(x):\n        try:\n            return (round(float(x), 2) == round(float(thr), 2)) or (abs(float(x) - float(thr)) < 1e-6)\n        except Exception:\n            return False\n\n    sub = sub[sub[\"Threshold\"].apply(_thr_match)]\n    if sub.empty:\n        print(f\"‚ö†Ô∏è No threshold={thr:.2f} row for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # If multiple rows (rare), take the first\n    row = sub.iloc[0]\n    cm = _confmat_from_row(row)\n\n    # Print nicely\n    print(f\"\\n=== Confusion Matrix ‚Äî {method} + {model} @ thr={thr:.2f} [{SPLIT}] ===\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Save PNG\n    safe_method = method.replace(\"/\",\"-\")\n    safe_model  = model.replace(\"/\",\"-\")\n    out_png = os.path.join(out_dir, f\"cm_{safe_method}__{safe_model}__thr_{thr:.2f}__{SPLIT}.png\")\n    _plot_cm(cm, f\"{method} + {model} @ thr={thr:.2f} [{SPLIT}]\", out_png)\n    outputs.append(out_png)\n\nprint(\"\\nüñºÔ∏è Saved confusion matrix images:\")\nfor p in outputs:\n    print(\" -\", p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T09:43:08.635081Z","iopub.execute_input":"2025-10-18T09:43:08.635547Z","iopub.status.idle":"2025-10-18T09:43:08.666415Z","shell.execute_reply.started":"2025-10-18T09:43:08.635524Z","shell.execute_reply":"2025-10-18T09:43:08.665256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  fixed-confusion-matrix PNGs","metadata":{}},{"cell_type":"code","source":"# ===========================================\n# Extra: fixed-confusion-matrix PNGs for\n#  - oversample_seq + LSTM_100 @ thr 0.50\n#  - undersample_seq + BiLSTM  @ thr 0.40\n#  - none          + LSTM_50   @ thr 0.50\n# ===========================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ---- where to find the results file ----\nRESULTS_CSV_CANDIDATES = [\n    \"/mnt/data/results_summary_all (9).csv\",\n    \"/mnt/data/results_summary_all.csv\",\n    \"/kaggle/working/results_summary_all.csv\",\n    \"/kaggle/working/outputs/results_summary_all.csv\",\n    \"outputs/results_summary_all.csv\"\n]\n\ndef _first_existing(paths):\n    for p in paths:\n        if os.path.exists(p):\n            return p\n    return None\n\ndef _ensure_columns(df: pd.DataFrame):\n    \"\"\"Make sure Method/Model/Threshold/Split are present (parse from Key if needed).\"\"\"\n    need = {\"Method\",\"Model\",\"Threshold\",\"Split\"}\n    if need.issubset(df.columns):\n        return df\n    if \"Key\" not in df.columns:\n        raise KeyError(\"Results file missing Method/Model/Threshold/Split and no 'Key' to parse.\")\n    parts = df[\"Key\"].astype(str).str.split(\"__\")\n    df[\"Method\"] = parts.str[0]\n    df[\"Model\"]  = parts.str[1]\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n    key = df[\"Key\"].astype(str)\n    df[\"Split\"] = np.where(key.str.endswith(\"__train\"), \"train\",\n                   np.where(key.str.endswith(\"__val\"),   \"val\",\n                   np.where(key.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                   np.where(key.str.endswith(\"__test\"), \"test\", np.nan))))\n    return df\n\ndef _confmat_from_row(row: pd.Series):\n    \"\"\"\n    Build confusion matrix from supports + recall/spec recorded in results CSV:\n      - Class1/Recall = TP / P\n      - Class1/Specificity = TN / N  (fallback to Class0/Recall if needed)\n      - Class0/Support = N, Class1/Support = P\n    \"\"\"\n    s0 = int(row.get(\"Class0/Support\", 0))  # negatives\n    s1 = int(row.get(\"Class1/Support\", 0))  # positives\n    rec1 = float(row.get(\"Class1/Recall\", np.nan))\n\n    sp1 = row.get(\"Class1/Specificity\", np.nan)\n    if not np.isfinite(sp1):\n        # fallback: recall for class 0 equals specificity wrt class 1\n        sp1 = float(row.get(\"Class0/Recall\", np.nan))\n\n    if not all(np.isfinite([s0, s1, rec1, sp1])):\n        raise ValueError(\"Row lacks required metrics to reconstruct confusion matrix.\")\n\n    tp = int(round(rec1 * s1))\n    fn = s1 - tp\n    tn = int(round(sp1 * s0))\n    fp = s0 - tn\n\n    # clamp for safety\n    tn = max(0, min(tn, s0)); fp = s0 - tn\n    tp = max(0, min(tp, s1)); fn = s1 - tp\n\n    return np.array([[tn, fp],\n                     [fn, tp]], dtype=int)\n\ndef _plot_cm(cm: np.ndarray, title: str, out_path: str):\n    fig, ax = plt.subplots(figsize=(4.8, 4.2))\n    im = ax.imshow(cm, cmap=\"Blues\")\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n    ax.set_xticks([0,1]); ax.set_xticklabels([\"0\",\"1\"])\n    ax.set_yticks([0,1]); ax.set_yticklabels([\"0\",\"1\"])\n    for (i,j), v in np.ndenumerate(cm):\n        ax.text(j, i, str(v),\n                ha=\"center\", va=\"center\",\n                color=(\"white\" if v > cm.max()/2 else \"black\"),\n                fontsize=12)\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=220)\n    plt.close(fig)\n\n# ---- load results ----\nres_path = _first_existing(RESULTS_CSV_CANDIDATES)\nif not res_path:\n    raise FileNotFoundError(\"Could not find results_summary_all*.csv ‚Äî update RESULTS_CSV_CANDIDATES.\")\nprint(f\"üìÑ Using results file: {res_path}\")\nres = pd.read_csv(res_path)\nres = _ensure_columns(res)\n\n# We'll take the 'test' split by default (change SPLIT to \"val\" if you want VAL instead)\nSPLIT = \"test\"\n\n# ---- combos you requested ----\nrequests = [\n    (\"oversample_seq\", \"LSTM_100\", 0.50),\n    (\"undersample_seq\", \"BiLSTM\",  0.40),\n    (\"none\",           \"LSTM_50\",  0.50),\n]\n\n# ---- make outputs dir if on Kaggle ----\nout_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\"\noutputs = []\n\nfor method, model, thr in requests:\n    # filter rows (case-insensitive Split; Threshold rounded to 2dp in pipeline)\n    sub = res[\n        (res[\"Method\"] == method) &\n        (res[\"Model\"]  == model)  &\n        (res[\"Split\"].astype(str).str.lower() == SPLIT.lower())\n    ].copy()\n\n    if sub.empty:\n        print(f\"‚ö†Ô∏è No rows for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # match threshold to 2 decimals\n    # accept both exact and near-equal due to float rounding\n    def _thr_match(x):\n        try:\n            return (round(float(x), 2) == round(float(thr), 2)) or (abs(float(x) - float(thr)) < 1e-6)\n        except Exception:\n            return False\n\n    sub = sub[sub[\"Threshold\"].apply(_thr_match)]\n    if sub.empty:\n        print(f\"‚ö†Ô∏è No threshold={thr:.2f} row for {method} + {model} on split='{SPLIT}'. Skipping.\")\n        continue\n\n    # If multiple rows (rare), take the first\n    row = sub.iloc[0]\n    cm = _confmat_from_row(row)\n\n    # Print nicely\n    print(f\"\\n=== Confusion Matrix ‚Äî {method} + {model} @ thr={thr:.2f} [{SPLIT}] ===\")\n    print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_string())\n\n    # Save PNG\n    safe_method = method.replace(\"/\",\"-\")\n    safe_model  = model.replace(\"/\",\"-\")\n    out_png = os.path.join(out_dir, f\"cm_{safe_method}__{safe_model}__thr_{thr:.2f}__{SPLIT}.png\")\n    _plot_cm(cm, f\"{method} + {model} @ thr={thr:.2f} [{SPLIT}]\", out_png)\n    outputs.append(out_png)\n\nprint(\"\\nüñºÔ∏è Saved confusion matrix images:\")\nfor p in outputs:\n    print(\" -\", p)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Shape \n### Fixed analysis + sequence utilities (single-source, de-duplicated)\n### - build_sequences_by_split   (scales using actual X.shape, not SEQ_LEN)\n### - _build_sequence_index_map  (one canonical copy)\n### - compute_visit_shap         (robust, seq_len inferred if omitted)\n### - aggregate_hourly_to_daily_risk (no hidden deps; direct model.predict)\n### - make_balanced_test         (shape-safe)","metadata":{}},{"cell_type":"code","source":"# ======================================================\n# Fixed analysis + sequence utilities + RUNNER\n#  - build_sequences_by_split   (scales using actual X.shape, not SEQ_LEN)\n#  - _build_sequence_index_map  (one canonical copy)\n#  - compute_visit_shap         (robust; seq_len inferred if omitted)\n#  - aggregate_hourly_to_daily_risk (direct model.predict; no hidden deps)\n#  - make_balanced_test         (shape-safe)\n#  - run_analyses_all           (calls everything and prints outputs)\n#  - quick model fit if none    (small LSTM; early stopping)\n# ======================================================\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Optional TF imports only used in the quick-fit path\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# ---------------------------\n# Global guards / safe fallbacks\n# ---------------------------\ndef _bool_env(name, default=False):\n    v = os.environ.get(name)\n    if v is None: return default\n    return str(v).strip().lower() in {\"1\",\"true\",\"yes\",\"y\",\"on\"}\n\ntry:\n    USE_STATIC_INPUT\nexcept NameError:\n    USE_STATIC_INPUT = True\n\ntry:\n    RANDOM_STATE\nexcept NameError:\n    RANDOM_STATE = 42\n\ntry:\n    DEFAULT_SEQ_FEATURE_COLS\nexcept NameError:\n    DEFAULT_SEQ_FEATURE_COLS = (\n        \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n        \"pc1_activity_energy\",\n        \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n    )\n\ntry:\n    STATIC_COLS\nexcept NameError:\n    STATIC_COLS = [\n        \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n        \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n    ]\n\ntry:\n    VISIT_COLS\nexcept NameError:\n    VISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\n\n# Analysis defaults\nVISIT_FEATURES       = VISIT_COLS\nSHAP_BACKGROUND_SIZE = 512\nSHAP_TEST_SAMPLES    = 1024\nRISK_THRESHOLD       = 0.50\n\ndef _check_globals():\n    \"\"\"No-op guard used by helpers that previously referenced external globals.\"\"\"\n    pass\n\n# ======================================================\n# build_sequences_by_split (uses actual X.shape[1] when reshaping)\n# ======================================================\ndef build_sequences_by_split(\n    hourly: pd.DataFrame,\n    splits,\n    seq_len: int,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    scale_features: bool = True\n):\n    \"\"\"\n    Build (X_seq, X_stat, y) arrays for train/val/test given hourly data and patient splits.\n    - Scalers are fit on TRAIN only.\n    - Uses actual window length (T) from arrays when reshaping; no reliance on global SEQ_LEN.\n    \"\"\"\n    # Required columns\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly = hourly.copy()\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    # Sequence features presence\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    # Static matrix per patient (fill zeros for missing patients)\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float)\n                      .fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        sub = sub.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len])\n                y.append(labels[i+seq_len])\n                if USE_STATIC_INPUT and static_mat is not None:\n                    if pid in static_mat.index:\n                        X_stat.append(static_mat.loc[pid].values.astype(float))\n                    else:\n                        X_stat.append(np.zeros(len(static_cols_present), dtype=float))\n        X_seq = np.array(X_seq)\n        y     = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (USE_STATIC_INPUT and static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    # -------- Scale on TRAIN only (use actual T, F) --------\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s is not None and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size == 0:\n                return X\n            n, T, F = X.shape\n            return seq_scaler.transform(X.reshape(-1, F)).reshape(n, T, F)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size > 0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size == 0:\n                return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"‚úÖ Sequences built | train={getattr(Xtr_s,'shape',None)}, val={getattr(Xva_s,'shape',None)}, test={getattr(Xte_s,'shape',None)}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ======================================================\n# Sequence index map (single canonical copy)\n# ======================================================\ndef _build_sequence_index_map(hourly_df: pd.DataFrame, split: str, seq_len: int) -> pd.DataFrame:\n    \"\"\"\n    Recreate the exact sequence ordering used in build_sequences_by_split so that\n    index i maps to the (i+seq_len)-th hour row for each patient.\n\n    Returns: ['seq_idx','patientID','hour','date','visit_assigned','period_main','row_idx']\n    \"\"\"\n    sub = (hourly_df[hourly_df[\"Split\"].astype(str).str.lower() == split.lower()]\n           .sort_values([\"patientID\",\"hour\"])\n           .reset_index())\n    sub = sub.rename(columns={\"index\": \"row_idx\"})\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\n                \"seq_idx\": len(rows),\n                \"patientID\": pid,\n                \"hour\": pd.to_datetime(tgt[\"hour\"]),\n                \"date\": pd.to_datetime(tgt.get(\"date\", pd.NaT)),\n                \"visit_assigned\": tgt.get(\"visit_assigned\", np.nan),\n                \"period_main\": tgt.get(\"period_main\", np.nan),\n                \"row_idx\": int(tgt[\"row_idx\"]),\n            })\n    return pd.DataFrame(rows)\n\n# ======================================================\n# SHAP on visit features (robust; seq_len inference; shape checks)\n# ======================================================\ndef compute_visit_shap(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    seq_features_used,\n    visit_features=None,\n    split: str = \"test\",\n    background_size: int = SHAP_BACKGROUND_SIZE,\n    max_test_windows: int = SHAP_TEST_SAMPLES,\n    out_dir: str = \"/kaggle/working\",\n    seq_len: int = None\n):\n    \"\"\"\n    Computes global and per-visit SHAP importance for visit features that are INCLUDED\n    in the sequence feature tensor. Saves two CSVs:\n      - shap_visit_global.csv\n      - shap_visit_per_visit.csv\n    \"\"\"\n    _check_globals()\n    os.makedirs(out_dir, exist_ok=True)\n    visit_features = list(visit_features) if visit_features is not None else list(VISIT_FEATURES)\n\n    # Infer seq_len from arrays if not provided\n    Xte = data[split][\"X_seq\"]\n    if seq_len is None:\n        if Xte is None or Xte.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len from data arrays.\")\n        seq_len = Xte.shape[1]\n\n    # Map visit features -> indices in the sequence features\n    feat_to_idx = {f: i for i, f in enumerate(seq_features_used)}\n    visit_in_seq = [f for f in visit_features if f in feat_to_idx]\n    if not visit_in_seq:\n        raise ValueError(\n            f\"None of the visit features are present in seq_features_used={seq_features_used}. \"\n            f\"Ensure your DEFAULT_SEQ_FEATURE_COLS includes items from VISIT_COLS.\"\n        )\n\n    Xtr, Xtr_stat = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"]\n    Xte, Xte_stat = data[split][\"X_seq\"],  data[split][\"X_stat\"]\n\n    bg_n = min(int(background_size), len(Xtr))\n    te_n = min(int(max_test_windows), len(Xte))\n    if bg_n < 1 or te_n < 1:\n        raise RuntimeError(f\"Not enough sequences for SHAP (bg_n={bg_n}, te_n={te_n}).\")\n\n    rng   = np.random.default_rng(42)\n    bg_ix = rng.choice(np.arange(len(Xtr)), size=bg_n, replace=False)\n    te_ix = rng.choice(np.arange(len(Xte)), size=te_n, replace=False)\n\n    bg_seq, te_seq = Xtr[bg_ix], Xte[te_ix]\n    if USE_STATIC_INPUT and (Xtr_stat is not None and Xte_stat is not None and Xtr_stat.size>0 and Xte_stat.size>0):\n        bg_static, te_static = Xtr_stat[bg_ix], Xte_stat[te_ix]\n    else:\n        bg_static = te_static = None\n\n    # ---- SHAP explainer\n    try:\n        import shap\n    except Exception as e:\n        raise ImportError(\n            \"This SHAP analysis requires the 'shap' package. Install with: pip install shap\"\n        ) from e\n\n    try:\n        if bg_static is not None:\n            explainer   = shap.DeepExplainer(model, [bg_seq, bg_static])\n            shap_values = explainer.shap_values([te_seq, te_static])\n        else:\n            explainer   = shap.DeepExplainer(model, bg_seq)\n            shap_values = explainer.shap_values(te_seq)\n    except Exception as e:\n        print(f\"[WARN] DeepExplainer failed ({e}). Falling back to GradientExplainer‚Ä¶\")\n        if bg_static is not None:\n            explainer   = shap.GradientExplainer(model, [bg_seq, bg_static])\n            shap_values = explainer.shap_values([te_seq, te_static])\n        else:\n            explainer   = shap.GradientExplainer(model, bg_seq)\n            shap_values = explainer.shap_values(te_seq)\n\n    shap_seq = shap_values[0] if isinstance(shap_values, list) else shap_values\n    if shap_seq.ndim != 3:\n        raise RuntimeError(f\"Unexpected SHAP shape for sequence input: {shap_seq.shape}\")\n\n    # Reduce over time ‚Üí mean |SHAP| across the window\n    shap_abs_time = np.mean(np.abs(shap_seq), axis=1)  # [n_samples, F]\n\n    # ---- GLOBAL visit feature importance\n    rows = [{\"feature\": f, \"mean_abs_shap\": float(np.mean(shap_abs_time[:, feat_to_idx[f]]))}\n            for f in visit_in_seq]\n    global_visit_df = pd.DataFrame(rows).sort_values(\"mean_abs_shap\", ascending=False)\n    gpath = os.path.join(out_dir, \"shap_visit_global.csv\")\n    global_visit_df.to_csv(gpath, index=False)\n    print(\"‚úÖ Saved global visit SHAP ‚Üí\", gpath)\n\n    # ---- PER‚ÄëVISIT importance\n    seq_map = _build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(Xte):\n        raise RuntimeError(\n            f\"Mapping length {len(seq_map)} != X_{split} length {len(Xte)}. \"\n            f\"seq_len={seq_len}. Rebuild hourly/sequences consistently.\"\n        )\n    seq_map_sub = seq_map.iloc[te_ix].reset_index(drop=True)\n\n    per_rows = []\n    for i in range(len(seq_map_sub)):\n        pid = seq_map_sub.loc[i, \"patientID\"]\n        dte = pd.to_datetime(seq_map_sub.loc[i, \"date\"])\n        for f in visit_in_seq:\n            per_rows.append({\n                \"patientID\": pid,\n                \"date\": dte,\n                \"feature\": f,\n                \"mean_abs_shap\": float(shap_abs_time[i, feat_to_idx[f]])\n            })\n    per_visit_df = (pd.DataFrame(per_rows)\n                    .groupby([\"patientID\",\"date\",\"feature\"], as_index=False)[\"mean_abs_shap\"].mean()\n                    .sort_values([\"patientID\",\"date\",\"mean_abs_shap\"], ascending=[True, True, False]))\n    ppath = os.path.join(out_dir, \"shap_visit_per_visit.csv\")\n    per_visit_df.to_csv(ppath, index=False)\n    print(\"‚úÖ Saved per‚Äëvisit SHAP ‚Üí\", ppath)\n\n    return global_visit_df, per_visit_df\n\n# ======================================================\n# Aggregate hourly predictions to daily risk\n# ======================================================\ndef aggregate_hourly_to_daily_risk(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    split: str = \"test\",\n    threshold: float = RISK_THRESHOLD,\n    out_dir: str = \"/kaggle/working\",\n    seq_len: int = None\n):\n    \"\"\"\n    Aggregate sequence‚Äëwindow predictions to daily risk summaries.\n    Saves 'daily_risk_<split>.csv' and (best effort) a small example plot.\n    \"\"\"\n    os.makedirs(out_dir, exist_ok=True)\n\n    # Predictions\n    Xs      = data[split][\"X_seq\"]\n    Xs_stat = data[split][\"X_stat\"]\n    y_true  = np.asarray(data[split][\"y\"]).astype(int).ravel()\n\n    if seq_len is None:\n        if Xs is None or Xs.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len from data arrays.\")\n        seq_len = Xs.shape[1]\n\n    if USE_STATIC_INPUT and (Xs_stat is not None and Xs_stat.size>0):\n        y_prob = model.predict([Xs, Xs_stat], verbose=0).ravel()\n    else:\n        y_prob = model.predict(Xs, verbose=0).ravel()\n    y_pred = (y_prob >= float(threshold)).astype(int)\n\n    # Map sequence rows back to hours/dates\n    seq_map = _build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(y_true):\n        raise RuntimeError(\n            f\"Sequence map length {len(seq_map)} != prediction length {len(y_true)}.\\n\"\n            f\"seq_len={seq_len}, X_{split}.shape={getattr(Xs, 'shape', None)}. \"\n            \"Use the same hourly/sequences and seq_len that were used for training.\"\n        )\n\n    pred_df = pd.DataFrame({\n        \"patientID\": seq_map[\"patientID\"].values,\n        \"hour\":      pd.to_datetime(seq_map[\"hour\"].values),\n        \"date\":      pd.to_datetime(seq_map[\"date\"].values),\n        \"visit_assigned\": seq_map.get(\"visit_assigned\", pd.Series([np.nan]*len(seq_map))).values,\n        \"period_main\":    seq_map.get(\"period_main\", pd.Series([np.nan]*len(seq_map))).values,\n        \"y_true\":    y_true,\n        \"y_prob\":    y_prob,\n        \"y_pred\":    y_pred\n    })\n\n    # Daily aggregates\n    grp = pred_df.groupby([\"patientID\",\"date\"], as_index=False)\n    daily = grp.agg(\n        n_windows=(\"y_true\",\"size\"),\n        true_positives=(\"y_true\",\"sum\"),\n        pred_positives=(\"y_pred\",\"sum\"),\n        risk_mean=(\"y_prob\",\"mean\"),\n        risk_max=(\"y_prob\",\"max\"),\n        risk_p95=(\"y_prob\", lambda x: float(np.quantile(x, 0.95))),\n        hours_above_thr=(\"y_pred\",\"sum\")\n    )\n    daily[\"prevalence\"] = daily[\"true_positives\"] / daily[\"n_windows\"].replace(0, np.nan)\n    daily_csv = os.path.join(out_dir, f\"daily_risk_{split}.csv\")\n    daily.to_csv(daily_csv, index=False)\n    print(f\"‚úÖ Saved daily risk aggregates ‚Üí {daily_csv}\")\n\n    # Optional quick plot\n    try:\n        example_pid = daily[\"patientID\"].iloc[0]\n        dsub = daily[daily[\"patientID\"] == example_pid].sort_values(\"date\")\n        plt.figure(figsize=(8,3))\n        plt.plot(dsub[\"date\"], dsub[\"risk_mean\"], label=\"Mean daily risk\")\n        plt.plot(dsub[\"date\"], dsub[\"risk_max\"],  label=\"Max daily risk\")\n        plt.axhline(threshold, linestyle=\"--\", label=f\"thr={threshold:.2f}\")\n        plt.xlabel(\"Date\"); plt.ylabel(\"Risk\"); plt.title(f\"Daily risk ‚Äî patient {example_pid}\")\n        plt.legend(); plt.tight_layout()\n        png = os.path.join(out_dir, f\"daily_risk_trend_patient_{example_pid}.png\")\n        plt.savefig(png, dpi=200); plt.close()\n        print(f\"üñºÔ∏è Saved example daily trend ‚Üí {png}\")\n    except Exception as e:\n        print(f\"[WARN] Could not plot daily trend example: {e}\")\n\n    return pred_df, daily\n\n# ======================================================\n# Balanced test subset (shape-safe)\n# ======================================================\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state: int = RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0:\n        return (X_test, y_test, (None if X_stat is None else np.asarray(X_stat)))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ======================================================\n# ---------- Helper: tiny LSTM builder for quick run ----------\n# ======================================================\ndef _make_quick_model(seq_len, n_seq_f, n_stat_f=0, lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = LSTM(64, return_sequences=True)(seq_in)\n    x = Dropout(0.2)(x)\n    x = LSTM(32)(x)\n    x = Dropout(0.2)(x)\n    x = Dense(16, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(16, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(16, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(16, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# ======================================================\n# ---------- Helper: load sequences from NPZ ----------\n# ======================================================\ndef _load_sequences_npz(npz_path):\n    d = np.load(npz_path, allow_pickle=True)\n    def _arr(name):\n        return None if name not in d or d[name].size==0 else d[name]\n    data = {\n        \"train\": {\"X_seq\": _arr(\"Xtr\"), \"X_stat\": _arr(\"Xtr_stat\"), \"y\": d[\"ytr\"]},\n        \"val\":   {\"X_seq\": _arr(\"Xva\"), \"X_stat\": _arr(\"Xva_stat\"), \"y\": d[\"yva\"]},\n        \"test\":  {\"X_seq\": _arr(\"Xte\"), \"X_stat\": _arr(\"Xte_stat\"), \"y\": d[\"yte\"]},\n        \"seq_features_used\": list(d[\"seq_features_used\"]),\n        \"static_features_used\": list(d[\"static_features_used\"]) if \"static_features_used\" in d else []\n    }\n    return data\n\n# ======================================================\n# ---------- Helper: find files & build splits ----------\n# ======================================================\ndef _first_existing(paths):\n    for p in paths:\n        if p and os.path.exists(p):\n            return p\n    return None\n\ndef _splits_from_hourly(hourly: pd.DataFrame):\n    train_p = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"train\",\"patientID\"].unique()))\n    val_p   = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"val\",\"patientID\"].unique()))\n    test_p  = np.array(sorted(hourly.loc[hourly[\"Split\"].str.lower()==\"test\",\"patientID\"].unique()))\n    if len(train_p)==0 or len(val_p)==0 or len(test_p)==0:\n        raise RuntimeError(\"hourly must include a 'Split' column with 'train'/'val'/'test' assignments.\")\n    return (train_p, val_p, test_p)\n\n# ======================================================\n# --------------------- RUNNER -------------------------\n# ======================================================\ndef run_analyses_all(model=None, data=None, hourly=None,\n                     split=\"test\", out_dir=None, seq_len_default=24,\n                     do_shap=True, train_epochs=6):\n    \"\"\"\n    If model/data/hourly are not supplied, this runner tries to:\n    - load sequences from NPZ (sequences_leakfree.npz)\n    - load hourly CSV (dynamic_hourly_features_ramadan.csv)\n    - fit a small LSTM quickly\n    Then it computes:\n    - SHAP CSVs (if 'shap' is installed and do_shap=True)\n    - Daily risk CSV + a small PNG\n    \"\"\"\n    # --- output dir\n    if out_dir is None:\n        if os.path.exists(\"/kaggle/working\"): out_dir = \"/kaggle/working\"\n        else:\n            out_dir = os.path.join(\".\", \"outputs\")\n            os.makedirs(out_dir, exist_ok=True)\n\n    # --- locate files if needed\n    if hourly is None:\n        HOURLY_CANDIDATES = [\n            \"/kaggle/working/dynamic_hourly_features_ramadan.csv\",\n            \"/mnt/data/dynamic_hourly_features_ramadan.csv\",\n            \"/kaggle/input/hmc-model-static-variables/dynamic_hourly_features_ramadan.csv\"\n        ]\n        hp = _first_existing(HOURLY_CANDIDATES)\n        if not hp:\n            raise FileNotFoundError(\"Could not find hourly CSV. Please pass 'hourly' DataFrame.\")\n        hourly = pd.read_csv(hp)\n        print(f\"üìÑ Loaded hourly table: {hp} | shape={hourly.shape}\")\n\n    if data is None:\n        NPZ_CANDIDATES = [\n            \"/kaggle/working/sequences_leakfree.npz\",\n            \"/kaggle/working/outputs/sequences_leakfree.npz\",\n            \"/mnt/data/sequences_leakfree.npz\"\n        ]\n        npz = _first_existing(NPZ_CANDIDATES)\n        if npz:\n            data = _load_sequences_npz(npz)\n            print(f\"üì¶ Loaded sequences NPZ: {npz}\")\n        else:\n            # rebuild sequences from hourly\n            splits = _splits_from_hourly(hourly)\n            data = build_sequences_by_split(hourly, splits, seq_len=seq_len_default,\n                                            seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n                                            static_cols=STATIC_COLS, scale_features=True)\n\n    # --- fit quick model if none provided\n    if model is None:\n        Xtr, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"y\"]\n        Xva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"y\"]\n        Xtr_stat = data[\"train\"][\"X_stat\"]; Xva_stat = data[\"val\"][\"X_stat\"]\n        seq_len  = Xtr.shape[1]; n_seq_f = Xtr.shape[2]\n        n_stat_f = 0 if (Xtr_stat is None or not USE_STATIC_INPUT) else Xtr_stat.shape[1]\n\n        model = _make_quick_model(seq_len, n_seq_f, n_stat_f=n_stat_f, lr=1e-3)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xtr, Xtr_stat], ytr, validation_data=([Xva, Xva_stat], yva),\n                      epochs=train_epochs, batch_size=64, callbacks=[es], verbose=1)\n        else:\n            model.fit(Xtr, ytr, validation_data=(Xva, yva),\n                      epochs=train_epochs, batch_size=64, callbacks=[es], verbose=1)\n        print(\"‚úÖ Model trained (quick fit).\")\n\n    # --- SHAP (optional)\n    if do_shap:\n        try:\n            _ = compute_visit_shap(model, data, hourly, data[\"seq_features_used\"],\n                                   visit_features=VISIT_FEATURES, split=split, out_dir=out_dir)\n        except ImportError as e:\n            print(f\"‚ö†Ô∏è SHAP not available; skipping. ({e})\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è SHAP step skipped due to error: {e}\")\n\n    # --- Daily risk aggregation\n    pred_df, daily = aggregate_hourly_to_daily_risk(model, data, hourly,\n                                                    split=split, threshold=RISK_THRESHOLD,\n                                                    out_dir=out_dir)\n    # Print small previews so you \"see output\"\n    with pd.option_context(\"display.width\", 160, \"display.max_columns\", 20):\n        print(\"\\nüîé Predictions (head):\")\n        print(pred_df.head(8).to_string(index=False))\n        print(\"\\nüìä Daily risk (head):\")\n        print(daily.head(8).to_string(index=False))\n\n    print(\"\\nüéØ Done. Outputs saved under:\", out_dir)\n    return model, data, hourly\n\n# ======================================================\n# Example: run automatically if this cell/file is executed\n# (You can comment this out if you already have model/data/hourly in memory.)\n# ======================================================\nif __name__ == \"__main__\":\n    _ = run_analyses_all(\n        model=None,          # set to your trained model object to skip quick fit\n        data=None,           # set to your 'data' dict to reuse existing arrays\n        hourly=None,         # set to your hourly DataFrame if already loaded\n        split=\"test\",\n        out_dir=None,        # default (/kaggle/working or ./outputs)\n        seq_len_default=24,  # used only if rebuilding sequences from hourly\n        do_shap=True,        # requires 'pip install shap'\n        train_epochs=6       # small quick fit; adjust as you like\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndr = pd.read_csv(\"/kaggle/working/daily_risk_test.csv\", parse_dates=[\"date\"])\nsummary = {\n    \"n_rows\": len(dr),\n    \"n_patients\": dr[\"patientID\"].nunique(),\n    \"date_min\": dr[\"date\"].min(),\n    \"date_max\": dr[\"date\"].max(),\n    \"n_days\": dr[\"date\"].nunique(),\n    \"risk_mean_median\": dr[\"risk_mean\"].median(),\n    \"risk_mean_IQR\": (dr[\"risk_mean\"].quantile(0.25), dr[\"risk_mean\"].quantile(0.75)),\n    \"risk_p95_median\": dr[\"risk_p95\"].median(),\n    \"prevalence_median\": dr[\"prevalence\"].median(),\n    \"corr_riskmean_prevalence\": dr[[\"risk_mean\",\"prevalence\"]].corr().iloc[0,1],\n    \"corr_riskp95_prevalence\": dr[[\"risk_p95\",\"prevalence\"]].corr().iloc[0,1],\n}\nsummary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# forecasts\n\nNotes & options\n\nChange patient: set _patient = <your id> before running.\n\nChange K: set K_HOURS to the forecast horizon you want (e.g., 12).\n\nChange threshold: set THRESH if you prefer the VAL‚Äëoptimized threshold you saw in your summary.\n\nBetter inputs for multi‚Äëstep: replace the ‚Äúnaive persistence‚Äù block with proper hour‚Äëahead forecasts of your sequence features (CGM, lifestyle PCs, visit vars) for stronger multi‚Äëhour accuracy.\n\n\nNotes for stronger multi‚Äëhour forecasts\n\nYou‚Äôre currently using naive persistence for the future input features when rolling forward (i.e., you reuse the last observed hour‚Äôs feature vector for each next hour). That‚Äôs OK to get started, but for better K>6 accuracy consider:\n\nCGM features (cgm_mean, cgm_std, ‚Ä¶): replace persistence with a small CGM forecaster (e.g., ARIMA/Prophet/LightGBM) to generate hour‚Äëahead CGM summaries, then feed those into your LSTM rolling window.\n\nLifestyle PCs (pc1_activity_energy, ‚Ä¶): if you have ‚Äútypical daily patterns‚Äù, a daily seasonal baseline (e.g., by hour‚Äëof‚Äëday) can outperform pure persistence.\n\nVisit features (carb, meals, ‚Ä¶): these are daily; step them forward from the last known day or incorporate the next known visit day if available.\n\nIf you want, tell me:\n\na different patientID (from the list above),\n\nyour preferred K_HOURS, and\n\nwhether you want a more conservative (fewer alerts) or more sensitive (more alerts) threshold,\n\nand I‚Äôll output a tailored snippet with those values baked in","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Forecasting & time-compare (fixed + improved)\n# ==========================\nimport os, numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib.dates as mdates\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, precision_recall_curve\nimport tensorflow as tf\n\n# --------- Fallbacks for global symbols if not already defined in the notebook ---------\ntry:\n    OUT_RESULTS_CSV\nexcept NameError:\n    OUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\n\ntry:\n    USE_STATIC_INPUT\nexcept NameError:\n    USE_STATIC_INPUT = True  # your training used static inputs\n\n# focal_loss (redefine here so load_model(custom_objects=...) always works)\ndef focal_loss(gamma=2.0, alpha=0.8):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\n# --------------------------\n# üîß Parameters you can edit\n# --------------------------\nOVERRIDE_PATIENT_ID        = 69    # force this patient (else it auto-suggests)\nK_HOURS                    = 12    # forecast horizon\nAUTO_USE_VAL_OPTIMAL_THR   = True  # use best VAL threshold from your results CSV\nTHRESH_MANUAL              = 0.49  # fallback if the VAL-optimal isn‚Äôt found\nFORECAST_METHOD            = \"ema\" # \"ema\" | \"linear\" | \"persistence\"\nEMA_ALPHA                  = 0.6\nLIN_STEPS                  = 6\nPATIENT_SELECTION_STRATEGY = \"positives\"\nTOP_N_PATIENTS_TO_PLOT     = 3\nSAVE_DIR                   = \"/kaggle/working\"\n\n# Improvement toggles\nUSE_HORIZON_THRESHOLDS     = True  # calibrate per-horizon thresholds on VAL (boosts recall at longer horizons)\nFBETA_FOR_CAL              = 1.5   # Œ≤>1 favors recall\nAPPLY_NOFM                 = True  # apply N-of-M decision smoothing\nNOFM_N, NOFM_M             = 2, 3  # 2 of last 3 positives -> positive\n\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# --- Datetime helpers: make all times tz-naive in UTC ---\ndef _to_naive_utc(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    # If tz-aware, convert to UTC then drop tz; if tz-naive, leave as is\n    if getattr(s.dt, \"tz\", None) is not None:\n        s = s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n    return s\n\n# ----------------------------------------------------\n# 0) Utilities: choose best checkpoint & VAL threshold\n# ----------------------------------------------------\ndef pick_best_checkpoint(results_csv=OUT_RESULTS_CSV, ckpt_dir=\"checkpoints\"):\n    if not os.path.exists(results_csv):\n        raise FileNotFoundError(f\"Results CSV not found: {results_csv}\")\n    df = pd.read_csv(results_csv)\n    dfv = df[df[\"Split\"].astype(str).str.lower().eq(\"val\")].copy()\n    if dfv.empty:\n        raise RuntimeError(\"No VAL rows found in results; cannot pick best model.\")\n    dfv = dfv.sort_values(\"Overall/F1_weighted\", ascending=False)\n    best = dfv.iloc[0]\n    method = str(best[\"Method\"]); model = str(best[\"Model\"])\n    ckpt_path = os.path.join(ckpt_dir, f\"{method}__{model}.h5\")\n    if not os.path.exists(ckpt_path):\n        files = [os.path.join(ckpt_dir, f) for f in os.listdir(ckpt_dir) if f.endswith(\".h5\")]\n        if not files: raise FileNotFoundError(\"No .h5 checkpoints found in 'checkpoints'.\")\n        ckpt_path = files[0]\n        print(f\"[WARN] Expected checkpoint not found; using {ckpt_path}\")\n    print(f\"‚úÖ Best (VAL F1) ‚Üí {method}/{model} | ckpt = {ckpt_path}\")\n    return ckpt_path, method, model\n\ndef get_val_optimal_threshold(results_csv, method, model):\n    try:\n        df = pd.read_csv(results_csv)\n        dfv = df[(df[\"Split\"].astype(str).str.lower()==\"val\") &\n                 (df[\"Method\"].astype(str)==method) &\n                 (df[\"Model\"].astype(str)==model)].copy()\n        if dfv.empty:\n            dfv = df[df[\"Split\"].astype(str).str.lower()==\"val\"].copy()\n        dfv = dfv.sort_values(\"Overall/F1_weighted\", ascending=False)\n        t = float(dfv.iloc[0][\"Threshold\"])\n        if np.isfinite(t): return t\n    except Exception as e:\n        print(f\"[WARN] Could not read VAL-optimal threshold: {e}\")\n    return None\n\n# ------------------------------------------------\n# 1) Align predictions to hours for a given split\n# ------------------------------------------------\ndef predict_split_prob_df(model, data, hourly, split=\"test\", threshold=0.50):\n    Xs = data[split][\"X_seq\"]; Xstat = data[split][\"X_stat\"]\n    ytrue = data[split][\"y\"].astype(int).ravel()\n    if Xs is None or Xs.ndim != 3:\n        raise ValueError(f\"No sequences for split={split}.\")\n    yprob = model.predict([Xs, Xstat], verbose=0).ravel() if (Xstat is not None and Xstat.size>0) else model.predict(Xs, verbose=0).ravel()\n    ypred = (yprob >= float(threshold)).astype(int)\n    seq_len = Xs.shape[1]\n\n    sub = (hourly[hourly[\"Split\"].astype(str).str.lower() == split.lower()]\n           .sort_values([\"patientID\",\"hour\"]).reset_index())\n    sub = sub.rename(columns={\"index\":\"row_idx\"})\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\"seq_idx\":len(rows), \"patientID\":pid,\n                         \"hour\":pd.to_datetime(tgt[\"hour\"]),\n                         \"date\":pd.to_datetime(tgt.get(\"date\", pd.NaT))})\n    idx_map = pd.DataFrame(rows)\n    if len(idx_map) != len(ytrue):\n        raise RuntimeError(f\"Mapping length {len(idx_map)} != predictions length {len(ytrue)}.\")\n    out = (pd.DataFrame({\n                \"patientID\": idx_map[\"patientID\"].values,\n                \"hour\":      pd.to_datetime(idx_map[\"hour\"].values),\n                \"date\":      pd.to_datetime(idx_map[\"date\"].values),\n                \"y_true\":    ytrue,\n                \"y_prob\":    yprob,\n                \"y_pred\":    ypred\n            })\n            .sort_values([\"patientID\",\"hour\"]).reset_index(drop=True))\n    out[\"hour\"] = _to_naive_utc(out[\"hour\"])  # belt-and-suspenders\n    return out\n\n# ------------------------------------------------------------\n# 2) Mini feature forecaster for multi-step rolling predictions\n# ------------------------------------------------------------\ndef _ema_next(v, alpha=0.6):\n    s = v[0]\n    for x in v[1:]:\n        s = alpha*x + (1-alpha)*s\n    return float(s)\n\ndef _linear_next(v):\n    n = len(v)\n    if n < 2: return float(v[-1])\n    x = np.arange(n, dtype=float)\n    try:\n        b1, b0 = np.polyfit(x, v.astype(float), 1)  # y = b1*x + b0\n        return float(b1*(n) + b0)\n    except Exception:\n        return float(v[-1])\n\ndef next_feature_vector(hist_raw, feat_names, method=\"ema\", ema_alpha=0.6, lin_steps=6):\n    T, F = hist_raw.shape\n    out = np.zeros(F, dtype=float)\n    for j, name in enumerate(feat_names):\n        col = hist_raw[:, j]\n        if method == \"persistence\":\n            out[j] = float(col[-1])\n        elif method == \"linear\":\n            w = min(len(col), max(2, lin_steps))\n            out[j] = _linear_next(col[-w:])\n        else:  # \"ema\" default for dynamic signals\n            if any(k in str(name).lower() for k in [\"cgm\",\"pca\",\"pc1\",\"pc2\",\"pc3\",\"steps\",\"calories\",\"heart\"]):\n                w = min(len(col), max(2, lin_steps))\n                out[j] = _ema_next(col[-w:], alpha=ema_alpha)\n            else:\n                out[j] = float(col[-1])\n    return out\n\n# -----------------------------------------------------------------\n# 3) Prepare one anchor window & rolling multi-step patient forecast\n# -----------------------------------------------------------------\ndef _prepare_window_for_patient_index(hourly, data, patient_id, idx, split=\"test\"):\n    seq_feats = list(data[\"seq_features_used\"])\n    seq_len   = int(data[\"train\"][\"X_seq\"].shape[1])\n    sub = hourly[(hourly[\"Split\"].astype(str).str.lower()==split.lower()) &\n                 (hourly[\"patientID\"]==patient_id)].sort_values(\"hour\").reset_index(drop=True)\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    if idx < seq_len: raise ValueError(\"idx must be >= seq_len\")\n    hist_raw   = sub.loc[idx-seq_len:idx-1, seq_feats].astype(float).values\n    seq_scaler = data[\"scalers\"][\"seq\"]\n    hist_scaled= seq_scaler.transform(hist_raw) if seq_scaler is not None else hist_raw\n    # static\n    if USE_STATIC_INPUT and data[\"train\"][\"X_stat\"] is not None:\n        stat_feats = list(data.get(\"static_features_used\", []))\n        srow = (hourly[[\"patientID\"]+stat_feats].drop_duplicates(subset=[\"patientID\"]).set_index(\"patientID\"))\n        s = (srow.loc[patient_id].astype(float).values if patient_id in srow.index else np.zeros(len(stat_feats), dtype=float))\n        stat_scaler = data[\"scalers\"][\"stat\"]\n        if stat_scaler is not None and s.size>0:\n            s = stat_scaler.transform(s.reshape(1,-1)).ravel()\n    else:\n        s = None\n    last_hour  = pd.to_datetime(sub.loc[idx-1, \"hour\"])\n    return hist_scaled, s, hist_raw, last_hour, sub\n\ndef rolling_forecast_patient(model, data, hourly, patient_id, k=6, split=\"test\", threshold=0.50,\n                             method=FORECAST_METHOD, ema_alpha=EMA_ALPHA, lin_steps=LIN_STEPS):\n    seq_feats   = list(data[\"seq_features_used\"])\n    seq_len     = int(data[\"train\"][\"X_seq\"].shape[1])\n    seq_scaler  = data[\"scalers\"][\"seq\"]\n\n    sub = hourly[(hourly[\"Split\"].astype(str).str.lower()==split.lower()) &\n                 (hourly[\"patientID\"]==patient_id)].sort_values(\"hour\").reset_index(drop=True)\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])  # <<< tz-fix\n    rows = []\n    for anchor_idx in range(seq_len, len(sub)-1):  # each anchor predicts +1..+k\n        last_needed = anchor_idx + k\n        if last_needed >= len(sub): break\n        Xwin_scaled, svec, raw_hist, _, _ = _prepare_window_for_patient_index(hourly, data, patient_id, anchor_idx, split=split)\n        cur_scaled = Xwin_scaled.copy()\n        cur_raw    = raw_hist.copy()\n        F = cur_scaled.shape[1]\n        for h in range(1, k+1):\n            xin = cur_scaled.reshape(1, seq_len, F)\n            prob = (model.predict([xin, svec.reshape(1,-1)], verbose=0).ravel()[0]\n                    if (svec is not None and USE_STATIC_INPUT) else\n                    model.predict(xin, verbose=0).ravel()[0])\n            tgt_idx  = anchor_idx + h\n            tgt_hour = pd.to_datetime(sub.loc[tgt_idx, \"hour\"])\n            y_true   = int(sub.loc[tgt_idx, \"hypo_label\"])\n            # horizon-specific threshold support\n            thr_h = (threshold.get(h, float(threshold.get(1, 0.50)))\n                     if isinstance(threshold, dict) else float(threshold))\n            rows.append({\n                \"patientID\": patient_id,\n                \"anchor_hour\": pd.to_datetime(sub.loc[anchor_idx, \"hour\"]),\n                \"forecast_hour\": tgt_hour,\n                \"horizon\": h,\n                \"y_prob\": float(prob),\n                \"y_pred\": int(prob >= thr_h),\n                \"y_true\": y_true\n            })\n            # roll features using mini forecaster\n            next_raw    = next_feature_vector(cur_raw, seq_feats, method=method, ema_alpha=ema_alpha, lin_steps=lin_steps)\n            next_scaled = seq_scaler.transform(next_raw.reshape(1,-1)).ravel() if seq_scaler is not None else next_raw\n            cur_scaled  = np.vstack([cur_scaled[1:], next_scaled])\n            cur_raw     = np.vstack([cur_raw[1:], next_raw])\n    out = (pd.DataFrame(rows).sort_values([\"forecast_hour\",\"horizon\"]).reset_index(drop=True))\n    out[\"forecast_hour\"] = _to_naive_utc(out[\"forecast_hour\"])  # <<< tz-fix\n    out[\"anchor_hour\"]   = _to_naive_utc(out[\"anchor_hour\"])\n    return out\n\n# -----------------------------------------\n# 4) Metrics by horizon + patient suggestion\n# -----------------------------------------\ndef metrics_by_horizon(df_forecast):\n    out = []\n    for h, g in df_forecast.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        p = g[\"y_prob\"].astype(float).values\n        yhat = g[\"y_pred\"].astype(int).values\n        try: roc = roc_auc_score(y, p)\n        except: roc = np.nan\n        try: pr  = average_precision_score(y, p)\n        except: pr  = np.nan\n        out.append({\n            \"horizon\": h, \"n\": len(g),\n            \"Accuracy\": accuracy_score(y, yhat),\n            \"Precision\": precision_score(y, yhat, zero_division=0),\n            \"Recall\": recall_score(y, yhat, zero_division=0),\n            \"F1\": f1_score(y, yhat, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr\n        })\n    return pd.DataFrame(out).sort_values(\"horizon\")\n\ndef metrics_by_horizon_on_col(df_forecast, col=\"y_pred\"):\n    rows = []\n    for h, g in df_forecast.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        yhat = g[col].astype(int).values\n        try: roc = roc_auc_score(y, g[\"y_prob\"].astype(float).values)\n        except: roc = np.nan\n        try: pr  = average_precision_score(y, g[\"y_prob\"].astype(float).values)\n        except: pr  = np.nan\n        rows.append({\n            \"horizon\": h, \"n\": len(g),\n            \"Accuracy\": accuracy_score(y, yhat),\n            \"Precision\": precision_score(y, yhat, zero_division=0),\n            \"Recall\": recall_score(y, yhat, zero_division=0),\n            \"F1\": f1_score(y, yhat, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr,\n            \"decision\": col\n        })\n    return pd.DataFrame(rows).sort_values(\"horizon\")\n\ndef suggest_patients_for_plots(df_nowcast, strategy=\"positives\", top_n=3):\n    g = df_nowcast.groupby(\"patientID\", as_index=False)\n    if strategy == \"coverage\":\n        s = g.size().rename(columns={\"size\":\"n\"}).sort_values(\"n\", ascending=False)\n        ids = list(s[\"patientID\"].head(top_n))\n    elif strategy == \"auc\":\n        rows = []\n        for pid, grp in df_nowcast.groupby(\"patientID\"):\n            y, p = grp[\"y_true\"].values, grp[\"y_prob\"].values\n            try: pr = average_precision_score(y, p)\n            except: pr = np.nan\n            rows.append({\"patientID\":pid, \"PR_AUC\":pr, \"n\":len(grp)})\n        s = (pd.DataFrame(rows).dropna(subset=[\"PR_AUC\"])\n             .sort_values([\"PR_AUC\",\"n\"], ascending=[False, False]))\n        ids = list(s[\"patientID\"].head(top_n)) if not s.empty else list(df_nowcast[\"patientID\"].value_counts().head(top_n).index)\n    else:  # \"positives\" default\n        pos = g[\"y_true\"].sum().rename(columns={\"y_true\":\"positives\"}) \\\n               .sort_values(\"positives\", ascending=False)\n        ids = list(pos[\"patientID\"].head(top_n))\n        if pos[\"positives\"].max() <= 0:\n            ids = list(df_nowcast[\"patientID\"].value_counts().head(top_n).index)\n    return ids\n\n# -----------------------\n# 5) Post-processing (N-of-M)\n# -----------------------\ndef apply_n_of_m_rule(df, n=2, m=3):\n    \"\"\"df must be one patient; columns: forecast_hour, y_pred.\"\"\"\n    df = df.sort_values(\"forecast_hour\").copy()\n    flags = df[\"y_pred\"].astype(int).values\n    out = np.zeros_like(flags)\n    for i in range(len(flags)):\n        win = flags[max(0, i-m+1):i+1]\n        out[i] = 1 if win.sum() >= n else 0\n    df[\"y_pred_nofm\"] = out\n    return df\n\n# -----------------------\n# 6) Plotting functions\n# -----------------------\ndef plot_nowcast_and_forecast_timeline(df_nowcast, df_forecast, patient_id,\n                                       hours_back=72, out_png=None, threshold=0.50):\n    past = df_nowcast[df_nowcast[\"patientID\"]==patient_id].sort_values(\"hour\").copy()\n    past[\"hour\"] = _to_naive_utc(past[\"hour\"])  # <<< tz-fix\n    if not past.empty and hours_back and hours_back>0:\n        cutoff = past[\"hour\"].max() - pd.Timedelta(hours=hours_back)\n        past = past[past[\"hour\"] >= cutoff]\n    if df_forecast.empty: raise ValueError(\"df_forecast is empty.\")\n    last_anchor = df_forecast[df_forecast[\"patientID\"]==patient_id][\"anchor_hour\"].max()\n    fut = (df_forecast[(df_forecast[\"patientID\"]==patient_id) &\n                       (df_forecast[\"anchor_hour\"]==last_anchor)]\n           .sort_values(\"forecast_hour\").copy())\n    fut[\"forecast_hour\"] = _to_naive_utc(fut[\"forecast_hour\"])  # <<< tz-fix\n\n    fig, ax = plt.subplots(figsize=(10,4))\n    if not past.empty:\n        ax.plot(past[\"hour\"], past[\"y_prob\"], lw=2, label=\"Nowcast (test) prob\")\n        ax.step(past[\"hour\"], past[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label (past)\")\n    ax.plot(fut[\"forecast_hour\"], fut[\"y_prob\"], lw=2, marker=\"o\",\n            label=f\"Forecast next {int(fut['horizon'].max())}h prob\")\n    if \"y_true\" in fut.columns and fut[\"y_true\"].notna().any():\n        ax.step(fut[\"forecast_hour\"], fut[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label (future)\")\n    ax.axhline(float(threshold) if not isinstance(threshold, dict) else float(threshold.get(1, 0.50)),\n               ls=\"--\", label=\"Decision threshold\")\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(\"Hour\"); ax.set_ylabel(\"Risk / Label\")\n    ax.set_title(f\"Patient {patient_id} ‚Äî past nowcast + future forecast\")\n    ax.legend(loc=\"upper left\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d %H:%M\"))\n    fig.autofmt_xdate(); fig.tight_layout()\n    if out_png:\n        fig.savefig(out_png, dpi=200); print(f\"üñºÔ∏è Saved timeline ‚Üí {out_png}\")\n    plt.show()\n\ndef plot_nowcast_vs_forecast_h1(df_nowcast, df_forecast, patient_id, out_png=None):\n    f1 = df_forecast[(df_forecast[\"patientID\"]==patient_id) & (df_forecast[\"horizon\"]==1)].copy()\n    f1 = f1.rename(columns={\"forecast_hour\":\"hour\", \"y_prob\":\"y_prob_forecast_h1\"})\n    past = df_nowcast[df_nowcast[\"patientID\"]==patient_id][[\"hour\",\"y_prob\",\"y_true\"]].copy()\n\n    # --- force both keys to the same dtype/timezone ---\n    past[\"hour\"] = _to_naive_utc(past[\"hour\"])\n    f1[\"hour\"]   = _to_naive_utc(f1[\"hour\"])\n\n    j = past.merge(f1[[\"hour\",\"y_prob_forecast_h1\"]], on=\"hour\", how=\"inner\").sort_values(\"hour\")\n    if j.empty:\n        print(\"[INFO] No overlap between nowcast timeline and horizon-1 forecasts for this patient.\"); return\n    fig, ax = plt.subplots(figsize=(10,4))\n    ax.plot(j[\"hour\"], j[\"y_prob\"], lw=2, label=\"Nowcast prob (test)\")\n    ax.plot(j[\"hour\"], j[\"y_prob_forecast_h1\"], lw=2, linestyle=\"--\", label=\"Forecast@+1h prob\")\n    ax.step(j[\"hour\"], j[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label\")\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(\"Hour\"); ax.set_ylabel(\"Risk / Label\")\n    ax.set_title(f\"Patient {patient_id} ‚Äî nowcast vs forecast@+1h\")\n    ax.legend(loc=\"upper left\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d %H:%M\"))\n    fig.autofmt_xdate(); fig.tight_layout()\n    if out_png:\n        fig.savefig(out_png, dpi=200); print(f\"üñºÔ∏è Saved compare@+1h ‚Üí {out_png}\")\n    plt.show()\n\n# -------------------------------------------------\n# 7) Threshold calibration on VAL (per horizon)\n# -------------------------------------------------\ndef _best_thr_by_fbeta(y, p, beta=1.0, thr_min=0.10, thr_max=0.70):\n    prec, rec, thr = precision_recall_curve(y, p)\n    prec, rec, thr = prec[:-1], rec[:-1], thr\n    mask = np.isfinite(thr) & (thr >= thr_min) & (thr <= thr_max)\n    if not mask.any():\n        return 0.50\n    fb = (1+beta**2) * (prec[mask]*rec[mask]) / (beta**2*prec[mask] + rec[mask] + 1e-9)\n    return float(thr[mask][np.nanargmax(fb)])\n\ndef calibrate_horizon_thresholds_on_val(model, data, hourly, k=12, beta=1.5, method=\"ema\"):\n    vals = []\n    val_mask = hourly[\"Split\"].astype(str).str.lower()==\"val\"\n    val_ids = sorted(hourly.loc[val_mask, \"patientID\"].unique())\n    for pid in val_ids:\n        df_fc_val = rolling_forecast_patient(\n            model, data, hourly, patient_id=pid, k=k, split=\"val\",\n            threshold=0.50, method=method  # temporary; only need probs\n        )\n        if not df_fc_val.empty:\n            vals.append(df_fc_val)\n    if not vals:\n        return {}\n    allv = pd.concat(vals, ignore_index=True)\n    out = {}\n    for h, g in allv.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        p = g[\"y_prob\"].astype(float).values\n        if len(np.unique(y)) < 2:\n            continue\n        out[h] = _best_thr_by_fbeta(y, p, beta=beta, thr_min=0.10, thr_max=0.70)\n    return out\n\n# ======================\n# 8) Run: load, forecast, plot\n# ======================\n# Preconditions\nif \"hourly\" not in globals() or \"data\" not in globals():\n    raise RuntimeError(\"hourly/data not found. Please run your feature builder and sequence builder cells first.\")\n\nCKPT_PATH, _best_method, _best_model = pick_best_checkpoint(OUT_RESULTS_CSV, \"checkpoints\")\nmodel = tf.keras.models.load_model(CKPT_PATH, custom_objects={\"loss\": focal_loss, \"focal_loss\": focal_loss})\n\n# (b) Threshold: VAL-optimal or manual\nif AUTO_USE_VAL_OPTIMAL_THR:\n    thr_val = get_val_optimal_threshold(OUT_RESULTS_CSV, _best_method, _best_model)\n    THRESH   = float(thr_val) if (thr_val is not None) else float(THRESH_MANUAL)\n    if thr_val is None:\n        print(f\"[INFO] Using manual THRESH={THRESH_MANUAL:.2f} (VAL-optimal not found).\")\n    else:\n        print(f\"‚úÖ Using VAL-optimal THRESH={THRESH:.2f}\")\nelse:\n    THRESH = float(THRESH_MANUAL)\n    print(f\"‚ÑπÔ∏è Using manual THRESH={THRESH:.2f}\")\n\n# (c) Nowcast on TEST + suggestions\ndf_test_nowcast = predict_split_prob_df(model, data, hourly, split=\"test\", threshold=THRESH)\nif OVERRIDE_PATIENT_ID is not None and OVERRIDE_PATIENT_ID in set(df_test_nowcast[\"patientID\"].unique()):\n    suggested = [OVERRIDE_PATIENT_ID]\nelse:\n    suggested = suggest_patients_for_plots(df_test_nowcast, strategy=PATIENT_SELECTION_STRATEGY,\n                                           top_n=TOP_N_PATIENTS_TO_PLOT)\nprint(\"üìå Suggested patientID(s) to plot:\", suggested)\n\n# (d) Optional: learn per-horizon thresholds on VAL\nHORIZON_THR = None\nif USE_HORIZON_THRESHOLDS:\n    HORIZON_THR = calibrate_horizon_thresholds_on_val(\n        model, data, hourly, k=K_HOURS, beta=FBETA_FOR_CAL, method=FORECAST_METHOD\n    )\n    print(\"Horizon thresholds (VAL):\", HORIZON_THR)\n\n# (e) Forecast & plot\nfor pid in suggested:\n    print(f\"\\n=== Forecasting patient {pid} | K={K_HOURS}, method={FORECAST_METHOD} ===\")\n    thr_to_use = (HORIZON_THR if (isinstance(HORIZON_THR, dict) and len(HORIZON_THR)>0) else THRESH)\n    df_fc = rolling_forecast_patient(model, data, hourly, patient_id=pid, k=K_HOURS,\n                                     split=\"test\", threshold=thr_to_use,\n                                     method=FORECAST_METHOD, ema_alpha=EMA_ALPHA, lin_steps=LIN_STEPS)\n    print(\"Forecast rows:\", len(df_fc))\n\n    # Optional N-of-M smoothing\n    if APPLY_NOFM and not df_fc.empty:\n        df_fc = df_fc.groupby(\"patientID\", group_keys=False).apply(apply_n_of_m_rule, n=NOFM_N, m=NOFM_M)\n\n    # Metrics\n    hz_base = metrics_by_horizon(df_fc)\n    print(\"Horizon metrics (base decision):\\n\", hz_base)\n    if APPLY_NOFM and \"y_pred_nofm\" in df_fc.columns:\n        hz_nofm = metrics_by_horizon_on_col(df_fc, col=\"y_pred_nofm\")\n        print(\"Horizon metrics (N-of-M decision):\\n\", hz_nofm)\n\n    # Plots\n    outA = os.path.join(SAVE_DIR, f\"nowcast_plus_forecast_patient_{pid}.png\")\n    plot_nowcast_and_forecast_timeline(df_test_nowcast, df_fc, patient_id=pid,\n                                       hours_back=72, threshold=thr_to_use, out_png=outA)\n    outB = os.path.join(SAVE_DIR, f\"nowcast_vs_forecast_h1_patient_{pid}.png\")\n    plot_nowcast_vs_forecast_h1(df_test_nowcast, df_fc, patient_id=pid, out_png=outB)\n\nprint(\"‚úÖ Done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T20:08:29.572494Z","iopub.execute_input":"2025-10-31T20:08:29.572849Z","iopub.status.idle":"2025-10-31T20:08:29.644027Z","shell.execute_reply.started":"2025-10-31T20:08:29.572827Z","shell.execute_reply":"2025-10-31T20:08:29.643083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================\n# Leak-safe Ramadan features + Balanced LSTM\n# (hourly builder + sequences + training utilities + significance testing)\n# ==============================================\nimport os, time, warnings, random, re\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\nfrom scipy.stats import norm  # <-- added\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nSTATIC_CSV      = \"/kaggle/input/hmc-model-static-variables/outcome_static.csv\"\nVISIT_WIDE_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_wide_by_variable.csv\"\nVISIT_LONG_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_long.csv\"\n\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree.npz\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\nOUT_PLOTS_PNG   = \"/kaggle/working/combined_roc_pr_curves.png\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\nHYPO_CUTOFF   = 70.0\nMIN_CGM_PER_H = 4\nSEQ_LEN       = 24\n\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n    \"pc1_activity_energy\",\n    \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n)\n\nRANDOM_STATE     = 42\nTHR_MIN, THR_MAX = 0.40, 0.60\nAUGMENT_SIGMA    = 0.01\nRESAMPLE_METHODS = [\"none\",\"oversample_seq\",\"undersample_seq\",\"smote\",\"smoteenn\",\"smotetomek\"]\nUSE_STATIC_INPUT = True\n\n# --------------------\n# Utilities (robust column picking)\n# --------------------\ndef set_global_seeds(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_global_seeds(RANDOM_STATE)\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef safe_encode_gender(series):\n    if series.dtype == \"object\":\n        return (series.str.strip().str.lower().map({\"male\":1, \"m\":1, \"female\":0, \"f\":0}))\n    return pd.to_numeric(series, errors=\"coerce\")\n\ndef split_patients(unique_pids, test_size=0.2, val_size=0.1, random_state=RANDOM_STATE):\n    train_pids, test_pids = train_test_split(unique_pids, test_size=test_size, random_state=random_state)\n    val_fraction = val_size / max(1e-9, (1.0 - test_size))\n    train_pids, val_pids = train_test_split(train_pids, test_size=val_fraction, random_state=random_state)\n    return np.array(train_pids), np.array(val_pids), np.array(test_pids)\n\ndef _normalize_date(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    return s.dt.normalize()\n\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef _pick_col_flex(df: pd.DataFrame, preferred=None, required=False, name=\"\", must_contain_all=None, any_contains=None):\n    cols = list(df.columns)\n    norm_map = {c: _norm_col(c) for c in cols}\n    if preferred:\n        lower_pref = {str(p).strip().lower(): p for p in preferred}\n        for c in cols:\n            if str(c).strip().lower() in lower_pref:\n                return c\n    if preferred:\n        pref_norm = {_norm_col(p): p for p in preferred}\n        for c, n in norm_map.items():\n            if n in pref_norm:\n                return c\n    cands = []\n    for c, n in norm_map.items():\n        ok = True\n        if must_contain_all:\n            for tok in must_contain_all:\n                if _norm_col(tok) not in n:\n                    ok = False; break\n        if ok and any_contains:\n            if not any(_norm_col(tok) in n for tok in any_contains):\n                ok = False\n        if ok: cands.append(c)\n    if cands:\n        def _priority(col: str):\n            n = norm_map[col]\n            starts_pid = n.startswith(\"patientid\")\n            has_pid    = \"patientid\" in n\n            return (-(starts_pid or has_pid), len(n))\n        cands.sort(key=_priority)\n        return cands[0]\n    if required:\n        raise KeyError(f\"Required column not found for {name}. preferred={preferred} | must_contain_all={must_contain_all} | any_contains={any_contains}. Available: {cols}\")\n    return None\n\ndef _pick_patient_col(df: pd.DataFrame) -> str:\n    preferred = [\"patientID\",\"patientId\",\"PatientID (Huawei Data)\",\"subject_id\",\"patid\",\"pid\",\"id\",\"huaweiid\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"patientID\",\n                          must_contain_all=[\"id\"], any_contains=[\"patient\",\"subject\",\"pat\",\"huawei\"])\n\ndef _pick_date_col(df: pd.DataFrame) -> str:\n    preferred = [\"date\",\"visit_date\",\"Date\",\"day\",\"timestamp\",\"Visit Date\",\"date_of_visit\",\"start\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"date\",\n                          any_contains=[\"date\",\"visit\",\"day\",\"timestamp\",\"start\"])\n\ndef _pick_variable_col(df: pd.DataFrame) -> str:\n    preferred = [\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"variable\",\n                          any_contains=[\"variable\",\"var\",\"feature\",\"name\",\"measure\",\"metric\"])\n\ndef _pick_value_col(df: pd.DataFrame) -> str:\n    preferred = [\"value\",\"val\",\"measure_value\",\"reading\",\"amount\",\"score\"]\n    return _pick_col_flex(df, preferred=preferred, required=True, name=\"value\",\n                          any_contains=[\"value\",\"val\",\"measurevalue\",\"reading\",\"amount\",\"score\"])\n\n# ---------------------------\n# Loaders for external files\n# ---------------------------\ndef load_static_df(static_csv=STATIC_CSV, needed=STATIC_COLS):\n    if not static_csv or not os.path.exists(static_csv):\n        print(\"‚ö†Ô∏è Static CSV not found; static features will be zero-filled.\")\n        return None\n    df = pd.read_csv(static_csv)\n    pid_col = _pick_patient_col(df)\n    df = df.rename(columns={pid_col:\"patientID\"})\n    keep = [\"patientID\"] + [c for c in needed if c in df.columns]\n    df = df[keep].drop_duplicates(subset=[\"patientID\"]).copy()\n    if \"Gender\" in df.columns:\n        df[\"Gender\"] = safe_encode_gender(df[\"Gender\"])\n    for c in keep:\n        if c != \"patientID\":\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    print(f\"‚ÑπÔ∏è static: using patientID column = '{pid_col}'\")\n    return df\n\ndef load_visit_df(visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV, needed=VISIT_COLS):\n    if visit_wide_csv and os.path.exists(visit_wide_csv):\n        wide = pd.read_csv(visit_wide_csv)\n        pid_col  = _pick_patient_col(wide)\n        date_col = _pick_date_col(wide)\n        wide = wide.rename(columns={pid_col:\"patientID\", date_col:\"date\"})\n        wide[\"date\"] = _normalize_date(wide[\"date\"])\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"‚ÑπÔ∏è visit-wide: patientID='{pid_col}', date='{date_col}', kept={keep[2:]}\")\n            return wide[keep].copy()\n        else:\n            print(\"‚ö†Ô∏è VISIT_WIDE_CSV found but none of the needed visit columns present; will try LONG if available.\")\n    if visit_long_csv and os.path.exists(visit_long_csv):\n        long = pd.read_csv(visit_long_csv)\n        pid_col   = _pick_patient_col(long)\n        date_col  = _pick_date_col(long)\n        var_col   = _pick_variable_col(long)\n        value_col = _pick_value_col(long)\n        long = long.rename(columns={pid_col:\"patientID\", date_col:\"date\", var_col:\"variable\", value_col:\"value\"})\n        long[\"date\"] = _normalize_date(long[\"date\"])\n        wide = (long\n                .pivot_table(index=[\"patientID\",\"date\"], columns=\"variable\", values=\"value\", aggfunc=\"mean\")\n                .reset_index())\n        keep = [\"patientID\",\"date\"] + [c for c in needed if c in wide.columns]\n        if len(keep) > 2:\n            print(f\"‚ÑπÔ∏è visit-long: patientID='{pid_col}', date='{date_col}', variables matched={keep[2:]}\")\n            return wide[keep].copy()\n        print(\"‚ö†Ô∏è VISIT_LONG_CSV present but none of the needed variables were found in the pivot.\")\n    print(\"‚ö†Ô∏è No usable visit CSVs found; visit features will be zero-filled.\")\n    return None\n\n# ----------------------------------------------------------------\n# Part A ‚Äî Build hourly Ramadan features and leak-safe transforms\n# ----------------------------------------------------------------\ndef build_hourly_features_with_leak_safe_transforms(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n    static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n    df = pd.read_csv(in_csv)\n    if \"patientID\" not in df.columns:\n        pid_col = _pick_patient_col(df)\n        df = df.rename(columns={pid_col: \"patientID\"})\n        print(f\"‚ÑπÔ∏è intraday: using patientID column = '{pid_col}'\")\n\n    start_col = \"start\" if \"start\" in df.columns else _pick_date_col(df)\n    df[start_col] = to_dt(df[start_col])\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[start_col].dt.date)\n    df[\"hour\"]       = df[start_col].dt.floor(\"h\")\n    df[\"hour_of_day\"]= df[\"hour\"].dt.hour\n\n    df = ensure_numeric(df)\n    df = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"‚ùå Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(\n                       cgm_min=(\"cgm\",\"min\"),\n                       cgm_max=(\"cgm\",\"max\"),\n                       cgm_mean=(\"cgm\",\"mean\"),\n                       cgm_std=(\"cgm\",\"std\")\n                   )\n                   .sort_values([\"patientID\",\"hour\"])\n                   .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean().fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    pids = hourly[\"patientID\"].dropna().unique()\n    train_p, val_p, test_p = split_patients(pids, test_size=test_size, val_size=val_size, random_state=random_state)\n    hourly[\"Split\"] = np.where(hourly[\"patientID\"].isin(train_p), \"train\",\n                        np.where(hourly[\"patientID\"].isin(val_p), \"val\", \"test\"))\n\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"] == \"train\"\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    def _apply_cgm_pca(df_in):\n        X = scal_cgm.transform(df_in[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df_in.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n    hourly = _apply_cgm_pca(hourly)\n\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        pca_life  = PCA(n_components=3, random_state=random_state).fit(\n            scal_life.transform(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        )\n        X_all = scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        Z_all = pca_life.transform(X_all)\n        hourly[\"pc1_activity_energy\"] = Z_all[:,0]\n        hourly[\"pc2_physiology\"]      = Z_all[:,1]\n        hourly[\"pc3_sleep_rest\"]      = Z_all[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    visit_df = load_visit_df(visit_wide_csv, visit_long_csv, VISIT_COLS)\n    hourly[\"date\"] = hourly[\"hour\"].dt.normalize()\n    if visit_df is not None:\n        visit_df[\"date\"] = pd.to_datetime(visit_df[\"date\"], errors=\"coerce\").dt.normalize()\n        visit_df = visit_df[(visit_df[\"date\"] >= RAMADAN_START) & (visit_df[\"date\"] <= RAMADAN_END)].copy()\n        hourly = hourly.merge(visit_df, on=[\"patientID\",\"date\"], how=\"left\")\n    for c in VISIT_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    static_df = load_static_df(static_csv, STATIC_COLS)\n    if static_df is not None:\n        hourly = hourly.merge(static_df, on=\"patientID\", how=\"left\")\n    for c in STATIC_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(cast := hourly[c], errors=\"coerce\").fillna(0.0)\n\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly.to_csv(out_csv, index=False)\n    print(f\"‚úÖ Saved hourly features (leak-safe) ‚Üí {out_csv}\")\n\n    return hourly, (train_p, val_p, test_p)\n\n# ---------------------------------------------------------------\n# Part B ‚Äî Build sequences (optionally with per-patient static)\n# ---------------------------------------------------------------\ndef build_sequences_by_split(hourly, splits, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n                             static_cols=STATIC_COLS, scale_features=True):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float).fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n                if static_mat is not None and pid in static_mat.index:\n                    X_stat.append(static_mat.loc[pid].values.astype(float))\n        X_seq = np.array(X_seq); y = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    seq_scaler  = None\n    stat_scaler = None\n    if scale_features and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n        def _scale_seq(X):\n            if X is None or X.size==0: return X\n            n = X.shape[0]; return seq_scaler.transform(X.reshape(-1, n_f)).reshape(n, SEQ_LEN, n_f)\n        Xtr_s = _scale_seq(Xtr_s); Xva_s = _scale_seq(Xva_s); Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size>0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n        def _scale_stat(X):\n            if X is None or X.size==0: return X\n            return stat_scaler.transform(X)\n        Xtr_stat = _scale_stat(Xtr_stat); Xva_stat = _scale_stat(Xva_stat); Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"‚úÖ Sequences built | train={Xtr_s.shape}, val={Xva_s.shape}, test={Xte_s.shape}\")\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n# ------------------------------------------------------\n# Balanced LSTM pipeline utilities (metrics, resampling)\n# ------------------------------------------------------\nTHR_MIN, THR_MAX = THR_MIN, THR_MAX\n\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask])); idx = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_overall(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef _specificity_per_class(y_true, y_pred, positive_label):\n    y_true_bin = (np.asarray(y_true).ravel() == positive_label).astype(int)\n    y_pred_bin = (np.asarray(y_pred).ravel() == positive_label).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_per_class(y_true, y_pred, positive_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    metrics[\"Overall/Specificity\"]          = _specificity_overall(y_true, y_pred)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/MSE_pred\"]             = mse_pred\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/MSE_prob\"]       = mse_prob\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/MSE_prob\"] = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n    return metrics\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\ndef augment_with_static(X_seq, X_stat, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0:\n        return X_seq, X_stat, y\n    noise = np.random.normal(0, sigma, X_seq.shape)\n    X_seq_aug = np.vstack([X_seq, X_seq + noise])\n    y_aug     = np.hstack([y, y])\n    if X_stat is not None:\n        X_stat_aug = np.vstack([X_stat, X_stat])\n    else:\n        X_stat_aug = None\n    return X_seq_aug, X_stat_aug, y_aug\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE, return_index=False, allow_smote=True):\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n    base_idx = np.arange(n)\n\n    if method == \"none\":\n        return (X, y, base_idx) if return_index else (X, y)\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        rng = np.random.default_rng(random_state)\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0:\n            return (X, y, base_idx) if return_index else (X, y)\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = rng.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = rng.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:\n            if n0 > n1:\n                keep0 = rng.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = rng.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        rng.shuffle(keep)\n        Xr, yr = X[keep], y[keep]\n        return (Xr, yr, keep) if return_index else (Xr, yr)\n\n    if not allow_smote:\n        print(f\"‚ö†Ô∏è {method} disabled when static input is used; falling back to 'none'.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    minority_n = int((y==1).sum())\n    majority_n = int((y==0).sum())\n    if minority_n < 2 or majority_n < 2:\n        print(\"‚ö†Ô∏è Not enough samples for SMOTE/SMOTEENN/SMOTETomek; skipping resampling.\")\n        return (X, y, base_idx) if return_index else (X, y)\n\n    Xf = X.reshape(n, -1)\n    if method == \"smote\":\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n\n    Xr = Xr.reshape(-1, T, F)\n    return (Xr, yr, None) if return_index else (Xr, yr)\n\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state=RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0:\n        return (X_test, y_test, (None if X_stat is None else X_stat))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n# ------------------------------------------------------\n# Model builders (supports seq-only or seq+static)\n# ------------------------------------------------------\ndef make_model(seq_len, n_seq_f, n_stat_f=0, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n        x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x)\n        x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x);                    x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L1\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l1(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l1(1e-5))(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x);                        x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x);                          x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(32, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(32, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(32, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# ==========================\n# Significance / Uncertainty\n# ==========================\ndef delong_bootstrap_auc(y_true, p1, p2, n_boot=2000, random_state=42):\n    \"\"\"\n    Bootstrap test for ŒîAUC = AUC(p1) - AUC(p2).\n    Returns (delta_auc, se, z, p_value_two_sided).\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    y_true = np.asarray(y_true).astype(int).ravel()\n    p1 = np.asarray(p1).astype(float).ravel()\n    p2 = np.asarray(p2).astype(float).ravel()\n\n    diffs = []\n    n = len(y_true)\n    for _ in range(n_boot):\n        idx = rng.integers(0, n, n)\n        yb, p1b, p2b = y_true[idx], p1[idx], p2[idx]\n        try:\n            diffs.append(roc_auc_score(yb, p1b) - roc_auc_score(yb, p2b))\n        except ValueError:\n            continue\n    diffs = np.array(diffs, dtype=float)\n    delta = float(np.nanmean(diffs))\n    se    = float(np.nanstd(diffs, ddof=1))\n    z     = 0.0 if se == 0.0 else delta / se\n    pval  = 2.0 * (1.0 - norm.cdf(abs(z)))\n    return delta, se, z, pval\n\ndef bootstrap_ci_auc(y_true, p, n_boot=2000, alpha=0.05, random_state=42):\n    \"\"\"\n    Percentile bootstrap CI for ROC-AUC. Returns (auc_hat, [lo, hi]).\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    y_true = np.asarray(y_true).astype(int).ravel()\n    p = np.asarray(p).astype(float).ravel()\n\n    stats = []\n    n = len(y_true)\n    for _ in range(n_boot):\n        idx = rng.integers(0, n, n)\n        yb, pb = y_true[idx], p[idx]\n        try:\n            stats.append(roc_auc_score(yb, pb))\n        except ValueError:\n            continue\n    stats = np.array(stats, dtype=float)\n    auc_hat = float(roc_auc_score(y_true, p))\n    lo = float(np.nanpercentile(stats, 2.5))\n    hi = float(np.nanpercentile(stats, 97.5))\n    return auc_hat, [lo, hi]\n\n# ------------------------------------------------------\n# Training runner (VAL for threshold; TEST for final)\n# ------------------------------------------------------\ndef run_balanced_lstm_pipeline(data,\n                               arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n                               resample_methods=RESAMPLE_METHODS,\n                               thr_min=THR_MIN, thr_max=THR_MAX,\n                               random_state=RANDOM_STATE,\n                               results_csv=OUT_RESULTS_CSV,\n                               plots_png=OUT_PLOTS_PNG):\n\n    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n    os.makedirs(os.path.dirname(plots_png), exist_ok=True)\n    os.makedirs(\"checkpoints\", exist_ok=True)\n\n    Xtr, Xtr_stat, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"], data[\"train\"][\"y\"]\n    Xva, Xva_stat, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"X_stat\"],   data[\"val\"][\"y\"]\n    Xte, Xte_stat, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"X_stat\"],  data[\"test\"][\"y\"]\n\n    Xtr, Xtr_stat, ytr = augment_with_static(Xtr, Xtr_stat, ytr, sigma=AUGMENT_SIGMA)\n\n    Xte_bal, yte_bal, Xte_stat_bal = make_balanced_test(Xte, yte, X_stat=Xte_stat)\n\n    results     = {}\n    roc_curves  = {}\n    pr_curves   = {}\n\n    # For post-hoc significance tests\n    prob_store = {}  # (method, arch, split) -> (y_true, y_prob)\n\n    allow_smote = (Xtr_stat is None or not USE_STATIC_INPUT)\n\n    def train_eval_one(method_name, arch_name):\n        nonlocal Xtr, ytr, Xtr_stat\n\n        Xrs, yrs, idx_map = seq_resample(Xtr, ytr, method=method_name, random_state=random_state,\n                                         return_index=True, allow_smote=allow_smote)\n        if Xtr_stat is not None and USE_STATIC_INPUT:\n            Xrs_stat = Xtr_stat if idx_map is None else Xtr_stat[idx_map]\n        else:\n            Xrs_stat = None\n\n        seq_len, n_seq_f = Xrs.shape[1], Xrs.shape[2]\n        n_stat_f = 0 if (Xrs_stat is None or not USE_STATIC_INPUT) else Xrs_stat.shape[1]\n        model = make_model(seq_len, n_seq_f, n_stat_f=n_stat_f, arch=arch_name, lr=1e-3)\n\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n        ckpt_path = f\"checkpoints/{method_name}__{arch_name}.h5\"\n        cp = ModelCheckpoint(ckpt_path, save_best_only=True, monitor=\"val_loss\", verbose=0)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xrs, Xrs_stat], yrs,\n                      validation_data=([Xva, Xva_stat], yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict([Xrs, Xrs_stat], verbose=0).ravel()\n            p_va  = model.predict([Xva, Xva_stat], verbose=0).ravel()\n            p_te  = model.predict([Xte, Xte_stat], verbose=0).ravel()\n            p_teB = model.predict([Xte_bal, Xte_stat_bal], verbose=0).ravel() if Xte_stat_bal is not None else model.predict(Xte_bal, verbose=0).ravel()\n        else:\n            model.fit(Xrs, yrs,\n                      validation_data=(Xva, yva),\n                      epochs=12, batch_size=64, callbacks=[es, cp],\n                      class_weight=make_class_weight(yrs), verbose=1)\n            p_tr  = model.predict(Xrs, verbose=0).ravel()\n            p_va  = model.predict(Xva, verbose=0).ravel()\n            p_te  = model.predict(Xte, verbose=0).ravel()\n            p_teB = model.predict(Xte_bal, verbose=0).ravel()\n\n        # thresholds on VAL\n        try:\n            fpr_va, tpr_va, thr_roc_va = roc_curve(yva, p_va); auc_roc = auc(fpr_va, tpr_va)\n        except ValueError:\n            fpr_va, tpr_va, thr_roc_va, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n        youden_va = tpr_va - fpr_va\n        t_roc, _ = _best_threshold_in_range(thr_roc_va, youden_va, thr_min, thr_max)\n\n        prec_va, rec_va, thr_pr_va = precision_recall_curve(yva, p_va)\n        f1s_va = 2*prec_va[:-1]*rec_va[:-1] / (prec_va[:-1]+rec_va[:-1]+1e-8)\n        t_pr, _ = _best_threshold_in_range(thr_pr_va, f1s_va, thr_min, thr_max)\n        ap_val  = average_precision_score(yva, p_va)\n\n        roc_curves[(method_name, arch_name)] = (fpr_va, tpr_va, auc_roc)\n        pr_curves[(method_name, arch_name)]  = (rec_va, prec_va, ap_val)\n        print(f\"üìå [{method_name}/{arch_name}] VAL thresholds ‚Üí Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{thr_min},{thr_max}])\")\n\n        eval_ts = sorted(set([thr_min, 0.50, thr_max, float(t_roc), float(t_pr)]))\n\n        for t in eval_ts:\n            yhat_tr  = (p_tr  >= t).astype(int)\n            yhat_va  = (p_va  >= t).astype(int)\n            yhat_te  = (p_te  >= t).astype(int)\n            yhat_teB = (p_teB >= t).astype(int)\n\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__train\"]        = evaluate_full_metrics(yrs,     yhat_tr,  p_tr)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__val\"]          = evaluate_full_metrics(yva,     yhat_va,  p_va)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__test\"]         = evaluate_full_metrics(yte,     yhat_te,  p_te)\n            results[f\"{method_name}__{arch_name}__thr_{t:.2f}__testBalanced\"] = evaluate_full_metrics(yte_bal, yhat_teB, p_teB)\n\n        # --- store probabilities for A/B significance (threshold-free AUC) ---\n        prob_store[(method_name, arch_name, \"test\")]         = (yte,     p_te)\n        prob_store[(method_name, arch_name, \"testBalanced\")] = (yte_bal, p_teB)\n\n    # Loop: resampling methods √ó architectures\n    for METHOD in resample_methods:\n        if METHOD in {\"smote\",\"smoteenn\",\"smotetomek\"} and (data[\"train\"][\"X_stat\"] is not None and USE_STATIC_INPUT):\n            print(f\"‚è≠Ô∏è  Skipping {METHOD} (static input enabled).\")\n            continue\n        print(f\"\\nüîÅ Resampling: {METHOD} | train y-dist = {Counter(data['train']['y'])}\")\n        for ARCH in arch_list:\n            train_eval_one(METHOD, ARCH)\n\n    # --- Plots (validation curves)\n    plt.figure(figsize=(14,6))\n    plt.subplot(1,2,1)\n    for (meth, arch), (fpr, tpr, auc_roc) in roc_curves.items():\n        plt.plot(fpr, tpr, label=f'{meth}/{arch} (VAL AUC={auc_roc:.3f})')\n    plt.plot([0,1],[0,1],'--',label='Random')\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (Validation)'); plt.legend(fontsize=8)\n    plt.subplot(1,2,2)\n    for (meth, arch), (rec, prec, ap) in pr_curves.items():\n        plt.plot(rec, prec, label=f'{meth}/{arch} (VAL AP={ap:.3f})')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR (Validation)'); plt.legend(fontsize=8)\n    plt.tight_layout(); plt.savefig(plots_png, dpi=300); plt.show()\n    print(f\"üñºÔ∏è Saved plots ‚Üí {plots_png}\")\n\n    # --- Results CSV\n    results_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\":\"Key\"})\n    k = results_df[\"Key\"].str.strip()\n    results_df[\"Split\"]  = np.where(k.str.endswith(\"__train\"), \"train\",\n                             np.where(k.str.endswith(\"__val\"), \"val\",\n                             np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n                             np.where(k.str.endswith(\"__test\"), \"test\", np.nan))))\n    parts = k.str.split(\"__\")\n    results_df[\"Method\"]    = parts.str[0]\n    results_df[\"Model\"]     = parts.str[1]\n    results_df[\"Threshold\"] = pd.to_numeric(parts.str[2].str.replace(\"thr_\",\"\", regex=False), errors=\"coerce\")\n    results_df.round(6).to_csv(results_csv, index=False)\n    print(f\"üìë Saved results ‚Üí {results_csv}\")\n\n    # ================================\n    # Pairwise AUC significance (A/B)\n    # ================================\n    pairs_to_compare = [\n        ((\"none\",\"LSTM_100\"), (\"oversample_seq\",\"LSTM_100\")),\n        ((\"none\",\"LSTM_100\"), (\"undersample_seq\",\"BiLSTM\")),\n        ((\"oversample_seq\",\"LSTM_100\"), (\"undersample_seq\",\"BiLSTM\")),\n    ]\n    splits_to_use = [\"test\", \"testBalanced\"]\n\n    sig_rows = []\n    for split_name in splits_to_use:\n        for (A, B) in pairs_to_compare:\n            methA, archA = A\n            methB, archB = B\n            keyA = (methA, archA, split_name)\n            keyB = (methB, archB, split_name)\n            if keyA not in prob_store or keyB not in prob_store:\n                continue\n            (yA, pA) = prob_store[keyA]\n            (yB, pB) = prob_store[keyB]\n            y_true = yA  # same split -> same ground truth\n            delta, se, z, p = delong_bootstrap_auc(y_true, pA, pB, n_boot=2000, random_state=random_state)\n            aucA, ciA = bootstrap_ci_auc(y_true, pA, n_boot=2000, alpha=0.05, random_state=random_state)\n            aucB, ciB = bootstrap_ci_auc(y_true, pB, n_boot=2000, alpha=0.05, random_state=random_state)\n            sig_rows.append({\n                \"Split\": split_name,\n                \"ModelA\": f\"{methA}/{archA}\",\n                \"ModelB\": f\"{methB}/{archB}\",\n                \"AUC_A\": aucA, \"AUC_A_CI95_L\": ciA[0], \"AUC_A_CI95_U\": ciA[1],\n                \"AUC_B\": aucB, \"AUC_B_CI95_L\": ciB[0], \"AUC_B_CI95_U\": ciB[1],\n                \"Delta_AUC\": delta, \"SE_Delta\": se, \"Z\": z, \"P_value\": p\n            })\n\n    sig_df = pd.DataFrame(sig_rows)\n    out_sig_csv = (os.path.join(os.path.dirname(results_csv), \"auc_significance.csv\")\n                   if os.path.dirname(results_csv) else \"auc_significance.csv\")\n    sig_df.to_csv(out_sig_csv, index=False)\n    print(f\"üìë Saved AUC significance table ‚Üí {out_sig_csv}\")\n    if not sig_df.empty:\n        print(sig_df.head(10).to_string(index=False))\n\n    return results_df\n\n# --------------------\n# Run end-to-end\n# --------------------\nif __name__ == \"__main__\":\n    hourly, splits = build_hourly_features_with_leak_safe_transforms(\n        in_csv=CSV_INTRADAY_WITH_VISITS,\n        out_csv=OUT_HOURLY_CSV,\n        min_cgm_per_hour=MIN_CGM_PER_H,\n        test_size=0.2, val_size=0.1, random_state=RANDOM_STATE,\n        static_csv=STATIC_CSV, visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV\n    )\n\n    data = build_sequences_by_split(\n        hourly, splits,\n        seq_len=SEQ_LEN,\n        seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n        static_cols=STATIC_COLS,\n        scale_features=True\n    )\n\n    np.savez_compressed(\n        OUT_SEQ_NPZ,\n        Xtr=data[\"train\"][\"X_seq\"],  Xtr_stat=(data[\"train\"][\"X_stat\"] if data[\"train\"][\"X_stat\"] is not None else np.empty((0,0))),\n        ytr=data[\"train\"][\"y\"],\n        Xva=data[\"val\"][\"X_seq\"],    Xva_stat=(data[\"val\"][\"X_stat\"] if data[\"val\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yva=data[\"val\"][\"y\"],\n        Xte=data[\"test\"][\"X_seq\"],   Xte_stat=(data[\"test\"][\"X_stat\"] if data[\"test\"][\"X_stat\"] is not None else np.empty((0,0))),\n        yte=data[\"test\"][\"y\"],\n        seq_features_used=np.array(data[\"seq_features_used\"], dtype=object),\n        static_features_used=np.array(data[\"static_features_used\"], dtype=object)\n    )\n    print(f\"üíæ Saved sequences ‚Üí {OUT_SEQ_NPZ}\")\n\n    results_df = run_balanced_lstm_pipeline(\n        data,\n        arch_list=(\"LSTM_100\",\"BiLSTM\",\"LSTM_50\"),\n        resample_methods=RESAMPLE_METHODS,\n        thr_min=THR_MIN, thr_max=THR_MAX,\n        random_state=RANDOM_STATE,\n        results_csv=OUT_RESULTS_CSV,\n        plots_png=OUT_PLOTS_PNG\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Cross-Phase Validation (Leak-safe)\n#   Train: Ramadan  ‚Üí Test: Shawwal\n#   Train: Shawwal  ‚Üí Test: Ramadan\n# Models: LSTM_100, BiLSTM, LSTM_50, LSTM_25_L2\n# ============================================================\n\nimport os, re, random, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, mean_squared_error\n)\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\n\n# --------------------------\n# CONFIG\n# --------------------------\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE); tf.random.set_seed(RANDOM_STATE)\n\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\n# If you don't have static/visit files ready, leave them None\nSTATIC_CSV      = None\nVISIT_WIDE_CSV  = None\nVISIT_LONG_CSV  = None\n\nRAMADAN_START  = pd.to_datetime(\"2023-03-22\"); RAMADAN_END  = pd.to_datetime(\"2023-04-19\")\nSHAWWAL_START  = pd.to_datetime(\"2023-04-20\"); SHAWWAL_END  = pd.to_datetime(\"2023-05-19\")\n\nSEQ_LEN       = 24\nHYPO_CUTOFF   = 70.0\nMIN_CGM_PER_H = 4\nVAL_FRACTION  = 0.15  # within *training phase* patients\n\nUSE_STATIC_INPUT = False  # this script runs seq-only (set True if you extend with static branch)\nARCH_LIST = (\"LSTM_100\",\"BiLSTM\",\"LSTM_50\",\"LSTM_25_L2\")\nTHR_MIN, THR_MAX = 0.40, 0.60\n\n# Features (sequence)\nLIFESTYLE_COLS_CANDIDATES = [\"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"]\nDEFAULT_SEQ_FEATURE_COLS = (\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\"pc1_activity_energy\"\n)\n\n# --------------------------\n# UTILS\n# --------------------------\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    return {0: float(N/(2.0*n0)), 1: float(N/(2.0*n1))}\n\n# --------------------------\n# DATA: build hourly per window\n# --------------------------\ndef build_hourly_for_window(in_csv, start_date, end_date, min_cgm_per_hour=MIN_CGM_PER_H):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(in_csv)\n    df = pd.read_csv(in_csv)\n\n    # normalize patient column\n    if \"patientID\" not in df.columns:\n        for c in df.columns:\n            if _norm_col(c).startswith(\"patientid\") or _norm_col(c) in {\"patientid\",\"id\",\"huaweiid\"}:\n                df = df.rename(columns={c:\"patientID\"})\n                break\n        if \"patientID\" not in df.columns:\n            raise KeyError(\"patientID column not found.\")\n\n    # time columns\n    time_col = \"start\" if \"start\" in df.columns else (\"timestamp\" if \"timestamp\" in df.columns else None)\n    if time_col is None and \"date\" not in df.columns:\n        raise KeyError(\"No start/timestamp/date column found.\")\n    if time_col is not None:\n        df[time_col] = to_dt(df[time_col])\n        df[\"date\"] = pd.to_datetime(df[time_col].dt.date)\n        df[\"hour\"] = df[time_col].dt.floor(\"h\")\n    else:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n        df[\"hour\"] = df[\"date\"].dt.floor(\"h\")\n\n    df[\"hour_of_day\"] = pd.to_datetime(df[\"hour\"]).dt.hour\n    df = ensure_numeric(df)\n\n    # filter window\n    df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)].copy()\n\n    # CGM present?\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # valid hours\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # hourly summaries\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(cgm_min=(\"cgm\",\"min\"),\n                        cgm_max=(\"cgm\",\"max\"),\n                        cgm_mean=(\"cgm\",\"mean\"),\n                        cgm_std=(\"cgm\",\"std\"))\n                   .sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = pd.to_datetime(hourly[\"hour\"]).dt.hour\n\n    # labels\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # composites\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # lifestyle means (if present)\n    life_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if life_cols:\n        life_hourly = (df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[life_cols].mean().fillna(0.0))\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\")\n    else:\n        for c in [\"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"]:\n            if c not in hourly.columns:\n                hourly[c] = 0.0\n\n    hourly[\"date\"] = pd.to_datetime(pd.to_datetime(hourly[\"hour\"]).dt.date)\n    return hourly\n\n# --------------------------\n# Fit leak-safe PCA (train phase only) and add PCA cols\n# --------------------------\ndef apply_leak_safe_pca(hourly, lifestyle_cols=None):\n    hourly = hourly.copy()\n    assert \"Split\" in hourly.columns, \"hourly must have Split\"\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = hourly[\"Split\"].astype(str).str.lower().eq(\"train\")\n\n    # CGM PCA\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=RANDOM_STATE).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols].fillna(0.0))\n    )\n    X_all = scal_cgm.transform(hourly[cgm_cols].fillna(0.0))\n    Z_all = pca_cgm.transform(X_all)\n    hourly[\"pca_cgm1\"], hourly[\"pca_cgm2\"], hourly[\"pca_cgm3\"] = Z_all[:,0], Z_all[:,1], Z_all[:,2]\n\n    # Lifestyle PCA (if present)\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, lifestyle_cols].fillna(0.0))\n        ZL = PCA(n_components=3, random_state=RANDOM_STATE).fit_transform(\n            scal_life.transform(hourly[lifestyle_cols].fillna(0.0))\n        )\n        hourly[\"pc1_activity_energy\"], hourly[\"pc2_physiology\"], hourly[\"pc3_sleep_rest\"] = ZL[:,0], ZL[:,1], ZL[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n\n    return hourly\n\n# --------------------------\n# Build sequences by split\n# --------------------------\ndef build_sequences(hourly, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS):\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing {c}\")\n\n    hourly = hourly.copy()\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    missing = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing:\n        raise KeyError(f\"Missing seq features: {missing}\")\n\n    train_p = sorted(hourly.loc[hourly[\"Split\"].eq(\"train\"), \"patientID\"].unique())\n    val_p   = sorted(hourly.loc[hourly[\"Split\"].eq(\"val\"),   \"patientID\"].unique())\n    test_p  = sorted(hourly.loc[hourly[\"Split\"].eq(\"test\"),  \"patientID\"].unique())\n\n    def _build_for(pid_list):\n        sub = hourly[hourly[\"patientID\"].isin(pid_list)].sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n        X, y = [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len: continue\n            feats  = grp[list(seq_feature_cols)].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n            for i in range(len(grp)-seq_len):\n                X.append(feats[i:i+seq_len]); y.append(labels[i+seq_len])\n        return (np.array(X), np.array(y).astype(int))\n\n    Xtr, ytr = _build_for(train_p)\n    Xva, yva = _build_for(val_p)\n    Xte, yte = _build_for(test_p)\n\n    # scaler on TRAIN only (sequence features)\n    seq_scaler = None\n    if Xtr.size > 0:\n        F = Xtr.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr.reshape(-1, F))\n        def _scale(X):\n            if X is None or X.size==0: return X\n            n, T, F = X.shape\n            return seq_scaler.transform(X.reshape(-1, F)).reshape(n, T, F)\n        Xtr = _scale(Xtr); Xva = _scale(Xva); Xte = _scale(Xte)\n\n    return {\n        \"train\": {\"X_seq\": Xtr, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte, \"y\": yte},\n        \"seq_features_used\": list(seq_feature_cols),\n        \"scalers\": {\"seq\": seq_scaler}\n    }\n\n# --------------------------\n# Models\n# --------------------------\ndef make_model(seq_len, n_seq_f, arch=\"LSTM_100\", lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = seq_in\n    if arch == \"BiLSTM\":\n        x = Bidirectional(LSTM(64, return_sequences=True))(x); x = Dropout(0.2)(x)\n        x = Bidirectional(LSTM(32))(x); x = Dropout(0.2)(x)\n        x = Dense(16, activation=\"relu\")(x)\n    elif arch == \"LSTM_50\":\n        x = LSTM(50, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(25)(x); x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\")(x)\n    elif arch == \"LSTM_25_L2\":\n        x = LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = LSTM(25, kernel_regularizer=l2(1e-5))(x); x = Dropout(0.2)(x)\n        x = Dense(10, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n    else:  # LSTM_100\n        x = LSTM(100, return_sequences=True)(x); x = Dropout(0.2)(x)\n        x = LSTM(50)(x); x = Dropout(0.2)(x)\n        x = Dense(25, activation=\"relu\")(x)\n\n    out = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=seq_in, outputs=out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=focal_loss(), metrics=[\"accuracy\"])\n    return model\n\n# --------------------------\n# Threshold selection & evaluation\n# --------------------------\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx = np.where(mask)[0][int(np.nanargmax(scores[mask]))]\n        return float(thresholds[idx])\n    # fallback: best overall, then clamp\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max))\n\ndef pick_val_thresholds(y_val, p_val, thr_min=THR_MIN, thr_max=THR_MAX):\n    # Youden's J\n    try:\n        fpr, tpr, thr_roc = roc_curve(y_val, p_val)\n        youden = tpr - fpr\n        t_roc = _best_threshold_in_range(thr_roc, youden, thr_min, thr_max)\n    except Exception:\n        t_roc = 0.50\n    # PR-F1\n    try:\n        prec, rec, thr_pr = precision_recall_curve(y_val, p_val)\n        f1s = 2*prec[:-1]*rec[:-1] / (prec[:-1]+rec[:-1]+1e-9)\n        t_pr = _best_threshold_in_range(thr_pr, f1s, thr_min, thr_max)\n    except Exception:\n        t_pr = 0.50\n    return t_roc, t_pr\n\ndef eval_probs(y_true, y_prob, thresholds=(0.40, 0.50, 0.60)):\n    \"\"\"\n    Evaluate thresholded metrics at specified operating points, plus AUC/PR-AUC/Brier (threshold-free).\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_prob = np.asarray(y_prob).astype(float).ravel()\n\n    try:\n        roc = roc_auc_score(y_true, y_prob)\n    except Exception:\n        roc = np.nan\n    try:\n        pr  = average_precision_score(y_true, y_prob)\n    except Exception:\n        pr  = np.nan\n    brier = mean_squared_error(y_true, y_prob)\n\n    rows = []\n    for t in thresholds:\n        yhat = (y_prob >= float(t)).astype(int)\n        rows.append({\n            \"Threshold\": round(float(t), 2),\n            \"Accuracy\": accuracy_score(y_true, yhat),\n            \"F1_weighted\": f1_score(y_true, yhat, average=\"weighted\", zero_division=0),\n            \"Prec_1\": precision_score(y_true, yhat, pos_label=1, zero_division=0),\n            \"Recall_1\": recall_score(y_true, yhat, pos_label=1, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr, \"Brier\": brier\n        })\n    return pd.DataFrame(rows)\n\n# --------------------------\n# Core runner (one direction)\n# --------------------------\ndef run_cross_phase(train_window, test_window, out_prefix=\"ram_to_sha\"):\n    \"\"\"\n    train_window/test_window: (start_date, end_date)\n    \"\"\"\n    # A) Build hourly tables for both phases\n    hourly_train = build_hourly_for_window(CSV_INTRADAY_WITH_VISITS, *train_window)\n    hourly_test  = build_hourly_for_window(CSV_INTRADAY_WITH_VISITS, *test_window)\n\n    # B) Create patient splits: TRAIN/VAL within train phase; TEST from test phase (no overlap)\n    train_patients_all = sorted(hourly_train[\"patientID\"].unique().tolist())\n    test_patients_all  = sorted(hourly_test[\"patientID\"].unique().tolist())\n\n    # hold-out VAL from train phase (patient-wise)\n    rng = np.random.default_rng(RANDOM_STATE)\n    rng.shuffle(train_patients_all)\n    n_val = max(1, int(len(train_patients_all) * VAL_FRACTION))\n    val_patients = sorted(train_patients_all[:n_val])\n    tr_patients  = sorted(train_patients_all[n_val:])\n\n    # ensure cross-phase TEST patients don't overlap training phase patients\n    test_only = sorted(list(set(test_patients_all) - set(tr_patients) - set(val_patients)))\n    if len(test_only) == 0:\n        # fallback: allow all test phase patients (still leak-free wrt transforms, but same subjects across phases)\n        test_only = test_patients_all\n\n    hourly_train[\"Split\"] = np.where(hourly_train[\"patientID\"].isin(tr_patients), \"train\",\n                              np.where(hourly_train[\"patientID\"].isin(val_patients), \"val\", \"drop\"))\n    hourly_train = hourly_train[hourly_train[\"Split\"] != \"drop\"].copy()\n\n    hourly_test[\"Split\"]  = np.where(hourly_test[\"patientID\"].isin(test_only), \"test\", \"drop\")\n    hourly_test = hourly_test[hourly_test[\"Split\"] != \"drop\"].copy()\n\n    # C) Merge to single hourly dataframe with Split: train/val/test\n    hourly = pd.concat([hourly_train, hourly_test], ignore_index=True)\n\n    # D) Leak-safe PCA (fit on TRAIN only)\n    life_cols_present = [c for c in LIFESTYLE_COLS_CANDIDATES if c in hourly.columns]\n    hourly = apply_leak_safe_pca(hourly, lifestyle_cols=life_cols_present)\n\n    # E) Build sequences\n    data = build_sequences(hourly, seq_len=SEQ_LEN, seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS)\n    Xtr, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"y\"]\n    Xva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"y\"]\n    Xte, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"y\"]\n\n    if any(arr is None or arr.size == 0 for arr in [Xtr, ytr, Xva, yva, Xte, yte]):\n        raise RuntimeError(\"Insufficient sequences in one of the splits. Check coverage or windows.\")\n\n    results_rows = []\n\n    # F) Train/evaluate per architecture\n    for arch in ARCH_LIST:\n        model = make_model(seq_len=Xtr.shape[1], n_seq_f=Xtr.shape[2], arch=arch, lr=1e-3)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=0)\n\n        hist = model.fit(Xtr, ytr,\n                         validation_data=(Xva, yva),\n                         epochs=12, batch_size=64,\n                         callbacks=[es],\n                         class_weight=make_class_weight(ytr),\n                         verbose=0)\n\n        # Predict\n        p_tr = model.predict(Xtr, verbose=0).ravel()\n        p_va = model.predict(Xva, verbose=0).ravel()\n        p_te = model.predict(Xte, verbose=0).ravel()\n\n        # Pick thresholds on VAL\n        t_roc, t_pr = pick_val_thresholds(yva, p_va, thr_min=THR_MIN, thr_max=THR_MAX)\n        eval_df = eval_probs(yte, p_te, thresholds=(THR_MIN, 0.50, THR_MAX, t_roc, t_pr))\n        eval_df.insert(0, \"Model\", arch)\n        eval_df.insert(1, \"Direction\", out_prefix)\n        results_rows.append(eval_df)\n\n    results = pd.concat(results_rows, ignore_index=True)\n    os.makedirs(\"/kaggle/working\", exist_ok=True)\n    out_csv = f\"/kaggle/working/crossphase_{out_prefix}.csv\"\n    results.to_csv(out_csv, index=False)\n    print(f\"‚úÖ Saved ‚Üí {out_csv}\")\n    return results\n\n# --------------------------\n# RUN BOTH DIRECTIONS\n# --------------------------\nramadan = (RAMADAN_START, RAMADAN_END)\nshawwal  = (SHAWWAL_START, SHAWWAL_END)\n\nres_ram_to_sha = run_cross_phase(ramadan, shawwal, out_prefix=\"Ramadan_to_Shawwal\")\nres_sha_to_ram = run_cross_phase(shawwal, ramadan, out_prefix=\"Shawwal_to_Ramadan\")\n\n# Combine + pivot for quick view\ncombined = pd.concat([res_ram_to_sha, res_sha_to_ram], ignore_index=True)\ncombined.to_csv(\"/kaggle/working/crossphase_combined.csv\", index=False)\n\n# Produce a compact comparison at VAL-optimized PR-F1 threshold (we pick the row with Threshold == t_pr ~= within [0.40,0.60])\ndef _pick_best_rows(df):\n    # We approximated two VAL-optimal thresholds (t_roc, t_pr) and included both in eval table.\n    # Here we pick the row with the highest F1_weighted per Model/Direction.\n    key_cols = [\"Direction\",\"Model\"]\n    best = (df.sort_values([\"Direction\",\"Model\",\"F1_weighted\"], ascending=[True, True, False])\n              .groupby(key_cols, as_index=False)\n              .head(1)\n              .reset_index(drop=True))\n    return best[[\"Direction\",\"Model\",\"Threshold\",\"ROC_AUC\",\"PR_AUC\",\"F1_weighted\",\"Recall_1\",\"Prec_1\",\"Brier\"]]\n\nsummary = _pick_best_rows(combined)\nsummary.to_csv(\"/kaggle/working/crossphase_summary_best.csv\", index=False)\nprint(\"üßæ Summary (best per Model/Direction):\")\nprint(summary.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T08:12:22.334284Z","iopub.execute_input":"2025-10-29T08:12:22.334997Z","iopub.status.idle":"2025-10-29T08:21:08.911634Z","shell.execute_reply.started":"2025-10-29T08:12:22.334973Z","shell.execute_reply":"2025-10-29T08:21:08.910816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================\n# FULL FIXED UTILITIES: Sequences + Analysis + Forecasting\n#  ‚úÖ Shape fix: scaling uses actual (n, T, F), never hard-coded SEQ_LEN\n#  ‚úÖ Calibration fix: VAL-optimal threshold propagates into daily risk + downstream\n#     - reads results_summary_all.csv when available\n#     - otherwise calibrates on VAL predictions (fallback)\n#  ‚úÖ Daily risk aggregates write threshold_used into CSVs\n#  ‚úÖ Forecasting script re-computes daily_risk_test.csv using THRESH / HORIZON_THR[1]\n# ======================================================\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    precision_recall_curve,\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, average_precision_score\n)\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\n# ---------------------------\n# Globals / safe fallbacks\n# ---------------------------\ntry:\n    OUT_RESULTS_CSV\nexcept NameError:\n    OUT_RESULTS_CSV = \"/kaggle/working/results_summary_all.csv\"\n\ntry:\n    USE_STATIC_INPUT\nexcept NameError:\n    USE_STATIC_INPUT = True\n\ntry:\n    RANDOM_STATE\nexcept NameError:\n    RANDOM_STATE = 42\n\ntry:\n    DEFAULT_SEQ_FEATURE_COLS\nexcept NameError:\n    DEFAULT_SEQ_FEATURE_COLS = (\n        \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n        \"pc1_activity_energy\",\n        \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n    )\n\ntry:\n    STATIC_COLS\nexcept NameError:\n    STATIC_COLS = [\n        \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n        \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n    ]\n\ntry:\n    VISIT_COLS\nexcept NameError:\n    VISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\n\nVISIT_FEATURES       = VISIT_COLS\nSHAP_BACKGROUND_SIZE = 512\nSHAP_TEST_SAMPLES    = 1024\n\n# fallback only (should not be silently used if results/val calibration exists)\nRISK_THRESHOLD       = 0.50\n\n\n# ======================================================\n# Threshold resolver (results CSV ‚Üí fallback to VAL calibration)\n# ======================================================\ndef _pick_score_col(df: pd.DataFrame, preferred=\"Overall/F1_weighted\"):\n    if preferred in df.columns:\n        return preferred\n    for c in [\"Overall/F1_macro\", \"Overall/F1\", \"F1\", \"f1\", \"val_f1\", \"Overall/PR_AUC\", \"PR_AUC\", \"Overall/ROC_AUC\", \"ROC_AUC\"]:\n        if c in df.columns:\n            return c\n    return None\n\ndef get_val_optimal_threshold(results_csv: str,\n                              method: str = None,\n                              model: str = None,\n                              preferred_score_col=\"Overall/F1_weighted\"):\n    \"\"\"\n    Return best VAL threshold for a specific (method, model) if provided,\n    else best VAL threshold overall.\n    \"\"\"\n    if not results_csv or not os.path.exists(results_csv):\n        return None\n\n    try:\n        df = pd.read_csv(results_csv)\n    except Exception:\n        return None\n\n    if \"Split\" not in df.columns or \"Threshold\" not in df.columns:\n        return None\n\n    dfv = df[df[\"Split\"].astype(str).str.lower().eq(\"val\")].copy()\n    if dfv.empty:\n        return None\n\n    if method is not None and \"Method\" in dfv.columns:\n        dfv = dfv[dfv[\"Method\"].astype(str) == str(method)]\n    if model is not None and \"Model\" in dfv.columns:\n        dfv = dfv[dfv[\"Model\"].astype(str) == str(model)]\n\n    if dfv.empty:\n        return None\n\n    sc = _pick_score_col(dfv, preferred=preferred_score_col)\n    if sc is not None:\n        dfv = dfv.sort_values(sc, ascending=False)\n\n    try:\n        thr = float(dfv.iloc[0][\"Threshold\"])\n        return thr if np.isfinite(thr) else None\n    except Exception:\n        return None\n\ndef get_best_val_threshold_from_results(results_csv: str = OUT_RESULTS_CSV,\n                                        preferred_score_col=\"Overall/F1_weighted\"):\n    \"\"\"\n    Returns (thr, method, model) from best VAL row overall (by preferred score col if present).\n    \"\"\"\n    if not results_csv or not os.path.exists(results_csv):\n        return None, None, None\n\n    df = pd.read_csv(results_csv)\n    if \"Split\" not in df.columns or \"Threshold\" not in df.columns:\n        return None, None, None\n\n    dfv = df[df[\"Split\"].astype(str).str.lower().eq(\"val\")].copy()\n    if dfv.empty:\n        return None, None, None\n\n    sc = _pick_score_col(dfv, preferred=preferred_score_col)\n    if sc is not None:\n        dfv = dfv.sort_values(sc, ascending=False)\n\n    best = dfv.iloc[0]\n    try:\n        thr = float(best[\"Threshold\"])\n        if not np.isfinite(thr):\n            return None, None, None\n    except Exception:\n        return None, None, None\n\n    return thr, str(best.get(\"Method\", \"\")), str(best.get(\"Model\", \"\"))\n\ndef _best_thr_by_fbeta(y, p, beta=1.0, thr_min=0.05, thr_max=0.95, default=0.50):\n    y = np.asarray(y).astype(int).ravel()\n    p = np.asarray(p).astype(float).ravel()\n    if len(np.unique(y)) < 2:\n        return float(default)\n\n    prec, rec, thr = precision_recall_curve(y, p)\n    prec, rec = prec[:-1], rec[:-1]\n    thr = np.asarray(thr)\n\n    mask = np.isfinite(thr) & (thr >= thr_min) & (thr <= thr_max)\n    if not mask.any():\n        return float(default)\n\n    fb = (1 + beta**2) * (prec[mask] * rec[mask]) / (beta**2 * prec[mask] + rec[mask] + 1e-12)\n    return float(thr[mask][np.nanargmax(fb)])\n\ndef calibrate_threshold_on_val(model, data, beta=1.0, default=0.50):\n    \"\"\"\n    Fallback if results CSV isn't available: calibrate directly on VAL probabilities.\n    Uses PR-curve FŒ≤ optimum (Œ≤=1 => F1).\n    \"\"\"\n    Xv = data[\"val\"][\"X_seq\"]\n    Sv = data[\"val\"][\"X_stat\"]\n    yv = np.asarray(data[\"val\"][\"y\"]).astype(int).ravel()\n\n    if Xv is None or getattr(Xv, \"size\", 0) == 0:\n        return float(default)\n\n    if USE_STATIC_INPUT and (Sv is not None and getattr(Sv, \"size\", 0) > 0):\n        pv = model.predict([Xv, Sv], verbose=0).ravel()\n    else:\n        pv = model.predict(Xv, verbose=0).ravel()\n\n    return _best_thr_by_fbeta(yv, pv, beta=beta, default=default)\n\ndef resolve_decision_threshold(threshold=None,\n                               results_csv=OUT_RESULTS_CSV,\n                               model_obj=None,\n                               data=None,\n                               beta_for_val_fallback=1.0):\n    \"\"\"\n    Returns (thr_use, src_string).\n      - if threshold is provided: use it\n      - else: try results_csv best VAL threshold\n      - else: calibrate on VAL via model+data (fallback)\n      - else: use RISK_THRESHOLD (last resort)\n    \"\"\"\n    if threshold is not None:\n        return threshold, \"manual\"\n\n    thr_csv, m, mo = get_best_val_threshold_from_results(results_csv)\n    if thr_csv is not None:\n        return float(thr_csv), f\"results CSV (VAL best: {m}/{mo})\"\n\n    if model_obj is not None and data is not None and \"val\" in data:\n        thr = calibrate_threshold_on_val(model_obj, data, beta=beta_for_val_fallback, default=RISK_THRESHOLD)\n        return float(thr), \"VAL calibration (fallback)\"\n\n    return float(RISK_THRESHOLD), \"fallback default (RISK_THRESHOLD)\"\n\n\n# ======================================================\n# build_sequences_by_split (shape-safe scaling)\n# ======================================================\ndef build_sequences_by_split(\n    hourly: pd.DataFrame,\n    splits,\n    seq_len: int,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    scale_features: bool = True\n):\n    \"\"\"\n    Build (X_seq, X_stat, y) arrays for train/val/test given hourly data and patient splits.\n      - Scalers fit on TRAIN only\n      - Scaling reshape uses actual (n, T, F)\n    \"\"\"\n    for c in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n\n    hourly = hourly.copy()\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    seq_feature_cols = list(seq_feature_cols)\n    missing_seq = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing_seq:\n        raise KeyError(f\"Sequence feature(s) not found in hourly: {missing_seq}\")\n\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    if static_cols_present and USE_STATIC_INPUT:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float)\n                      .fillna(0.0))\n    else:\n        static_mat = None\n        static_cols_present = []\n\n    train_p, val_p, test_p = splits\n\n    def _build_for_pidset(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        sub = sub.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n\n        X_seq, X_stat, y = [], [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len])\n                y.append(labels[i+seq_len])\n\n                if USE_STATIC_INPUT and static_mat is not None:\n                    if pid in static_mat.index:\n                        X_stat.append(static_mat.loc[pid].values.astype(float))\n                    else:\n                        X_stat.append(np.zeros(len(static_cols_present), dtype=float))\n\n        X_seq = np.array(X_seq)\n        y     = np.array(y).astype(int)\n        X_stat = np.array(X_stat) if (USE_STATIC_INPUT and static_mat is not None and len(X_stat) > 0) else None\n        return X_seq, X_stat, y\n\n    Xtr_s, Xtr_stat, ytr = _build_for_pidset(train_p)\n    Xva_s, Xva_stat, yva = _build_for_pidset(val_p)\n    Xte_s, Xte_stat, yte = _build_for_pidset(test_p)\n\n    seq_scaler  = None\n    stat_scaler = None\n\n    if scale_features and Xtr_s is not None and Xtr_s.size > 0:\n        n_f = Xtr_s.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr_s.reshape(-1, n_f))\n\n        def _scale_seq(X):\n            if X is None or X.size == 0:\n                return X\n            n, T, F = X.shape\n            return seq_scaler.transform(X.reshape(-1, F)).reshape(n, T, F)\n\n        Xtr_s = _scale_seq(Xtr_s)\n        Xva_s = _scale_seq(Xva_s)\n        Xte_s = _scale_seq(Xte_s)\n\n    if scale_features and Xtr_stat is not None and Xtr_stat.size > 0:\n        stat_scaler = StandardScaler().fit(Xtr_stat)\n\n        def _scale_stat(X):\n            if X is None or X.size == 0:\n                return X\n            return stat_scaler.transform(X)\n\n        Xtr_stat = _scale_stat(Xtr_stat)\n        Xva_stat = _scale_stat(Xva_stat)\n        Xte_stat = _scale_stat(Xte_stat)\n\n    print(f\"‚úÖ Sequences built | train={getattr(Xtr_s,'shape',None)}, val={getattr(Xva_s,'shape',None)}, test={getattr(Xte_s,'shape',None)}\")\n\n    return {\n        \"train\": {\"X_seq\": Xtr_s, \"X_stat\": Xtr_stat, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva_s, \"X_stat\": Xva_stat, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte_s, \"X_stat\": Xte_stat, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n\n# ======================================================\n# Canonical sequence index map\n# ======================================================\ndef _build_sequence_index_map(hourly_df: pd.DataFrame, split: str, seq_len: int) -> pd.DataFrame:\n    \"\"\"\n    Recreate exact sequence ordering used in build_sequences_by_split:\n    seq window i targets the (i+seq_len)-th hour per patient.\n    \"\"\"\n    sub = (hourly_df[hourly_df[\"Split\"].astype(str).str.lower() == split.lower()]\n           .sort_values([\"patientID\",\"hour\"])\n           .reset_index())\n    sub = sub.rename(columns={\"index\": \"row_idx\"})\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\n                \"seq_idx\": len(rows),\n                \"patientID\": pid,\n                \"hour\": pd.to_datetime(tgt[\"hour\"]),\n                \"date\": pd.to_datetime(tgt.get(\"date\", pd.NaT)),\n                \"visit_assigned\": tgt.get(\"visit_assigned\", np.nan),\n                \"period_main\": tgt.get(\"period_main\", np.nan),\n                \"row_idx\": int(tgt[\"row_idx\"]),\n            })\n    return pd.DataFrame(rows)\n\n\n# ======================================================\n# SHAP on visit features (optional)\n# ======================================================\ndef compute_visit_shap(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    seq_features_used,\n    visit_features=None,\n    split: str = \"test\",\n    background_size: int = SHAP_BACKGROUND_SIZE,\n    max_test_windows: int = SHAP_TEST_SAMPLES,\n    out_dir: str = \"/kaggle/working\",\n    seq_len: int = None\n):\n    os.makedirs(out_dir, exist_ok=True)\n    visit_features = list(visit_features) if visit_features is not None else list(VISIT_FEATURES)\n\n    Xte = data[split][\"X_seq\"]\n    if seq_len is None:\n        if Xte is None or Xte.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len from data arrays.\")\n        seq_len = Xte.shape[1]\n\n    feat_to_idx = {f: i for i, f in enumerate(seq_features_used)}\n    visit_in_seq = [f for f in visit_features if f in feat_to_idx]\n    if not visit_in_seq:\n        raise ValueError(\n            f\"No visit features found inside seq_features_used. \"\n            f\"visit_features={visit_features} | seq_features_used={seq_features_used}\"\n        )\n\n    Xtr, Xtr_stat = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"]\n    Xte, Xte_stat = data[split][\"X_seq\"],  data[split][\"X_stat\"]\n\n    bg_n = min(int(background_size), len(Xtr))\n    te_n = min(int(max_test_windows), len(Xte))\n    if bg_n < 1 or te_n < 1:\n        raise RuntimeError(f\"Not enough sequences for SHAP (bg_n={bg_n}, te_n={te_n}).\")\n\n    rng   = np.random.default_rng(42)\n    bg_ix = rng.choice(np.arange(len(Xtr)), size=bg_n, replace=False)\n    te_ix = rng.choice(np.arange(len(Xte)), size=te_n, replace=False)\n\n    bg_seq, te_seq = Xtr[bg_ix], Xte[te_ix]\n    if USE_STATIC_INPUT and (Xtr_stat is not None and Xte_stat is not None and Xtr_stat.size > 0 and Xte_stat.size > 0):\n        bg_static, te_static = Xtr_stat[bg_ix], Xte_stat[te_ix]\n    else:\n        bg_static = te_static = None\n\n    try:\n        import shap\n    except Exception as e:\n        raise ImportError(\"Install shap to run this: pip install shap\") from e\n\n    try:\n        if bg_static is not None:\n            explainer   = shap.DeepExplainer(model, [bg_seq, bg_static])\n            shap_values = explainer.shap_values([te_seq, te_static])\n        else:\n            explainer   = shap.DeepExplainer(model, bg_seq)\n            shap_values = explainer.shap_values(te_seq)\n    except Exception as e:\n        print(f\"[WARN] DeepExplainer failed ({e}). Falling back to GradientExplainer‚Ä¶\")\n        if bg_static is not None:\n            explainer   = shap.GradientExplainer(model, [bg_seq, bg_static])\n            shap_values = explainer.shap_values([te_seq, te_static])\n        else:\n            explainer   = shap.GradientExplainer(model, bg_seq)\n            shap_values = explainer.shap_values(te_seq)\n\n    shap_seq = shap_values[0] if isinstance(shap_values, list) else shap_values\n    if shap_seq.ndim != 3:\n        raise RuntimeError(f\"Unexpected SHAP shape: {shap_seq.shape}\")\n\n    shap_abs_time = np.mean(np.abs(shap_seq), axis=1)  # [n_samples, F]\n\n    global_rows = []\n    for f in visit_in_seq:\n        global_rows.append({\"feature\": f, \"mean_abs_shap\": float(np.mean(shap_abs_time[:, feat_to_idx[f]]))})\n    global_visit_df = pd.DataFrame(global_rows).sort_values(\"mean_abs_shap\", ascending=False)\n\n    gpath = os.path.join(out_dir, \"shap_visit_global.csv\")\n    global_visit_df.to_csv(gpath, index=False)\n    print(\"‚úÖ Saved global visit SHAP ‚Üí\", gpath)\n\n    seq_map = _build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(Xte):\n        raise RuntimeError(f\"Mapping length {len(seq_map)} != X_{split} length {len(Xte)}\")\n\n    seq_map_sub = seq_map.iloc[te_ix].reset_index(drop=True)\n    per_rows = []\n    for i in range(len(seq_map_sub)):\n        pid = seq_map_sub.loc[i, \"patientID\"]\n        dte = pd.to_datetime(seq_map_sub.loc[i, \"date\"])\n        for f in visit_in_seq:\n            per_rows.append({\n                \"patientID\": pid,\n                \"date\": dte,\n                \"feature\": f,\n                \"mean_abs_shap\": float(shap_abs_time[i, feat_to_idx[f]])\n            })\n\n    per_visit_df = (pd.DataFrame(per_rows)\n                    .groupby([\"patientID\",\"date\",\"feature\"], as_index=False)[\"mean_abs_shap\"].mean()\n                    .sort_values([\"patientID\",\"date\",\"mean_abs_shap\"], ascending=[True, True, False]))\n\n    ppath = os.path.join(out_dir, \"shap_visit_per_visit.csv\")\n    per_visit_df.to_csv(ppath, index=False)\n    print(\"‚úÖ Saved per-visit SHAP ‚Üí\", ppath)\n\n    return global_visit_df, per_visit_df\n\n\n# ======================================================\n# Daily risk aggregation (CALIBRATED threshold + provenance)\n# ======================================================\ndef aggregate_hourly_to_daily_risk(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    split: str = \"test\",\n    threshold = RISK_THRESHOLD,          # float OR dict (horizon thresholds)\n    out_dir: str = \"/kaggle/working\",\n    seq_len: int = None\n):\n    os.makedirs(out_dir, exist_ok=True)\n\n    # Accept float OR dict; daily nowcast aligns with horizon=1\n    thr = float(threshold.get(1, threshold.get(\"default\", 0.50))) if isinstance(threshold, dict) else float(threshold)\n\n    Xs      = data[split][\"X_seq\"]\n    Xs_stat = data[split][\"X_stat\"]\n    y_true  = np.asarray(data[split][\"y\"]).astype(int).ravel()\n\n    if seq_len is None:\n        if Xs is None or Xs.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len from data arrays.\")\n        seq_len = Xs.shape[1]\n\n    if USE_STATIC_INPUT and (Xs_stat is not None and getattr(Xs_stat, \"size\", 0) > 0):\n        y_prob = model.predict([Xs, Xs_stat], verbose=0).ravel()\n    else:\n        y_prob = model.predict(Xs, verbose=0).ravel()\n\n    y_pred = (y_prob >= thr).astype(int)\n\n    seq_map = _build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(y_true):\n        raise RuntimeError(\n            f\"Sequence map length {len(seq_map)} != prediction length {len(y_true)} \"\n            f\"(seq_len={seq_len}, X_{split}.shape={getattr(Xs,'shape',None)})\"\n        )\n\n    pred_df = pd.DataFrame({\n        \"patientID\": seq_map[\"patientID\"].values,\n        \"hour\":      pd.to_datetime(seq_map[\"hour\"].values),\n        \"date\":      pd.to_datetime(seq_map[\"date\"].values, errors=\"coerce\"),\n        \"visit_assigned\": seq_map.get(\"visit_assigned\", pd.Series([np.nan]*len(seq_map))).values,\n        \"period_main\":    seq_map.get(\"period_main\",    pd.Series([np.nan]*len(seq_map))).values,\n        \"y_true\":    y_true,\n        \"y_prob\":    y_prob,\n        \"y_pred\":    y_pred,\n        \"threshold_used\": thr,\n    })\n\n    # If date missing, derive from hour\n    pred_df[\"date\"] = pred_df[\"date\"].fillna(pred_df[\"hour\"].dt.floor(\"D\"))\n    pred_df[\"date\"] = pred_df[\"date\"].dt.normalize()\n\n    grp = pred_df.groupby([\"patientID\",\"date\"], as_index=False)\n    daily = grp.agg(\n        n_windows=(\"y_true\",\"size\"),\n        true_positives=(\"y_true\",\"sum\"),\n        pred_positives=(\"y_pred\",\"sum\"),\n        risk_mean=(\"y_prob\",\"mean\"),\n        risk_max=(\"y_prob\",\"max\"),\n        risk_p95=(\"y_prob\", lambda x: float(np.quantile(x, 0.95))),\n        hours_above_thr=(\"y_pred\",\"sum\"),\n    )\n    daily[\"prevalence\"] = daily[\"true_positives\"] / daily[\"n_windows\"].replace(0, np.nan)\n    daily[\"threshold_used\"] = thr\n\n    daily_csv = os.path.join(out_dir, f\"daily_risk_{split}.csv\")\n    daily.to_csv(daily_csv, index=False)\n    print(f\"‚úÖ Saved daily risk aggregates ‚Üí {daily_csv} (thr={thr:.3f})\")\n\n    # Optional quick plot\n    try:\n        example_pid = daily[\"patientID\"].iloc[0]\n        dsub = daily[daily[\"patientID\"] == example_pid].sort_values(\"date\")\n        plt.figure(figsize=(8,3))\n        plt.plot(dsub[\"date\"], dsub[\"risk_mean\"], label=\"Mean daily risk\")\n        plt.plot(dsub[\"date\"], dsub[\"risk_max\"],  label=\"Max daily risk\")\n        plt.axhline(thr, linestyle=\"--\", label=f\"thr={thr:.2f}\")\n        plt.xlabel(\"Date\"); plt.ylabel(\"Risk\"); plt.title(f\"Daily risk ‚Äî patient {example_pid}\")\n        plt.legend(); plt.tight_layout()\n        png = os.path.join(out_dir, f\"daily_risk_trend_patient_{example_pid}.png\")\n        plt.savefig(png, dpi=200); plt.close()\n        print(f\"üñºÔ∏è Saved example daily trend ‚Üí {png}\")\n    except Exception as e:\n        print(f\"[WARN] Could not plot daily trend example: {e}\")\n\n    return pred_df, daily\n\n\n# ======================================================\n# Balanced test subset (shape-safe)\n# ======================================================\ndef make_balanced_test(X_test, y_test, X_stat=None, random_state: int = RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0:\n        return (X_test, y_test, (None if X_stat is None else np.asarray(X_stat)))\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    Xb, yb = X_test[keep], y_test[keep]\n    Xsb = (None if X_stat is None else np.asarray(X_stat)[keep])\n    return Xb, yb, Xsb\n\n\n# ======================================================\n# Quick model builder (only used if model=None)\n# ======================================================\ndef _make_quick_model(seq_len, n_seq_f, n_stat_f=0, lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = LSTM(64, return_sequences=True)(seq_in)\n    x = Dropout(0.2)(x)\n    x = LSTM(32)(x)\n    x = Dropout(0.2)(x)\n    x = Dense(16, activation=\"relu\")(x)\n\n    if n_stat_f and n_stat_f > 0 and USE_STATIC_INPUT:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(16, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(16, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(16, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n\n# ======================================================\n# Load sequences from NPZ (always includes scalers keys)\n# ======================================================\ndef _load_sequences_npz(npz_path):\n    d = np.load(npz_path, allow_pickle=True)\n\n    def _arr(name):\n        return None if name not in d or d[name] is None or getattr(d[name], \"size\", 0) == 0 else d[name]\n\n    data = {\n        \"train\": {\"X_seq\": _arr(\"Xtr\"), \"X_stat\": _arr(\"Xtr_stat\"), \"y\": d[\"ytr\"]},\n        \"val\":   {\"X_seq\": _arr(\"Xva\"), \"X_stat\": _arr(\"Xva_stat\"), \"y\": d[\"yva\"]},\n        \"test\":  {\"X_seq\": _arr(\"Xte\"), \"X_stat\": _arr(\"Xte_stat\"), \"y\": d[\"yte\"]},\n        \"seq_features_used\": list(d[\"seq_features_used\"]) if \"seq_features_used\" in d else list(DEFAULT_SEQ_FEATURE_COLS),\n        \"static_features_used\": list(d[\"static_features_used\"]) if \"static_features_used\" in d else [],\n        \"scalers\": {\"seq\": None, \"stat\": None},\n    }\n    return data\n\n\n# ======================================================\n# File helpers + splits\n# ======================================================\ndef _first_existing(paths):\n    for p in paths:\n        if p and os.path.exists(p):\n            return p\n    return None\n\ndef _splits_from_hourly(hourly: pd.DataFrame):\n    if \"Split\" not in hourly.columns:\n        raise RuntimeError(\"hourly must include a 'Split' column with 'train'/'val'/'test' assignments.\")\n    train_p = np.array(sorted(hourly.loc[hourly[\"Split\"].astype(str).str.lower()==\"train\",\"patientID\"].unique()))\n    val_p   = np.array(sorted(hourly.loc[hourly[\"Split\"].astype(str).str.lower()==\"val\",\"patientID\"].unique()))\n    test_p  = np.array(sorted(hourly.loc[hourly[\"Split\"].astype(str).str.lower()==\"test\",\"patientID\"].unique()))\n    if len(train_p)==0 or len(val_p)==0 or len(test_p)==0:\n        raise RuntimeError(\"Split assignments missing: need non-empty train/val/test patient sets.\")\n    return (train_p, val_p, test_p)\n\n\n# ======================================================\n# RUNNER (now resolves threshold once and reuses it everywhere)\n# ======================================================\ndef run_analyses_all(\n    model=None, data=None, hourly=None,\n    split=\"test\", out_dir=None, seq_len_default=24,\n    do_shap=True, train_epochs=6,\n    results_csv=OUT_RESULTS_CSV,\n    threshold=None,\n    beta_for_val_fallback=1.0\n):\n    \"\"\"\n    If model/data/hourly not supplied:\n      - loads hourly csv (dynamic_hourly_features_ramadan.csv) if present\n      - loads sequences NPZ (sequences_leakfree.npz) if present\n      - else rebuilds sequences from hourly\n      - trains a small quick LSTM if model=None\n\n    Then:\n      - resolves decision threshold (manual ‚Üí results csv ‚Üí val calibration)\n      - optional SHAP\n      - daily risk aggregation using resolved threshold (NOT 0.50)\n    \"\"\"\n    if out_dir is None:\n        out_dir = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else os.path.join(\".\", \"outputs\")\n    os.makedirs(out_dir, exist_ok=True)\n\n    # Load hourly\n    if hourly is None:\n        HOURLY_CANDIDATES = [\n            \"/kaggle/working/dynamic_hourly_features_ramadan.csv\",\n            \"/mnt/data/dynamic_hourly_features_ramadan.csv\",\n            \"/kaggle/input/hmc-model-static-variables/dynamic_hourly_features_ramadan.csv\",\n        ]\n        hp = _first_existing(HOURLY_CANDIDATES)\n        if not hp:\n            raise FileNotFoundError(\"Could not find hourly CSV. Pass `hourly` DataFrame explicitly.\")\n        hourly = pd.read_csv(hp)\n        print(f\"üìÑ Loaded hourly table: {hp} | shape={hourly.shape}\")\n\n    # Load/build sequences\n    if data is None:\n        NPZ_CANDIDATES = [\n            \"/kaggle/working/sequences_leakfree.npz\",\n            \"/kaggle/working/outputs/sequences_leakfree.npz\",\n            \"/mnt/data/sequences_leakfree.npz\",\n        ]\n        npz = _first_existing(NPZ_CANDIDATES)\n        if npz:\n            data = _load_sequences_npz(npz)\n            print(f\"üì¶ Loaded sequences NPZ: {npz}\")\n        else:\n            splits = _splits_from_hourly(hourly)\n            data = build_sequences_by_split(\n                hourly, splits, seq_len=seq_len_default,\n                seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n                static_cols=STATIC_COLS,\n                scale_features=True\n            )\n\n    # Train quick model if needed\n    if model is None:\n        Xtr, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"y\"]\n        Xva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"y\"]\n        Xtr_stat = data[\"train\"][\"X_stat\"]\n        Xva_stat = data[\"val\"][\"X_stat\"]\n\n        seq_len  = Xtr.shape[1]\n        n_seq_f  = Xtr.shape[2]\n        n_stat_f = 0 if (Xtr_stat is None or not USE_STATIC_INPUT) else Xtr_stat.shape[1]\n\n        model = _make_quick_model(seq_len, n_seq_f, n_stat_f=n_stat_f, lr=1e-3)\n        es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n\n        if n_stat_f > 0 and USE_STATIC_INPUT:\n            model.fit([Xtr, Xtr_stat], ytr,\n                      validation_data=([Xva, Xva_stat], yva),\n                      epochs=train_epochs, batch_size=64,\n                      callbacks=[es], verbose=1)\n        else:\n            model.fit(Xtr, ytr,\n                      validation_data=(Xva, yva),\n                      epochs=train_epochs, batch_size=64,\n                      callbacks=[es], verbose=1)\n\n        print(\"‚úÖ Model trained (quick fit).\")\n\n    # Resolve threshold ONCE\n    thr_use, src = resolve_decision_threshold(\n        threshold=threshold,\n        results_csv=results_csv,\n        model_obj=model,\n        data=data,\n        beta_for_val_fallback=beta_for_val_fallback\n    )\n    thr_print = float(thr_use.get(1, thr_use)) if isinstance(thr_use, dict) else float(thr_use)\n    print(f\"üéöÔ∏è Decision threshold in use: {thr_print:.3f}  [{src}]\")\n\n    # Optional SHAP\n    if do_shap:\n        try:\n            _ = compute_visit_shap(\n                model, data, hourly, data.get(\"seq_features_used\", DEFAULT_SEQ_FEATURE_COLS),\n                visit_features=VISIT_FEATURES, split=split, out_dir=out_dir\n            )\n        except ImportError as e:\n            print(f\"‚ö†Ô∏è SHAP not installed; skipping. ({e})\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è SHAP skipped due to error: {e}\")\n\n    # Daily risk (CALIBRATED)\n    pred_df, daily = aggregate_hourly_to_daily_risk(\n        model, data, hourly,\n        split=split,\n        threshold=thr_use,\n        out_dir=out_dir\n    )\n\n    with pd.option_context(\"display.width\", 160, \"display.max_columns\", 30):\n        print(\"\\nüîé Predictions (head):\")\n        print(pred_df.head(8).to_string(index=False))\n        print(\"\\nüìä Daily risk (head):\")\n        print(daily.head(8).to_string(index=False))\n\n    print(\"\\n‚úÖ Done. Outputs saved under:\", out_dir)\n    return model, data, hourly, thr_use\n\n\n# ======================================================\n# ===================== FORECASTING ====================\n# ======================================================\n\n# --- Datetime helpers: make all times tz-naive in UTC ---\ndef _to_naive_utc(s):\n    s = pd.to_datetime(s, errors=\"coerce\")\n    if getattr(s.dt, \"tz\", None) is not None:\n        s = s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n    return s\n\n# focal_loss (so load_model(custom_objects=...) always works)\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef pick_best_checkpoint(results_csv=OUT_RESULTS_CSV, ckpt_dir=\"checkpoints\", preferred_score_col=\"Overall/F1_weighted\"):\n    if not os.path.exists(results_csv):\n        raise FileNotFoundError(f\"Results CSV not found: {results_csv}\")\n    df = pd.read_csv(results_csv)\n    dfv = df[df[\"Split\"].astype(str).str.lower().eq(\"val\")].copy()\n    if dfv.empty:\n        raise RuntimeError(\"No VAL rows found in results; cannot pick best model.\")\n    sc = _pick_score_col(dfv, preferred=preferred_score_col)\n    if sc is not None:\n        dfv = dfv.sort_values(sc, ascending=False)\n\n    best = dfv.iloc[0]\n    method = str(best.get(\"Method\", \"\"))\n    model  = str(best.get(\"Model\", \"\"))\n\n    ckpt_path = os.path.join(ckpt_dir, f\"{method}__{model}.h5\")\n    if not os.path.exists(ckpt_path):\n        files = [os.path.join(ckpt_dir, f) for f in os.listdir(ckpt_dir) if f.endswith(\".h5\")]\n        if not files:\n            raise FileNotFoundError(\"No .h5 checkpoints found in checkpoints dir.\")\n        ckpt_path = files[0]\n        print(f\"[WARN] Expected checkpoint not found; using {ckpt_path}\")\n\n    print(f\"‚úÖ Best (VAL) ‚Üí {method}/{model} | ckpt = {ckpt_path}\")\n    return ckpt_path, method, model\n\ndef predict_split_prob_df(model, data, hourly, split=\"test\", threshold=0.50):\n    Xs = data[split][\"X_seq\"]\n    Xstat = data[split][\"X_stat\"]\n    ytrue = np.asarray(data[split][\"y\"]).astype(int).ravel()\n    if Xs is None or Xs.ndim != 3:\n        raise ValueError(f\"No sequences for split={split}.\")\n    yprob = (model.predict([Xs, Xstat], verbose=0).ravel()\n             if (USE_STATIC_INPUT and Xstat is not None and getattr(Xstat, \"size\", 0) > 0)\n             else model.predict(Xs, verbose=0).ravel())\n    ypred = (yprob >= float(threshold)).astype(int)\n    seq_len = Xs.shape[1]\n\n    sub = (hourly[hourly[\"Split\"].astype(str).str.lower() == split.lower()]\n           .sort_values([\"patientID\",\"hour\"]).reset_index())\n    sub = sub.rename(columns={\"index\":\"row_idx\"})\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\n                \"seq_idx\":len(rows),\n                \"patientID\":pid,\n                \"hour\":pd.to_datetime(tgt[\"hour\"]),\n                \"date\":pd.to_datetime(tgt.get(\"date\", pd.NaT))\n            })\n    idx_map = pd.DataFrame(rows)\n    if len(idx_map) != len(ytrue):\n        raise RuntimeError(f\"Mapping length {len(idx_map)} != predictions length {len(ytrue)}.\")\n\n    out = pd.DataFrame({\n        \"patientID\": idx_map[\"patientID\"].values,\n        \"hour\":      pd.to_datetime(idx_map[\"hour\"].values),\n        \"date\":      pd.to_datetime(idx_map[\"date\"].values),\n        \"y_true\":    ytrue,\n        \"y_prob\":    yprob,\n        \"y_pred\":    ypred\n    }).sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    out[\"hour\"] = _to_naive_utc(out[\"hour\"])\n    return out\n\n# ---- feature forecasters for rolling window simulation ----\ndef _ema_next(v, alpha=0.6):\n    s = v[0]\n    for x in v[1:]:\n        s = alpha*x + (1-alpha)*s\n    return float(s)\n\ndef _linear_next(v):\n    n = len(v)\n    if n < 2:\n        return float(v[-1])\n    x = np.arange(n, dtype=float)\n    try:\n        b1, b0 = np.polyfit(x, v.astype(float), 1)\n        return float(b1*(n) + b0)\n    except Exception:\n        return float(v[-1])\n\ndef next_feature_vector(hist_raw, feat_names, method=\"ema\", ema_alpha=0.6, lin_steps=6):\n    T, F = hist_raw.shape\n    out = np.zeros(F, dtype=float)\n    for j, name in enumerate(feat_names):\n        col = hist_raw[:, j]\n        if method == \"persistence\":\n            out[j] = float(col[-1])\n        elif method == \"linear\":\n            w = min(len(col), max(2, lin_steps))\n            out[j] = _linear_next(col[-w:])\n        else:\n            if any(k in str(name).lower() for k in [\"cgm\",\"pca\",\"pc1\",\"pc2\",\"pc3\",\"steps\",\"calories\",\"heart\"]):\n                w = min(len(col), max(2, lin_steps))\n                out[j] = _ema_next(col[-w:], alpha=ema_alpha)\n            else:\n                out[j] = float(col[-1])\n    return out\n\ndef _prepare_window_for_patient_index(hourly, data, patient_id, idx, split=\"test\"):\n    seq_feats = list(data[\"seq_features_used\"])\n    seq_len   = int(data[\"train\"][\"X_seq\"].shape[1])\n\n    sub = hourly[(hourly[\"Split\"].astype(str).str.lower()==split.lower()) &\n                 (hourly[\"patientID\"]==patient_id)].sort_values(\"hour\").reset_index(drop=True)\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])\n    if idx < seq_len:\n        raise ValueError(\"idx must be >= seq_len\")\n\n    hist_raw   = sub.loc[idx-seq_len:idx-1, seq_feats].astype(float).values\n    seq_scaler = data.get(\"scalers\", {}).get(\"seq\", None)\n    hist_scaled= seq_scaler.transform(hist_raw) if seq_scaler is not None else hist_raw\n\n    # static\n    if USE_STATIC_INPUT and data[\"train\"][\"X_stat\"] is not None:\n        stat_feats = list(data.get(\"static_features_used\", []))\n        if stat_feats:\n            srow = (hourly[[\"patientID\"]+stat_feats]\n                    .drop_duplicates(subset=[\"patientID\"])\n                    .set_index(\"patientID\"))\n            s = (srow.loc[patient_id].astype(float).values\n                 if patient_id in srow.index else np.zeros(len(stat_feats), dtype=float))\n            stat_scaler = data.get(\"scalers\", {}).get(\"stat\", None)\n            if stat_scaler is not None and s.size > 0:\n                s = stat_scaler.transform(s.reshape(1,-1)).ravel()\n        else:\n            s = None\n    else:\n        s = None\n\n    last_hour  = pd.to_datetime(sub.loc[idx-1, \"hour\"])\n    return hist_scaled, s, hist_raw, last_hour, sub\n\ndef rolling_forecast_patient(model, data, hourly, patient_id, k=6, split=\"test\", threshold=0.50,\n                             method=\"ema\", ema_alpha=0.6, lin_steps=6):\n    seq_feats   = list(data[\"seq_features_used\"])\n    seq_len     = int(data[\"train\"][\"X_seq\"].shape[1])\n    seq_scaler  = data.get(\"scalers\", {}).get(\"seq\", None)\n\n    sub = hourly[(hourly[\"Split\"].astype(str).str.lower()==split.lower()) &\n                 (hourly[\"patientID\"]==patient_id)].sort_values(\"hour\").reset_index(drop=True)\n    sub[\"hour\"] = _to_naive_utc(sub[\"hour\"])\n\n    rows = []\n    for anchor_idx in range(seq_len, len(sub)-1):\n        last_needed = anchor_idx + k\n        if last_needed >= len(sub):\n            break\n\n        Xwin_scaled, svec, raw_hist, _, _ = _prepare_window_for_patient_index(hourly, data, patient_id, anchor_idx, split=split)\n        cur_scaled = Xwin_scaled.copy()\n        cur_raw    = raw_hist.copy()\n        F = cur_scaled.shape[1]\n\n        for h in range(1, k+1):\n            xin = cur_scaled.reshape(1, seq_len, F)\n\n            if (USE_STATIC_INPUT and svec is not None and getattr(svec, \"size\", 0) > 0):\n                prob = model.predict([xin, svec.reshape(1,-1)], verbose=0).ravel()[0]\n            else:\n                prob = model.predict(xin, verbose=0).ravel()[0]\n\n            tgt_idx  = anchor_idx + h\n            tgt_hour = pd.to_datetime(sub.loc[tgt_idx, \"hour\"])\n            y_true   = int(sub.loc[tgt_idx, \"hypo_label\"])\n\n            thr_h = (threshold.get(h, float(threshold.get(1, 0.50)))\n                     if isinstance(threshold, dict) else float(threshold))\n\n            rows.append({\n                \"patientID\": patient_id,\n                \"anchor_hour\": pd.to_datetime(sub.loc[anchor_idx, \"hour\"]),\n                \"forecast_hour\": tgt_hour,\n                \"horizon\": h,\n                \"y_prob\": float(prob),\n                \"y_pred\": int(prob >= thr_h),\n                \"y_true\": y_true\n            })\n\n            next_raw    = next_feature_vector(cur_raw, seq_feats, method=method, ema_alpha=ema_alpha, lin_steps=lin_steps)\n            next_scaled = (seq_scaler.transform(next_raw.reshape(1,-1)).ravel()\n                           if seq_scaler is not None else next_raw)\n            cur_scaled  = np.vstack([cur_scaled[1:], next_scaled])\n            cur_raw     = np.vstack([cur_raw[1:], next_raw])\n\n    out = pd.DataFrame(rows).sort_values([\"forecast_hour\",\"horizon\"]).reset_index(drop=True)\n    out[\"forecast_hour\"] = _to_naive_utc(out[\"forecast_hour\"])\n    out[\"anchor_hour\"]   = _to_naive_utc(out[\"anchor_hour\"])\n    return out\n\ndef metrics_by_horizon(df_forecast):\n    out = []\n    for h, g in df_forecast.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        p = g[\"y_prob\"].astype(float).values\n        yhat = g[\"y_pred\"].astype(int).values\n        try: roc = roc_auc_score(y, p)\n        except: roc = np.nan\n        try: pr  = average_precision_score(y, p)\n        except: pr  = np.nan\n        out.append({\n            \"horizon\": h, \"n\": len(g),\n            \"Accuracy\": accuracy_score(y, yhat),\n            \"Precision\": precision_score(y, yhat, zero_division=0),\n            \"Recall\": recall_score(y, yhat, zero_division=0),\n            \"F1\": f1_score(y, yhat, zero_division=0),\n            \"ROC_AUC\": roc, \"PR_AUC\": pr\n        })\n    return pd.DataFrame(out).sort_values(\"horizon\")\n\ndef apply_n_of_m_rule(df, n=2, m=3):\n    df = df.sort_values(\"forecast_hour\").copy()\n    flags = df[\"y_pred\"].astype(int).values\n    out = np.zeros_like(flags)\n    for i in range(len(flags)):\n        win = flags[max(0, i-m+1):i+1]\n        out[i] = 1 if win.sum() >= n else 0\n    df[\"y_pred_nofm\"] = out\n    return df\n\ndef plot_nowcast_and_forecast_timeline(df_nowcast, df_forecast, patient_id,\n                                       hours_back=72, out_png=None, threshold=0.50):\n    past = df_nowcast[df_nowcast[\"patientID\"]==patient_id].sort_values(\"hour\").copy()\n    past[\"hour\"] = _to_naive_utc(past[\"hour\"])\n    if not past.empty and hours_back and hours_back > 0:\n        cutoff = past[\"hour\"].max() - pd.Timedelta(hours=hours_back)\n        past = past[past[\"hour\"] >= cutoff]\n\n    if df_forecast.empty:\n        raise ValueError(\"df_forecast is empty.\")\n\n    last_anchor = df_forecast[df_forecast[\"patientID\"]==patient_id][\"anchor_hour\"].max()\n    fut = (df_forecast[(df_forecast[\"patientID\"]==patient_id) &\n                       (df_forecast[\"anchor_hour\"]==last_anchor)]\n           .sort_values(\"forecast_hour\").copy())\n    fut[\"forecast_hour\"] = _to_naive_utc(fut[\"forecast_hour\"])\n\n    fig, ax = plt.subplots(figsize=(10,4))\n    if not past.empty:\n        ax.plot(past[\"hour\"], past[\"y_prob\"], lw=2, label=\"Nowcast prob\")\n        ax.step(past[\"hour\"], past[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label (past)\")\n    ax.plot(fut[\"forecast_hour\"], fut[\"y_prob\"], lw=2, marker=\"o\",\n            label=f\"Forecast next {int(fut['horizon'].max())}h prob\")\n    if fut[\"y_true\"].notna().any():\n        ax.step(fut[\"forecast_hour\"], fut[\"y_true\"], where=\"post\", alpha=0.35, label=\"True label (future)\")\n\n    thr_line = float(threshold.get(1, 0.50)) if isinstance(threshold, dict) else float(threshold)\n    ax.axhline(thr_line, ls=\"--\", label=f\"Decision thr={thr_line:.2f}\")\n\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(\"Hour\"); ax.set_ylabel(\"Risk / Label\")\n    ax.set_title(f\"Patient {patient_id} ‚Äî past nowcast + future forecast\")\n    ax.legend(loc=\"upper left\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d %H:%M\"))\n    fig.autofmt_xdate(); fig.tight_layout()\n\n    if out_png:\n        fig.savefig(out_png, dpi=200)\n        print(f\"üñºÔ∏è Saved timeline ‚Üí {out_png}\")\n    plt.show()\n\ndef calibrate_horizon_thresholds_on_val(model, data, hourly, k=12, beta=1.5, method=\"ema\"):\n    vals = []\n    val_mask = hourly[\"Split\"].astype(str).str.lower()==\"val\"\n    val_ids = sorted(hourly.loc[val_mask, \"patientID\"].unique())\n    for pid in val_ids:\n        df_fc_val = rolling_forecast_patient(\n            model, data, hourly,\n            patient_id=pid, k=k, split=\"val\",\n            threshold=0.50, method=method\n        )\n        if not df_fc_val.empty:\n            vals.append(df_fc_val)\n    if not vals:\n        return {}\n\n    allv = pd.concat(vals, ignore_index=True)\n    out = {}\n    for h, g in allv.groupby(\"horizon\"):\n        y = g[\"y_true\"].astype(int).values\n        p = g[\"y_prob\"].astype(float).values\n        if len(np.unique(y)) < 2:\n            continue\n        out[h] = _best_thr_by_fbeta(y, p, beta=beta, thr_min=0.05, thr_max=0.95, default=0.50)\n    return out\n\n\n# ======================================================\n# Example Forecast Runner (call this in your notebook)\n#   - loads best ckpt\n#   - gets THRESH from results CSV\n#   - optionally calibrates per-horizon thresholds on VAL\n#   - NOW ALSO recomputes daily_risk_test.csv using that threshold (fix)\n# ======================================================\ndef run_forecasting_demo(\n    hourly, data,\n    ckpt_dir=\"checkpoints\",\n    save_dir=\"/kaggle/working\",\n    override_patient_id=69,\n    k_hours=12,\n    auto_use_val_optimal_thr=True,\n    thresh_manual=0.49,\n    forecast_method=\"ema\",\n    ema_alpha=0.6,\n    lin_steps=6,\n    use_horizon_thresholds=True,\n    fbeta_for_cal=1.5,\n    apply_nofm=True,\n    nofm_n=2, nofm_m=3\n):\n    os.makedirs(save_dir, exist_ok=True)\n\n    ckpt_path, best_method, best_model = pick_best_checkpoint(OUT_RESULTS_CSV, ckpt_dir)\n    model = tf.keras.models.load_model(ckpt_path, custom_objects={\"loss\": focal_loss, \"focal_loss\": focal_loss})\n\n    # THRESH from results CSV for the chosen best model (fallback to manual)\n    if auto_use_val_optimal_thr:\n        thr_val = get_val_optimal_threshold(OUT_RESULTS_CSV, method=best_method, model=best_model)\n        THRESH = float(thr_val) if thr_val is not None else float(thresh_manual)\n        print(f\"üéöÔ∏è Using THRESH={THRESH:.3f} ({'VAL-optimal' if thr_val is not None else 'manual fallback'})\")\n    else:\n        THRESH = float(thresh_manual)\n        print(f\"üéöÔ∏è Using THRESH={THRESH:.3f} (manual)\")\n\n    # Nowcast df for plotting\n    df_test_nowcast = predict_split_prob_df(model, data, hourly, split=\"test\", threshold=THRESH)\n\n    # Optional per-horizon thresholds on VAL\n    HORIZON_THR = {}\n    if use_horizon_thresholds:\n        HORIZON_THR = calibrate_horizon_thresholds_on_val(\n            model, data, hourly, k=k_hours, beta=fbeta_for_cal, method=forecast_method\n        )\n        print(\"Horizon thresholds (VAL):\", HORIZON_THR)\n\n    thr_to_use = HORIZON_THR if (isinstance(HORIZON_THR, dict) and len(HORIZON_THR) > 0) else THRESH\n\n    # ‚úÖ FIX: daily risk recomputed using THRESH or horizon-1 threshold (NOT 0.50)\n    _ = aggregate_hourly_to_daily_risk(\n        model, data, hourly,\n        split=\"test\",\n        threshold=thr_to_use,   # dict -> uses [1] internally; float -> uses float\n        out_dir=save_dir\n    )\n\n    # Choose patient\n    pid = override_patient_id\n    if pid not in set(df_test_nowcast[\"patientID\"].unique()):\n        pid = int(df_test_nowcast[\"patientID\"].value_counts().index[0])\n\n    df_fc = rolling_forecast_patient(\n        model, data, hourly,\n        patient_id=pid, k=k_hours,\n        split=\"test\",\n        threshold=thr_to_use,\n        method=forecast_method,\n        ema_alpha=ema_alpha,\n        lin_steps=lin_steps\n    )\n    if apply_nofm and not df_fc.empty:\n        df_fc = df_fc.groupby(\"patientID\", group_keys=False).apply(apply_n_of_m_rule, n=nofm_n, m=nofm_m)\n\n    print(\"Forecast rows:\", len(df_fc))\n    print(\"Horizon metrics:\\n\", metrics_by_horizon(df_fc).to_string(index=False))\n\n    out_png = os.path.join(save_dir, f\"nowcast_plus_forecast_patient_{pid}.png\")\n    plot_nowcast_and_forecast_timeline(df_test_nowcast, df_fc, patient_id=pid,\n                                       hours_back=72, out_png=out_png, threshold=thr_to_use)\n\n    return model, df_test_nowcast, df_fc, THRESH, HORIZON_THR\n\n\n# ======================================================\n# Optional: auto-run analyses if executed as a script\n# (In notebooks, you can ignore this.)\n# ======================================================\nif __name__ == \"__main__\":\n    _ = run_analyses_all(\n        model=None,\n        data=None,\n        hourly=None,\n        split=\"test\",\n        out_dir=None,\n        seq_len_default=24,\n        do_shap=True,\n        train_epochs=6,\n        results_csv=OUT_RESULTS_CSV,\n        threshold=None,             # set float here to force\n        beta_for_val_fallback=1.0\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================\n# Shape-safe + threshold-calibrated LSTM pipeline\n#   ‚úÖ tz-naive datetime everywhere (merges won't zero-match)\n#   ‚úÖ sequence scaling uses actual (n, T, F) ‚Äî never global SEQ_LEN\n#   ‚úÖ SMOTE/SMOTEENN/SMOTETomek safe when minority is tiny\n#   ‚úÖ TF clear_session + gc in loops\n#   ‚úÖ VAL-calibrated threshold saved + reused for daily risk + downstream\n# ======================================================\n\nimport os\nimport gc\nimport json\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    roc_auc_score, average_precision_score,\n    accuracy_score, balanced_accuracy_score,\n    precision_score, recall_score, f1_score,\n    confusion_matrix, precision_recall_curve, roc_curve\n)\n\n# ---------------------------\n# Optional: imbalanced-learn\n# ---------------------------\ntry:\n    from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n    from imblearn.under_sampling import RandomUnderSampler\n    from imblearn.combine import SMOTEENN, SMOTETomek\nexcept Exception:\n    RandomOverSampler = SMOTE = ADASYN = RandomUnderSampler = SMOTEENN = SMOTETomek = None\n\n\n# ---------------------------\n# TensorFlow / Keras\n# ---------------------------\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\n# ======================================================\n# Config (edit these once)\n# ======================================================\nRANDOM_STATE = 42\nUSE_STATIC_INPUT = True\n\n# Your sequence features (must exist in hourly df)\nDEFAULT_SEQ_FEATURE_COLS = [\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n    \"pc1_activity_energy\",\n    \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n]\n\n# Optional static features (patient-level; will be taken once per patient)\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\n# Risk outputs\nDEFAULT_RISK_THRESHOLD = 0.50\n\n# Artifacts\nDEFAULT_OUT_DIR = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \"./outputs\"\nDEFAULT_RESULTS_CSV = os.path.join(DEFAULT_OUT_DIR, \"results_summary_all.csv\")\nDEFAULT_THR_JSON    = os.path.join(DEFAULT_OUT_DIR, \"best_threshold.json\")\n\n\n# ======================================================\n# 1) Datetime utilities (tz-naive, merge-friendly)\n# ======================================================\ndef to_dt(x):\n    \"\"\"\n    Parse to datetime in UTC then drop tz info => tz-naive.\n    Prevents pandas merge chaos (tz-aware vs tz-naive).\n    \"\"\"\n    ts = pd.to_datetime(x, errors=\"coerce\", utc=True)\n    if isinstance(ts, pd.Series):\n        return ts.dt.tz_convert(None)\n    if ts is pd.NaT:\n        return ts\n    return ts.tz_convert(None) if getattr(ts, \"tzinfo\", None) else ts\n\n\ndef ensure_hour_date_columns(df: pd.DataFrame, time_col=\"hour\", start_col=None) -> pd.DataFrame:\n    \"\"\"\n    Ensures df has tz-naive:\n      - hour: floor to hour\n      - date: normalized midnight\n      - hour_of_day\n    If time_col missing, uses start_col to build it.\n    \"\"\"\n    df = df.copy()\n\n    if time_col in df.columns:\n        df[time_col] = to_dt(df[time_col])\n        df[\"hour\"] = pd.to_datetime(df[time_col], errors=\"coerce\")\n    elif start_col and start_col in df.columns:\n        df[start_col] = to_dt(df[start_col])\n        df[\"hour\"] = df[start_col].dt.floor(\"H\")\n    else:\n        raise KeyError(f\"Need '{time_col}' or start_col='{start_col}' in dataframe.\")\n\n    df[\"hour\"] = df[\"hour\"].dt.floor(\"H\")\n    df[\"date\"] = df[\"hour\"].dt.normalize()\n    df[\"hour_of_day\"] = df[\"hour\"].dt.hour\n    return df\n\n\n# ======================================================\n# 2) Sequence building (shape-safe scaling)\n# ======================================================\ndef splits_from_hourly(hourly: pd.DataFrame):\n    \"\"\"\n    Derive patient splits from hourly['Split'].\n    Expects Split values like train/val/test (case-insensitive).\n    \"\"\"\n    if \"Split\" not in hourly.columns:\n        raise KeyError(\"hourly must have a 'Split' column to build splits automatically.\")\n    s = hourly[\"Split\"].astype(str).str.lower()\n    train_p = np.array(sorted(hourly.loc[s.eq(\"train\"), \"patientID\"].unique()))\n    val_p   = np.array(sorted(hourly.loc[s.eq(\"val\"),   \"patientID\"].unique()))\n    test_p  = np.array(sorted(hourly.loc[s.eq(\"test\"),  \"patientID\"].unique()))\n    if len(train_p)==0 or len(val_p)==0 or len(test_p)==0:\n        raise RuntimeError(\"Split must contain non-empty train/val/test patient groups.\")\n    return (train_p, val_p, test_p)\n\n\ndef build_sequences_by_split(\n    hourly: pd.DataFrame,\n    splits,\n    seq_len: int,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    scale_features: bool = True\n):\n    \"\"\"\n    Build (X_seq, X_stat, y) for train/val/test.\n      - Scalers fitted on TRAIN only.\n      - Scaling reshape uses actual (n, T, F), never global SEQ_LEN.\n    \"\"\"\n    required = [\"patientID\", \"hour\", \"hypo_label\"]\n    for c in required:\n        if c not in hourly.columns:\n            raise KeyError(f\"hourly missing required column: {c}\")\n\n    hourly = ensure_hour_date_columns(hourly, time_col=\"hour\")\n\n    seq_feature_cols = list(seq_feature_cols)\n    missing = [c for c in seq_feature_cols if c not in hourly.columns]\n    if missing:\n        raise KeyError(f\"Missing sequence feature columns: {missing}\")\n\n    # Static matrix per patient\n    static_cols_present = [c for c in static_cols if c in hourly.columns]\n    static_mat = None\n    if USE_STATIC_INPUT and static_cols_present:\n        static_mat = (hourly[[\"patientID\"] + static_cols_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float)\n                      .fillna(0.0))\n\n    train_p, val_p, test_p = splits\n\n    def _build(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        sub = sub.sort_values([\"patientID\", \"hour\"]).reset_index(drop=True)\n\n        X_seq, y, X_stat = [], [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n\n            feats  = grp[seq_feature_cols].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len])\n                y.append(labels[i+seq_len])\n\n                if USE_STATIC_INPUT and static_mat is not None:\n                    if pid in static_mat.index:\n                        X_stat.append(static_mat.loc[pid].values.astype(float))\n                    else:\n                        X_stat.append(np.zeros(len(static_cols_present), dtype=float))\n\n        X_seq = np.asarray(X_seq, dtype=float)\n        y = np.asarray(y, dtype=int)\n\n        if USE_STATIC_INPUT and static_mat is not None and len(X_stat) > 0:\n            X_stat = np.asarray(X_stat, dtype=float)\n        else:\n            X_stat = None\n\n        return X_seq, X_stat, y\n\n    Xtr, Str, ytr = _build(train_p)\n    Xva, Sva, yva = _build(val_p)\n    Xte, Ste, yte = _build(test_p)\n\n    # -------- Scale on TRAIN only (shape-safe) --------\n    seq_scaler = stat_scaler = None\n\n    if scale_features and Xtr.size > 0:\n        F = Xtr.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr.reshape(-1, F))\n\n        def _scale_seq(X):\n            if X is None or X.size == 0:\n                return X\n            n, T, F_ = X.shape\n            return seq_scaler.transform(X.reshape(-1, F_)).reshape(n, T, F_)\n\n        Xtr = _scale_seq(Xtr)\n        Xva = _scale_seq(Xva)\n        Xte = _scale_seq(Xte)\n\n        # sanity check: if this fires, you're back in SEQ_LEN hell\n        assert Xtr.shape[1] == seq_len, (Xtr.shape, seq_len)\n\n    if scale_features and Str is not None and Str.size > 0:\n        stat_scaler = StandardScaler().fit(Str)\n\n        def _scale_stat(S):\n            if S is None or S.size == 0:\n                return S\n            return stat_scaler.transform(S)\n\n        Str = _scale_stat(Str)\n        Sva = _scale_stat(Sva)\n        Ste = _scale_stat(Ste)\n\n    print(f\"‚úÖ Built sequences | train={Xtr.shape}, val={Xva.shape}, test={Xte.shape}\")\n\n    return {\n        \"train\": {\"X_seq\": Xtr, \"X_stat\": Str, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva, \"X_stat\": Sva, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte, \"X_stat\": Ste, \"y\": yte},\n        \"seq_features_used\": seq_feature_cols,\n        \"static_features_used\": static_cols_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n\ndef save_sequences_npz(data: dict, path: str):\n    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n    np.savez(\n        path,\n        Xtr=data[\"train\"][\"X_seq\"], ytr=data[\"train\"][\"y\"], Xtr_stat=(data[\"train\"][\"X_stat\"] if data[\"train\"][\"X_stat\"] is not None else np.array([])),\n        Xva=data[\"val\"][\"X_seq\"],   yva=data[\"val\"][\"y\"],   Xva_stat=(data[\"val\"][\"X_stat\"] if data[\"val\"][\"X_stat\"] is not None else np.array([])),\n        Xte=data[\"test\"][\"X_seq\"],  yte=data[\"test\"][\"y\"],  Xte_stat=(data[\"test\"][\"X_stat\"] if data[\"test\"][\"X_stat\"] is not None else np.array([])),\n        seq_features_used=np.array(data[\"seq_features_used\"], dtype=object),\n        static_features_used=np.array(data[\"static_features_used\"], dtype=object),\n        allow_pickle=True\n    )\n    print(f\"üì¶ Saved sequences ‚Üí {path}\")\n\n\n# ======================================================\n# 3) Resampling (SMOTEENN/SMOTETomek won't explode)\n# ======================================================\ndef seq_resample(X_seq, y, X_stat=None, method=\"none\", random_state=RANDOM_STATE):\n    \"\"\"\n    Resample on flattened window features (and static, if provided), then reshape back.\n    Supports:\n      none, ros, rus, smote, adasyn, smoteenn, smotetomek\n    \"\"\"\n    method = str(method).lower().strip()\n    X_seq = np.asarray(X_seq, dtype=float)\n    y = np.asarray(y, dtype=int).ravel()\n\n    if method == \"none\":\n        return X_seq, y, (None if X_stat is None else np.asarray(X_stat, dtype=float))\n\n    if RandomOverSampler is None:\n        raise ImportError(\"imbalanced-learn not installed. Install: pip install imbalanced-learn\")\n\n    n, T, F = X_seq.shape\n    seq_dim = T * F\n\n    X_flat = X_seq.reshape(n, seq_dim)\n    stat_dim = 0\n    if USE_STATIC_INPUT and X_stat is not None and np.asarray(X_stat).size > 0:\n        X_stat = np.asarray(X_stat, dtype=float)\n        stat_dim = X_stat.shape[1]\n        X_flat = np.concatenate([X_flat, X_stat], axis=1)\n\n    # minority count (for safe k)\n    n0 = int((y == 0).sum())\n    n1 = int((y == 1).sum())\n    minority_n = min(n0, n1)\n\n    # If minority too small for SMOTE-style synth, fall back to ROS.\n    def _safe_k():\n        return max(1, min(5, minority_n - 1))\n\n    if method in {\"smote\", \"adasyn\", \"smoteenn\", \"smotetomek\"} and minority_n < 2:\n        method = \"ros\"\n\n    if method == \"ros\":\n        sampler = RandomOverSampler(random_state=random_state)\n    elif method == \"rus\":\n        sampler = RandomUnderSampler(random_state=random_state)\n    elif method == \"smote\":\n        sampler = SMOTE(random_state=random_state, k_neighbors=_safe_k())\n    elif method == \"adasyn\":\n        # ADASYN uses n_neighbors internally\n        sampler = ADASYN(random_state=random_state, n_neighbors=_safe_k())\n    elif method == \"smoteenn\":\n        sm = SMOTE(random_state=random_state, k_neighbors=_safe_k())\n        sampler = SMOTEENN(random_state=random_state, smote=sm)\n    elif method == \"smotetomek\":\n        sm = SMOTE(random_state=random_state, k_neighbors=_safe_k())\n        sampler = SMOTETomek(random_state=random_state, smote=sm)\n    else:\n        raise ValueError(f\"Unknown resample method: {method}\")\n\n    Xr, yr = sampler.fit_resample(X_flat, y)\n\n    Xr_seq = Xr[:, :seq_dim].reshape(-1, T, F)\n    Xr_stat = None\n    if stat_dim > 0:\n        Xr_stat = Xr[:, seq_dim:].reshape(-1, stat_dim)\n\n    return Xr_seq, yr.astype(int), Xr_stat\n\n\ndef make_balanced_subset(X_seq, y, X_stat=None, random_state=RANDOM_STATE):\n    \"\"\"Downsample majority to match minority for a balanced eval set.\"\"\"\n    y = np.asarray(y).astype(int).ravel()\n    idx0 = np.where(y == 0)[0]\n    idx1 = np.where(y == 1)[0]\n    if len(idx0) == 0 or len(idx1) == 0:\n        return X_seq, y, X_stat\n\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n\n    Xb = np.asarray(X_seq)[keep]\n    yb = y[keep]\n    Sb = None if X_stat is None else np.asarray(X_stat)[keep]\n    return Xb, yb, Sb\n\n\n# ======================================================\n# 4) Model\n# ======================================================\ndef make_lstm_model(seq_len: int, n_seq_f: int, n_stat_f: int = 0, lr: float = 1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n\n    x = Bidirectional(LSTM(64, return_sequences=True))(seq_in)\n    x = Dropout(0.25)(x)\n    x = LSTM(32)(x)\n    x = Dropout(0.25)(x)\n    x = Dense(32, activation=\"relu\")(x)\n\n    if USE_STATIC_INPUT and n_stat_f and n_stat_f > 0:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(16, activation=\"relu\")(stat_in)\n        s = Dropout(0.25)(s)\n        h = Concatenate()([x, s])\n        h = Dense(16, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model([seq_in, stat_in], out)\n    else:\n        h = Dense(16, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(seq_in, out)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n        loss=\"binary_crossentropy\",\n        metrics=[tf.keras.metrics.AUC(name=\"auc\"), tf.keras.metrics.AUC(curve=\"PR\", name=\"auc_pr\")]\n    )\n    return model\n\n\ndef predict_proba(model, X_seq, X_stat=None):\n    if USE_STATIC_INPUT and X_stat is not None and np.asarray(X_stat).size > 0:\n        return model.predict([X_seq, X_stat], verbose=0).ravel()\n    return model.predict(X_seq, verbose=0).ravel()\n\n\n# ======================================================\n# 5) Threshold calibration (VAL) + evaluation helpers\n# ======================================================\ndef best_threshold_fbeta(y_true, y_prob, beta=1.0, thr_min=0.05, thr_max=0.95, default=0.50):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_prob = np.asarray(y_prob).astype(float).ravel()\n    if len(np.unique(y_true)) < 2:\n        return float(default)\n\n    prec, rec, thr = precision_recall_curve(y_true, y_prob)\n    # thr has length len(prec)-1\n    prec, rec = prec[:-1], rec[:-1]\n    thr = np.asarray(thr)\n\n    mask = np.isfinite(thr) & (thr >= thr_min) & (thr <= thr_max)\n    if not mask.any():\n        return float(default)\n\n    b2 = beta ** 2\n    f = (1 + b2) * (prec[mask] * rec[mask]) / (b2 * prec[mask] + rec[mask] + 1e-12)\n    return float(thr[mask][np.nanargmax(f)])\n\n\ndef best_threshold_youden(y_true, y_prob, default=0.50):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_prob = np.asarray(y_prob).astype(float).ravel()\n    if len(np.unique(y_true)) < 2:\n        return float(default)\n\n    fpr, tpr, thr = roc_curve(y_true, y_prob)\n    j = tpr - fpr\n    return float(thr[np.nanargmax(j)])\n\n\ndef eval_metrics(y_true, y_prob, thr):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_prob = np.asarray(y_prob).astype(float).ravel()\n    y_pred = (y_prob >= float(thr)).astype(int)\n\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n\n    def _safe_auc(fn_auc, default=np.nan):\n        try:\n            return float(fn_auc(y_true, y_prob))\n        except Exception:\n            return float(default)\n\n    return {\n        \"threshold\": float(thr),\n        \"acc\": float(accuracy_score(y_true, y_pred)),\n        \"bal_acc\": float(balanced_accuracy_score(y_true, y_pred)),\n        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n        \"roc_auc\": _safe_auc(roc_auc_score),\n        \"pr_auc\": _safe_auc(average_precision_score),\n        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n        \"pos_rate_true\": float(np.mean(y_true)) if len(y_true) else np.nan,\n        \"pos_rate_pred\": float(np.mean(y_pred)) if len(y_pred) else np.nan,\n    }\n\n\ndef save_threshold_json(path, threshold, meta=None):\n    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n    payload = {\"threshold\": float(threshold), \"meta\": (meta or {})}\n    with open(path, \"w\") as f:\n        json.dump(payload, f, indent=2)\n    print(f\"üéöÔ∏è Saved threshold ‚Üí {path} (thr={float(threshold):.4f})\")\n\n\ndef load_threshold_json(path):\n    if not path or not os.path.exists(path):\n        return None\n    try:\n        with open(path, \"r\") as f:\n            return float(json.load(f).get(\"threshold\"))\n    except Exception:\n        return None\n\n\ndef get_best_val_threshold_from_results(results_csv, method=None, model_name=None, score_col=\"f1\"):\n    \"\"\"\n    Read a results CSV and pick best VAL row. If method/model specified, filter to them.\n    Expects columns: split, threshold, and score_col (default f1).\n    \"\"\"\n    if not results_csv or not os.path.exists(results_csv):\n        return None\n    df = pd.read_csv(results_csv)\n    if \"split\" not in df.columns or \"threshold\" not in df.columns:\n        return None\n\n    d = df.copy()\n    d[\"split\"] = d[\"split\"].astype(str).str.lower()\n    d = d[d[\"split\"].eq(\"val\")]\n\n    if method is not None and \"method\" in d.columns:\n        d = d[d[\"method\"].astype(str) == str(method)]\n    if model_name is not None and \"model\" in d.columns:\n        d = d[d[\"model\"].astype(str) == str(model_name)]\n\n    if d.empty:\n        return None\n\n    if score_col in d.columns:\n        d = d.sort_values(score_col, ascending=False)\n\n    try:\n        thr = float(d.iloc[0][\"threshold\"])\n        return thr if np.isfinite(thr) else None\n    except Exception:\n        return None\n\n\ndef resolve_threshold(\n    threshold=None,\n    thr_json=DEFAULT_THR_JSON,\n    results_csv=DEFAULT_RESULTS_CSV,\n    model=None,\n    data=None,\n    beta_fallback=1.0,\n    default=DEFAULT_RISK_THRESHOLD,\n    method=None,\n    model_name=None\n):\n    \"\"\"\n    One rule to rule them all:\n      1) explicit threshold param\n      2) threshold JSON\n      3) best threshold from results CSV\n      4) calibrate from VAL (fallback)\n      5) default\n    \"\"\"\n    if threshold is not None:\n        return float(threshold), \"manual\"\n\n    thr = load_threshold_json(thr_json)\n    if thr is not None:\n        return float(thr), \"threshold.json\"\n\n    thr = get_best_val_threshold_from_results(results_csv, method=method, model_name=model_name, score_col=\"f1\")\n    if thr is not None:\n        return float(thr), \"results.csv\"\n\n    if model is not None and data is not None:\n        yv = data[\"val\"][\"y\"]\n        pv = predict_proba(model, data[\"val\"][\"X_seq\"], data[\"val\"][\"X_stat\"])\n        thr = best_threshold_fbeta(yv, pv, beta=beta_fallback, default=default)\n        return float(thr), \"val_calibration\"\n\n    return float(default), \"default\"\n\n\n# ======================================================\n# 6) Sequence index mapping + Daily risk aggregation (uses calibrated thr)\n# ======================================================\ndef build_sequence_index_map(hourly_df: pd.DataFrame, split: str, seq_len: int) -> pd.DataFrame:\n    \"\"\"\n    Recreate sequence ordering from build_sequences_by_split:\n      sequence i maps to the (i+seq_len)-th hour row for that patient.\n    \"\"\"\n    h = ensure_hour_date_columns(hourly_df, time_col=\"hour\")\n    if \"Split\" not in h.columns:\n        raise KeyError(\"hourly_df must contain Split for mapping.\")\n    s = h[\"Split\"].astype(str).str.lower()\n    sub = (h[s.eq(str(split).lower())]\n           .sort_values([\"patientID\", \"hour\"])\n           .reset_index()\n           .rename(columns={\"index\": \"row_idx\"}))\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i + seq_len]\n            rows.append({\n                \"seq_idx\": len(rows),\n                \"patientID\": pid,\n                \"hour\": pd.to_datetime(tgt[\"hour\"]),\n                \"date\": pd.to_datetime(tgt[\"date\"]).normalize() if \"date\" in tgt else pd.to_datetime(tgt[\"hour\"]).normalize(),\n                \"visit_assigned\": tgt.get(\"visit_assigned\", np.nan),\n                \"period_main\": tgt.get(\"period_main\", np.nan),\n                \"row_idx\": int(tgt[\"row_idx\"]),\n            })\n    return pd.DataFrame(rows)\n\n\ndef aggregate_hourly_to_daily_risk(\n    model,\n    data,\n    hourly: pd.DataFrame,\n    split: str = \"test\",\n    threshold: float = DEFAULT_RISK_THRESHOLD,\n    out_dir: str = DEFAULT_OUT_DIR,\n    seq_len: int = None\n):\n    os.makedirs(out_dir, exist_ok=True)\n\n    X = data[split][\"X_seq\"]\n    S = data[split][\"X_stat\"]\n    y_true = np.asarray(data[split][\"y\"]).astype(int).ravel()\n\n    if seq_len is None:\n        if X is None or X.ndim != 3:\n            raise RuntimeError(\"Cannot infer seq_len; data[split]['X_seq'] is missing or not 3D.\")\n        seq_len = X.shape[1]\n\n    thr = float(threshold)\n\n    y_prob = predict_proba(model, X, S)\n    y_pred = (y_prob >= thr).astype(int)\n\n    seq_map = build_sequence_index_map(hourly, split=split, seq_len=seq_len)\n    if len(seq_map) != len(y_true):\n        raise RuntimeError(\n            f\"Mapping length {len(seq_map)} != y length {len(y_true)}. \"\n            f\"Use the SAME hourly table + seq_len used to build sequences.\"\n        )\n\n    pred_df = pd.DataFrame({\n        \"patientID\": seq_map[\"patientID\"].values,\n        \"hour\": pd.to_datetime(seq_map[\"hour\"].values),\n        \"date\": pd.to_datetime(seq_map[\"date\"].values).normalize(),\n        \"visit_assigned\": seq_map.get(\"visit_assigned\", pd.Series([np.nan]*len(seq_map))).values,\n        \"period_main\": seq_map.get(\"period_main\", pd.Series([np.nan]*len(seq_map))).values,\n        \"y_true\": y_true,\n        \"y_prob\": y_prob,\n        \"y_pred\": y_pred,\n        \"threshold_used\": thr,\n    })\n\n    grp = pred_df.groupby([\"patientID\", \"date\"], as_index=False)\n    daily = grp.agg(\n        n_hours=(\"y_true\", \"size\"),\n        true_events=(\"y_true\", \"sum\"),\n        pred_events=(\"y_pred\", \"sum\"),\n        risk_mean=(\"y_prob\", \"mean\"),\n        risk_max=(\"y_prob\", \"max\"),\n        risk_p95=(\"y_prob\", lambda x: float(np.quantile(x, 0.95))),\n        hours_above_thr=(\"y_pred\", \"sum\"),\n    )\n    daily[\"prevalence\"] = daily[\"true_events\"] / daily[\"n_hours\"].replace(0, np.nan)\n    daily[\"high_risk_day_mean\"] = (daily[\"risk_mean\"] >= thr).astype(int)\n    daily[\"high_risk_day_max\"]  = (daily[\"risk_max\"] >= thr).astype(int)\n    daily[\"threshold_used\"] = thr\n\n    pred_path  = os.path.join(out_dir, f\"hourly_pred_{split}.csv\")\n    daily_path = os.path.join(out_dir, f\"daily_risk_{split}.csv\")\n    pred_df.to_csv(pred_path, index=False)\n    daily.to_csv(daily_path, index=False)\n\n    print(f\"‚úÖ Saved hourly preds ‚Üí {pred_path}\")\n    print(f\"‚úÖ Saved daily risk  ‚Üí {daily_path} (thr={thr:.3f})\")\n\n    # tiny plot (because humans like pictures)\n    try:\n        ex_pid = daily[\"patientID\"].iloc[0]\n        dsub = daily[daily[\"patientID\"] == ex_pid].sort_values(\"date\")\n        plt.figure(figsize=(9, 3))\n        plt.plot(dsub[\"date\"], dsub[\"risk_mean\"], label=\"risk_mean\")\n        plt.plot(dsub[\"date\"], dsub[\"risk_max\"], label=\"risk_max\")\n        plt.axhline(thr, linestyle=\"--\", label=f\"thr={thr:.2f}\")\n        plt.title(f\"Daily risk ‚Äî patient {ex_pid}\")\n        plt.xlabel(\"date\"); plt.ylabel(\"risk\")\n        plt.legend(); plt.tight_layout()\n        png = os.path.join(out_dir, f\"daily_risk_trend_patient_{ex_pid}.png\")\n        plt.savefig(png, dpi=180)\n        plt.close()\n        print(f\"üñºÔ∏è Saved plot ‚Üí {png}\")\n    except Exception as e:\n        print(f\"[WARN] Plot skipped: {e}\")\n\n    return pred_df, daily\n\n\n# ======================================================\n# 7) Training + evaluation (calibrate once, save once, use everywhere)\n# ======================================================\ndef train_eval_one(\n    data: dict,\n    method_name: str = \"none\",\n    model_name: str = \"bilstm\",\n    out_dir: str = DEFAULT_OUT_DIR,\n    epochs: int = 20,\n    batch_size: int = 64,\n    lr: float = 1e-3,\n    beta_for_thr: float = 1.0,\n    results_csv: str = DEFAULT_RESULTS_CSV,\n    save_threshold_json_path: str = DEFAULT_THR_JSON\n):\n    os.makedirs(out_dir, exist_ok=True)\n\n    # TF memory hygiene (this matters if you're looping)\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    Xtr, Str, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"], data[\"train\"][\"y\"]\n    Xva, Sva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"X_stat\"],   data[\"val\"][\"y\"]\n    Xte, Ste, yte = data[\"test\"][\"X_seq\"],  data[\"test\"][\"X_stat\"],  data[\"test\"][\"y\"]\n\n    seq_len = Xtr.shape[1]\n    n_seq_f = Xtr.shape[2]\n    n_stat_f = 0\n    if USE_STATIC_INPUT and Str is not None and np.asarray(Str).size > 0:\n        n_stat_f = Str.shape[1]\n\n    # Resample train only\n    Xtr_r, ytr_r, Str_r = seq_resample(Xtr, ytr, X_stat=Str, method=method_name, random_state=RANDOM_STATE)\n\n    # Build + fit\n    model = make_lstm_model(seq_len=seq_len, n_seq_f=n_seq_f, n_stat_f=n_stat_f, lr=lr)\n\n    ckpt_path = os.path.join(out_dir, f\"ckpt__{method_name}__{model_name}.keras\")\n    callbacks = [\n        EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True, verbose=1),\n        ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=0),\n    ]\n\n    if USE_STATIC_INPUT and n_stat_f > 0:\n        model.fit(\n            [Xtr_r, Str_r], ytr_r,\n            validation_data=([Xva, Sva], yva),\n            epochs=epochs, batch_size=batch_size,\n            callbacks=callbacks, verbose=1\n        )\n    else:\n        model.fit(\n            Xtr_r, ytr_r,\n            validation_data=(Xva, yva),\n            epochs=epochs, batch_size=batch_size,\n            callbacks=callbacks, verbose=1\n        )\n\n    # Load best checkpoint\n    model = tf.keras.models.load_model(ckpt_path)\n\n    # Calibrate threshold on VAL probs\n    pva = predict_proba(model, Xva, Sva)\n    thr_f = best_threshold_fbeta(yva, pva, beta=beta_for_thr, default=DEFAULT_RISK_THRESHOLD)\n    thr_y = best_threshold_youden(yva, pva, default=DEFAULT_RISK_THRESHOLD)\n\n    # Pick the one you want as \"the\" decision rule\n    thr_use = thr_f  # <--- default: F1/Fbeta from PR curve\n\n    save_threshold_json(\n        save_threshold_json_path,\n        thr_use,\n        meta={\n            \"method\": method_name,\n            \"model\": model_name,\n            \"threshold_fbeta\": float(thr_f),\n            \"threshold_youden\": float(thr_y),\n            \"beta\": float(beta_for_thr),\n            \"seq_len\": int(seq_len)\n        }\n    )\n\n    # Evaluate splits at thr_use\n    ptr = predict_proba(model, Xtr, Str)\n    pte = predict_proba(model, Xte, Ste)\n\n    # Balanced test eval too\n    Xb, yb, Sb = make_balanced_subset(Xte, yte, Ste, random_state=RANDOM_STATE)\n    pb = predict_proba(model, Xb, Sb)\n\n    rows = []\n    for split, yt, pp in [\n        (\"train\", ytr, ptr),\n        (\"val\",   yva, pva),\n        (\"test\",  yte, pte),\n        (\"test_balanced\", yb, pb),\n    ]:\n        m = eval_metrics(yt, pp, thr_use)\n        rows.append({\n            \"method\": method_name,\n            \"model\": model_name,\n            \"split\": split,\n            **m\n        })\n\n    res = pd.DataFrame(rows)\n\n    # Append to results CSV (stable + boring, like it should be)\n    os.makedirs(os.path.dirname(results_csv) or \".\", exist_ok=True)\n    if os.path.exists(results_csv):\n        old = pd.read_csv(results_csv)\n        res_all = pd.concat([old, res], ignore_index=True)\n    else:\n        res_all = res\n    res_all.to_csv(results_csv, index=False)\n\n    print(f\"üìÑ Results appended ‚Üí {results_csv}\")\n    print(res.to_string(index=False))\n\n    return model, {\n        \"ckpt_path\": ckpt_path,\n        \"threshold\": float(thr_use),\n        \"threshold_fbeta\": float(thr_f),\n        \"threshold_youden\": float(thr_y),\n        \"results\": res\n    }\n\n\n# ======================================================\n# 8) Runner (end-to-end: build sequences -> train -> daily risk with calibrated threshold)\n# ======================================================\ndef run_end_to_end(\n    hourly: pd.DataFrame,\n    seq_len: int = 24,\n    seq_feature_cols=DEFAULT_SEQ_FEATURE_COLS,\n    static_cols=STATIC_COLS,\n    resample_method: str = \"none\",\n    out_dir: str = DEFAULT_OUT_DIR,\n    epochs: int = 20,\n    beta_for_thr: float = 1.0,\n    threshold_override=None\n):\n    os.makedirs(out_dir, exist_ok=True)\n\n    # Ensure datetime consistency up front\n    hourly = ensure_hour_date_columns(hourly, time_col=\"hour\")\n\n    # Build splits and sequences\n    splits = splits_from_hourly(hourly)\n    data = build_sequences_by_split(\n        hourly, splits, seq_len=seq_len,\n        seq_feature_cols=seq_feature_cols,\n        static_cols=static_cols,\n        scale_features=True\n    )\n\n    # Save sequences for reuse (optional but nice)\n    save_sequences_npz(data, os.path.join(out_dir, \"sequences_leakfree.npz\"))\n\n    # Train + calibrate\n    model, info = train_eval_one(\n        data,\n        method_name=resample_method,\n        model_name=\"bilstm\",\n        out_dir=out_dir,\n        epochs=epochs,\n        beta_for_thr=beta_for_thr,\n        results_csv=os.path.join(out_dir, \"results_summary_all.csv\"),\n        save_threshold_json_path=os.path.join(out_dir, \"best_threshold.json\")\n    )\n\n    # Resolve threshold (manual > json > results > val-cal > default)\n    thr, src = resolve_threshold(\n        threshold=threshold_override,\n        thr_json=os.path.join(out_dir, \"best_threshold.json\"),\n        results_csv=os.path.join(out_dir, \"results_summary_all.csv\"),\n        model=model,\n        data=data,\n        beta_fallback=beta_for_thr,\n        default=DEFAULT_RISK_THRESHOLD,\n        method=resample_method,\n        model_name=\"bilstm\"\n    )\n    print(f\"üéõÔ∏è Using threshold {thr:.3f} [{src}]\")\n\n    # Daily risk with the SAME threshold\n    _ = aggregate_hourly_to_daily_risk(\n        model, data, hourly,\n        split=\"test\",\n        threshold=thr,\n        out_dir=out_dir,\n        seq_len=seq_len\n    )\n\n    return model, data\n\n\n# ======================================================\n# Example usage\n# ======================================================\n# hourly = pd.read_csv(\"/kaggle/working/dynamic_hourly_features_ramadan.csv\")\n# model, data = run_end_to_end(\n#     hourly,\n#     seq_len=24,\n#     resample_method=\"smoteenn\",   # none | ros | rus | smote | adasyn | smoteenn | smotetomek\n#     epochs=12,\n#     beta_for_thr=1.0,\n#     out_dir=\"/kaggle/working\"\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Leak-safe Ramadan CGM ‚Üí BiLSTM ‚Üí Calibrated Threshold ‚Üí Daily Risk\n# with full calibration metrics (ROC, PR, Brier, ECE)\n# ============================================================\n\nimport os\nimport random\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    precision_recall_curve,\n    roc_auc_score,\n    average_precision_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n    roc_curve,\n    brier_score_loss,\n)\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\n# ============================================================\n# CONFIG\n# ============================================================\n\n# Kaggle paths ‚Äì change if your files are elsewhere\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nSTATIC_CSV      = \"/kaggle/input/hmc-model-static-variables/outcome_static.csv\"\nVISIT_WIDE_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_wide_by_variable.csv\"\nVISIT_LONG_CSV  = \"/kaggle/input/hmc-model-static-variables/outcome_visit_long.csv\"\n\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree_calibrated.npz\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_calibrated.csv\"\nOUT_DAILY_RISK  = \"/kaggle/working/daily_risk_test.csv\"\n\n# Ramadan window\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\n\nHYPO_CUTOFF   = 70.0   # mg/dL\nMIN_CGM_PER_H = 4      # minimum CGM readings to keep an hour\n\nSEQ_LEN       = 36     # sliding window length (hours)\nRANDOM_STATE  = 42     # üîë GLOBAL SEED\nUSE_STATIC    = True   # include static patient features in the model\n\n# Lifestyle candidates in intraday table\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\n# Visit-level features you want (per day)\nVISIT_COLS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\n\n# Static features (per patient)\nSTATIC_COLS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\n    \"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\n# Sequence features used by the LSTM\nSEQ_FEATURE_COLS = [\n    \"cgm_mean\",\"cgm_std\",\"pca_cgm1\",\n    \"pc1_activity_energy\",\n    \"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"\n]\n\n\n# ============================================================\n# REPRO\n# ============================================================\n\ndef set_seeds(seed=RANDOM_STATE):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nset_seeds()\n\n\n# ============================================================\n# SMALL UTILS\n# ============================================================\n\ndef to_dt(x):\n    return pd.to_datetime(x, errors=\"coerce\", utc=True).dt.tz_convert(None)\n\ndef _norm_col(s: str) -> str:\n    return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\ndef _pick_col_flex(df, preferred=None, required=False, name=\"\", any_contains=None):\n    cols = list(df.columns)\n    norm_map = {c: _norm_col(c) for c in cols}\n\n    # direct match\n    if preferred:\n        lower_map = {str(p).lower(): p for p in preferred}\n        for c in cols:\n            if str(c).lower() in lower_map:\n                return c\n\n    # fuzzy ‚Äúcontains‚Äù\n    if any_contains:\n        cands = []\n        for c, n in norm_map.items():\n            if any(_norm_col(tok) in n for tok in any_contains):\n                cands.append(c)\n        if cands:\n            return cands[0]\n\n    if required:\n        raise KeyError(f\"Could not find column for {name}. Have: {cols}\")\n    return None\n\ndef pick_patient_col(df):\n    return _pick_col_flex(\n        df,\n        preferred=[\"patientID\",\"PatientID (Huawei Data)\",\"subject_id\",\"id\",\"huaweiID\"],\n        required=True,\n        name=\"patientID\",\n        any_contains=[\"patient\",\"subject\",\"id\",\"huawei\"]\n    )\n\ndef pick_date_col(df):\n    return _pick_col_flex(\n        df,\n        preferred=[\"date\",\"visit_date\",\"Date\",\"day\",\"timestamp\",\"start\"],\n        required=True,\n        name=\"date\",\n        any_contains=[\"date\",\"visit\",\"day\",\"timestamp\",\"start\"]\n    )\n\ndef safe_encode_gender(s):\n    if s.dtype == \"object\":\n        return s.str.strip().str.lower().map({\"male\":1,\"m\":1,\"female\":0,\"f\":0})\n    return pd.to_numeric(s, errors=\"coerce\")\n\ndef ensure_numeric(df, ignore=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ignore = set(ignore)\n    for c in df.columns:\n        if c not in ignore:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\n\n# ============================================================\n# STATIC + VISIT LOADERS\n# ============================================================\n\ndef load_static(static_csv=STATIC_CSV):\n    if not static_csv or not os.path.exists(static_csv):\n        print(\"‚ö†Ô∏è No static CSV; static features will be zero.\")\n        return None\n    df = pd.read_csv(static_csv)\n    pid_col = pick_patient_col(df)\n    df = df.rename(columns={pid_col: \"patientID\"})\n    keep = [\"patientID\"] + [c for c in STATIC_COLS if c in df.columns]\n    df = df[keep].drop_duplicates(subset=[\"patientID\"]).copy()\n    if \"Gender\" in df.columns:\n        df[\"Gender\"] = safe_encode_gender(df[\"Gender\"])\n    for c in keep:\n        if c != \"patientID\":\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef load_visit(visit_wide_csv=VISIT_WIDE_CSV, visit_long_csv=VISIT_LONG_CSV):\n    # wide preferred\n    if visit_wide_csv and os.path.exists(visit_wide_csv):\n        wide = pd.read_csv(visit_wide_csv)\n        pid_col  = pick_patient_col(wide)\n        date_col = pick_date_col(wide)\n        wide = wide.rename(columns={pid_col:\"patientID\", date_col:\"date\"})\n        wide[\"date\"] = pd.to_datetime(wide[\"date\"], errors=\"coerce\").dt.normalize()\n        keep = [\"patientID\",\"date\"] + [c for c in VISIT_COLS if c in wide.columns]\n        if len(keep) > 2:\n            return wide[keep].copy()\n        else:\n            print(\"‚ö†Ô∏è visit_wide found but no desired visit columns; trying LONG‚Ä¶\")\n\n    # long ‚Üí pivot\n    if visit_long_csv and os.path.exists(visit_long_csv):\n        long = pd.read_csv(visit_long_csv)\n        pid_col  = pick_patient_col(long)\n        date_col = pick_date_col(long)\n        var_col  = _pick_col_flex(long, preferred=[\"variable\",\"name\",\"measure\"], required=True, name=\"variable\")\n        val_col  = _pick_col_flex(long, preferred=[\"value\",\"reading\",\"amount\"], required=True, name=\"value\")\n\n        long = long.rename(columns={pid_col:\"patientID\", date_col:\"date\", var_col:\"variable\", val_col:\"value\"})\n        long[\"date\"] = pd.to_datetime(long[\"date\"], errors=\"coerce\").dt.normalize()\n\n        wide = (long\n                .pivot_table(index=[\"patientID\",\"date\"], columns=\"variable\", values=\"value\", aggfunc=\"mean\")\n                .reset_index())\n        keep = [\"patientID\",\"date\"] + [c for c in VISIT_COLS if c in wide.columns]\n        if len(keep) > 2:\n            return wide[keep].copy()\n        print(\"‚ö†Ô∏è visit_long has no desired visit variables after pivot.\")\n\n    print(\"‚ö†Ô∏è No usable visit CSV; visit features will be zero.\")\n    return None\n\n\n# ============================================================\n# HOURLY FEATURES (Ramadan, leak-safe PCA)\n# ============================================================\n\ndef build_hourly_features():\n    if not os.path.exists(CSV_INTRADAY_WITH_VISITS):\n        raise FileNotFoundError(CSV_INTRADAY_WITH_VISITS)\n\n    df = pd.read_csv(CSV_INTRADAY_WITH_VISITS)\n\n    # patient\n    if \"patientID\" not in df.columns:\n        pid_col = pick_patient_col(df)\n        df = df.rename(columns={pid_col: \"patientID\"})\n\n    # time\n    if \"start\" in df.columns:\n        df[\"start\"] = to_dt(df[\"start\"])\n        df[\"date\"]  = df[\"start\"].dt.normalize()\n        df[\"hour\"]  = df[\"start\"].dt.floor(\"H\")\n    else:\n        date_col = pick_date_col(df)\n        df[date_col] = to_dt(df[date_col])\n        df[\"date\"]   = df[date_col].dt.normalize()\n        df[\"hour\"]   = df[date_col].dt.floor(\"H\")\n\n    df[\"hour_of_day\"] = df[\"hour\"].dt.hour\n    df = ensure_numeric(df)\n\n    # Ramadan filter\n    df = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"Dataset must contain 'cgm' column.\")\n\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # keep hours with ‚â• MIN_CGM_PER_H\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= MIN_CGM_PER_H)\n    )\n\n    # basic CGM stats\n    hourly = (\n        valid_hours.groupby([\"patientID\",\"hour\"], as_index=False)\n                   .agg(\n                       cgm_min=(\"cgm\",\"min\"),\n                       cgm_max=(\"cgm\",\"max\"),\n                       cgm_mean=(\"cgm\",\"mean\"),\n                       cgm_std=(\"cgm\",\"std\"),\n                   )\n                   .sort_values([\"patientID\",\"hour\"])\n                   .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    # hypo label (any CBG < 70 in that hour)\n    hyp = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n                   .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n                   .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(hyp, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # lifestyle hourly means (if columns exist)\n    life_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if life_cols:\n        life_h = (df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[life_cols].mean())\n        hourly = hourly.merge(life_h, on=[\"patientID\",\"hour\"], how=\"left\")\n    else:\n        for c in LIFESTYLE_COLS_CANDIDATES:\n            if c not in hourly.columns:\n                hourly[c] = 0.0\n\n    # composites\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # split patients (train/val/test) ‚Äì leak-safe\n    pids = hourly[\"patientID\"].unique()\n    train_p, test_p = train_test_split(pids, test_size=0.2, random_state=RANDOM_STATE)\n    train_p, val_p  = train_test_split(train_p, test_size=0.1/(0.8), random_state=RANDOM_STATE)\n\n    def _split(pid):\n        if pid in train_p: return \"train\"\n        if pid in val_p:   return \"val\"\n        return \"test\"\n\n    hourly[\"Split\"] = hourly[\"patientID\"].map(_split)\n    hourly[\"date\"]  = hourly[\"hour\"].dt.normalize()\n\n    # PCA on CGM (fit on train only)\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    tr_mask  = (hourly[\"Split\"] == \"train\")\n    scal_cgm = StandardScaler().fit(hourly.loc[tr_mask, cgm_cols])\n    pca_cgm  = PCA(n_components=3, random_state=RANDOM_STATE).fit(\n        scal_cgm.transform(hourly.loc[tr_mask, cgm_cols])\n    )\n    Xc = scal_cgm.transform(hourly[cgm_cols])\n    Zc = pca_cgm.transform(Xc)\n    hourly[\"pca_cgm1\"] = Zc[:,0]\n    hourly[\"pca_cgm2\"] = Zc[:,1]\n    hourly[\"pca_cgm3\"] = Zc[:,2]\n    print(\"CGM PCA explained variance:\", np.round(pca_cgm.explained_variance_ratio_, 3))\n\n    # PCA on lifestyle (fit on train only)\n    life_used = [c for c in [\"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"]\n                 if c in hourly.columns]\n    if life_used:\n        scal_life = StandardScaler().fit(hourly.loc[tr_mask, life_used])\n        ZL = PCA(n_components=3, random_state=RANDOM_STATE).fit_transform(\n            scal_life.transform(hourly[life_used].fillna(0.0))\n        )\n        hourly[\"pc1_activity_energy\"] = ZL[:,0]\n        hourly[\"pc2_physiology\"]      = ZL[:,1]\n        hourly[\"pc3_sleep_rest\"]      = ZL[:,2]\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    # visit features\n    visit_df = load_visit()\n    if visit_df is not None:\n        visit_df[\"date\"] = pd.to_datetime(visit_df[\"date\"], errors=\"coerce\").dt.normalize()\n        visit_df = visit_df[(visit_df[\"date\"] >= RAMADAN_START) & (visit_df[\"date\"] <= RAMADAN_END)]\n        hourly = hourly.merge(visit_df, on=[\"patientID\",\"date\"], how=\"left\")\n    for c in VISIT_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    # static features\n    static_df = load_static()\n    if static_df is not None:\n        hourly = hourly.merge(static_df, on=\"patientID\", how=\"left\")\n    for c in STATIC_COLS:\n        if c not in hourly.columns:\n            hourly[c] = 0.0\n        hourly[c] = pd.to_numeric(hourly[c], errors=\"coerce\").fillna(0.0)\n\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly.to_csv(OUT_HOURLY_CSV, index=False)\n    print(f\"‚úÖ Saved hourly features ‚Üí {OUT_HOURLY_CSV}\")\n    return hourly\n\n\n# ============================================================\n# SEQUENCES + SCALING\n# ============================================================\n\ndef build_sequences(hourly, seq_len=SEQ_LEN):\n    for col in [\"patientID\",\"hour\",\"hypo_label\",\"Split\"]:\n        if col not in hourly.columns:\n            raise KeyError(f\"hourly missing '{col}'\")\n\n    hourly = hourly.copy()\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n    hourly[\"date\"] = hourly[\"hour\"].dt.normalize()\n\n    missing = [c for c in SEQ_FEATURE_COLS if c not in hourly.columns]\n    if missing:\n        raise KeyError(f\"Missing seq features: {missing}\")\n\n    def _pids(split):\n        s = hourly[\"Split\"].astype(str).str.lower()\n        return np.array(sorted(hourly.loc[s==split, \"patientID\"].unique()))\n\n    train_p = _pids(\"train\")\n    val_p   = _pids(\"val\")\n    test_p  = _pids(\"test\")\n\n    static_present = [c for c in STATIC_COLS if c in hourly.columns]\n    if USE_STATIC and static_present:\n        static_mat = (hourly[[\"patientID\"]+static_present]\n                      .drop_duplicates(subset=[\"patientID\"])\n                      .set_index(\"patientID\")\n                      .astype(float)\n                      .fillna(0.0))\n    else:\n        static_mat = None\n\n    def _build_for(pid_set):\n        sub = hourly[hourly[\"patientID\"].isin(pid_set)].copy()\n        sub = sub.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n\n        X_seq, y, X_stat = [], [], []\n        for pid, grp in sub.groupby(\"patientID\", sort=True):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            if len(grp) <= seq_len:\n                continue\n\n            feats  = grp[SEQ_FEATURE_COLS].astype(float).values\n            labels = grp[\"hypo_label\"].astype(int).values\n\n            for i in range(len(grp) - seq_len):\n                X_seq.append(feats[i:i+seq_len])\n                y.append(labels[i+seq_len])\n                if USE_STATIC and static_mat is not None:\n                    if pid in static_mat.index:\n                        X_stat.append(static_mat.loc[pid].values.astype(float))\n                    else:\n                        X_stat.append(np.zeros(len(static_present), dtype=float))\n\n        X_seq = np.asarray(X_seq, dtype=float)\n        y     = np.asarray(y, dtype=int)\n        X_stat = np.asarray(X_stat, dtype=float) if (USE_STATIC and static_mat is not None and len(X_stat)>0) else None\n        return X_seq, X_stat, y\n\n    Xtr, Str, ytr = _build_for(train_p)\n    Xva, Sva, yva = _build_for(val_p)\n    Xte, Ste, yte = _build_for(test_p)\n\n    # scale seq features on train only\n    seq_scaler = None\n    if Xtr.size > 0:\n        F = Xtr.shape[2]\n        seq_scaler = StandardScaler().fit(Xtr.reshape(-1, F))\n\n        def _scale_seq(X):\n            if X is None or X.size == 0:\n                return X\n            n, T, F_ = X.shape\n            return seq_scaler.transform(X.reshape(-1, F_)).reshape(n, T, F_)\n\n        Xtr = _scale_seq(Xtr)\n        Xva = _scale_seq(Xva)\n        Xte = _scale_seq(Xte)\n\n        # sanity check\n        assert Xtr.shape[1] == seq_len, (Xtr.shape, seq_len)\n\n    # scale static\n    stat_scaler = None\n    if Str is not None and Str.size > 0:\n        stat_scaler = StandardScaler().fit(Str)\n\n        def _scale_stat(S):\n            if S is None or S.size == 0:\n                return S\n            return stat_scaler.transform(S)\n\n        Str = _scale_stat(Str)\n        Sva = _scale_stat(Sva)\n        Ste = _scale_stat(Ste)\n\n    data = {\n        \"train\": {\"X_seq\": Xtr, \"X_stat\": Str, \"y\": ytr},\n        \"val\":   {\"X_seq\": Xva, \"X_stat\": Sva, \"y\": yva},\n        \"test\":  {\"X_seq\": Xte, \"X_stat\": Ste, \"y\": yte},\n        \"seq_features_used\": list(SEQ_FEATURE_COLS),\n        \"static_features_used\": static_present,\n        \"scalers\": {\"seq\": seq_scaler, \"stat\": stat_scaler}\n    }\n\n    np.savez_compressed(\n        OUT_SEQ_NPZ,\n        Xtr=Xtr, Xtr_stat=(Str if Str is not None else np.empty((0,0))),\n        ytr=ytr,\n        Xva=Xva, Xva_stat=(Sva if Sva is not None else np.empty((0,0))),\n        yva=yva,\n        Xte=Xte, Xte_stat=(Ste if Ste is not None else np.empty((0,0))),\n        yte=yte,\n        seq_features_used=np.array(SEQ_FEATURE_COLS, dtype=object),\n        static_features_used=np.array(static_present, dtype=object)\n    )\n    print(f\"üì¶ Saved sequences ‚Üí {OUT_SEQ_NPZ}\")\n    print(\"Train/val/test shapes:\", Xtr.shape, Xva.shape, Xte.shape)\n    print(\"Train label dist:\", Counter(ytr))\n\n    return data\n\n\n# ============================================================\n# MODEL + TRAIN + CALIBRATION\n# ============================================================\n\ndef make_model(seq_len, n_seq_f, n_stat_f=0, lr=1e-3):\n    seq_in = Input(shape=(seq_len, n_seq_f), name=\"seq_in\")\n    x = Bidirectional(LSTM(64, return_sequences=True))(seq_in)\n    x = Dropout(0.2)(x)\n    x = LSTM(32)(x)\n    x = Dropout(0.2)(x)\n    x = Dense(32, activation=\"relu\")(x)\n\n    if USE_STATIC and n_stat_f > 0:\n        stat_in = Input(shape=(n_stat_f,), name=\"stat_in\")\n        s = Dense(16, activation=\"relu\")(stat_in)\n        s = Dropout(0.2)(s)\n        h = Concatenate()([x, s])\n        h = Dense(16, activation=\"relu\")(h)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=[seq_in, stat_in], outputs=out)\n    else:\n        h = Dense(16, activation=\"relu\")(x)\n        out = Dense(1, activation=\"sigmoid\")(h)\n        model = Model(inputs=seq_in, outputs=out)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    return model\n\ndef predict_proba(model, X, S=None):\n    if USE_STATIC and S is not None and S.size > 0:\n        return model.predict([X, S], verbose=0).ravel()\n    return model.predict(X, verbose=0).ravel()\n\ndef calibrate_threshold_on_val(y_val, p_val, beta=1.0, thr_min=0.05, thr_max=0.95, default=0.5):\n    y_val = np.asarray(y_val).astype(int).ravel()\n    p_val = np.asarray(p_val).astype(float).ravel()\n    if len(np.unique(y_val)) < 2:\n        return float(default)\n\n    prec, rec, thr = precision_recall_curve(y_val, p_val)\n    prec, rec = prec[:-1], rec[:-1]\n    thr = np.asarray(thr)\n\n    mask = np.isfinite(thr) & (thr >= thr_min) & (thr <= thr_max)\n    if not mask.any():\n        return float(default)\n\n    beta2 = beta**2\n    fbeta = (1 + beta2) * (prec[mask]*rec[mask]) / (beta2*prec[mask] + rec[mask] + 1e-12)\n    best_thr = float(thr[mask][np.nanargmax(fbeta)])\n    return best_thr\n\n# ---------- Full calibration evaluation (ROC, PR, Brier, ECE + plots) ----------\n\ndef evaluate_calibration(y_true, p_pred, n_bins=10, title_prefix=\"Test\"):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    p_pred = np.asarray(p_pred).astype(float).ravel()\n\n    # ROC & PR\n    try:\n        roc_auc = roc_auc_score(y_true, p_pred)\n    except Exception:\n        roc_auc = np.nan\n    try:\n        pr_auc  = average_precision_score(y_true, p_pred)\n    except Exception:\n        pr_auc = np.nan\n\n    # Brier score\n    try:\n        brier = brier_score_loss(y_true, p_pred)\n    except Exception:\n        brier = np.nan\n\n    # Binning for calibration curve and ECE\n    bins = np.linspace(0.0, 1.0, n_bins + 1)\n    bin_ids = np.digitize(p_pred, bins) - 1\n    bin_ids = np.clip(bin_ids, 0, n_bins - 1)\n\n    prob_true = []\n    prob_pred_bin = []\n    bin_counts = []\n\n    for b in range(n_bins):\n        mask = bin_ids == b\n        if not np.any(mask):\n            continue\n        bin_counts.append(mask.sum())\n        prob_true.append(y_true[mask].mean())\n        prob_pred_bin.append(p_pred[mask].mean())\n\n    prob_true = np.array(prob_true)\n    prob_pred_bin = np.array(prob_pred_bin)\n    bin_counts = np.array(bin_counts)\n    total = bin_counts.sum() if len(bin_counts) > 0 else 1.0\n\n    ece = 0.0\n    if len(bin_counts) > 0:\n        ece = np.sum((bin_counts / total) * np.abs(prob_true - prob_pred_bin))\n\n    # Plots: ROC, PR, Calibration\n    fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n\n    # ROC curve\n    try:\n        fpr, tpr, _ = roc_curve(y_true, p_pred)\n        ax[0].plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n        ax[0].plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n        ax[0].set_title(f\"{title_prefix} ROC\")\n        ax[0].set_xlabel(\"False positive rate\")\n        ax[0].set_ylabel(\"True positive rate\")\n        ax[0].legend(loc=\"lower right\")\n    except Exception:\n        ax[0].set_title(f\"{title_prefix} ROC (unavailable)\")\n\n    # PR curve\n    try:\n        prec, rec, _ = precision_recall_curve(y_true, p_pred)\n        ax[1].plot(rec, prec, label=f\"AP={pr_auc:.3f}\")\n        ax[1].set_title(f\"{title_prefix} Precision‚ÄìRecall\")\n        ax[1].set_xlabel(\"Recall\")\n        ax[1].set_ylabel(\"Precision\")\n        ax[1].legend(loc=\"lower left\")\n    except Exception:\n        ax[1].set_title(f\"{title_prefix} PR (unavailable)\")\n\n    # Calibration curve\n    if len(prob_pred_bin) > 0:\n        ax[2].plot(prob_pred_bin, prob_true, \"o-\", label=\"Calibration\")\n        ax[2].plot([0, 1], [0, 1], \"k--\", label=\"Perfect\")\n        ax[2].set_title(f\"Calibration Curve\\nBrier={brier:.3f} | ECE={ece:.3f}\")\n        ax[2].set_xlabel(\"Predicted probability\")\n        ax[2].set_ylabel(\"Observed frequency\")\n        ax[2].legend(loc=\"lower right\")\n    else:\n        ax[2].set_title(\"Calibration (no bins)\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        \"ROC_AUC\": roc_auc,\n        \"PR_AUC\": pr_auc,\n        \"Brier\": brier,\n        \"ECE\": ece,\n    }\n\n# ---------- Training + threshold calibration ----------\n\ndef train_and_calibrate(data):\n    Xtr, Str, ytr = data[\"train\"][\"X_seq\"], data[\"train\"][\"X_stat\"], data[\"train\"][\"y\"]\n    Xva, Sva, yva = data[\"val\"][\"X_seq\"],   data[\"val\"][\"X_stat\"],   data[\"val\"][\"y\"]\n\n    seq_len = Xtr.shape[1]\n    n_seq_f = Xtr.shape[2]\n    n_stat_f = 0 if (Str is None or Str.size == 0) else Str.shape[1]\n\n    model = make_model(seq_len, n_seq_f, n_stat_f=n_stat_f, lr=1e-3)\n\n    es = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True, verbose=1)\n    ckpt = \"/kaggle/working/bilstm_best.keras\"\n    mc  = ModelCheckpoint(ckpt, monitor=\"val_loss\", save_best_only=True, verbose=0)\n\n    if USE_STATIC and n_stat_f > 0:\n        model.fit(\n            [Xtr, Str], ytr,\n            validation_data=([Xva, Sva], yva),\n            epochs=20, batch_size=64,\n            callbacks=[es, mc],\n            verbose=1\n        )\n    else:\n        model.fit(\n            Xtr, ytr,\n            validation_data=(Xva, yva),\n            epochs=20, batch_size=64,\n            callbacks=[es, mc],\n            verbose=1\n        )\n\n    model = tf.keras.models.load_model(ckpt)  # best val\n\n    # calibration on VALIDATION (for threshold)\n    p_val = predict_proba(model, Xva, Sva)\n    tau = calibrate_threshold_on_val(yva, p_val, beta=1.0, thr_min=0.1, thr_max=0.9, default=0.5)\n    print(f\"üéöÔ∏è Calibrated decision threshold œÑ = {tau:.4f}\")\n\n    # Optional: show validation calibration\n    print(\"Validation calibration metrics:\")\n    _ = evaluate_calibration(yva, p_val, n_bins=10, title_prefix=\"Validation\")\n\n    # quick test metrics\n    Xte, Ste, yte = data[\"test\"][\"X_seq\"], data[\"test\"][\"X_stat\"], data[\"test\"][\"y\"]\n    p_test = predict_proba(model, Xte, Ste)\n    y_pred = (p_test >= tau).astype(int)\n\n    calib_test = evaluate_calibration(yte, p_test, n_bins=10, title_prefix=\"Test\")\n\n    print(\"Test metrics @œÑ:\")\n    print(\"  ROC-AUC:\", calib_test[\"ROC_AUC\"])\n    print(\"  PR-AUC :\", calib_test[\"PR_AUC\"])\n    print(\"  F1     :\", f1_score(yte, y_pred, zero_division=0))\n    print(\"  Recall :\", recall_score(yte, y_pred, zero_division=0))\n    print(\"  Prec   :\", precision_score(yte, y_pred, zero_division=0))\n\n    # save simple CSV summary\n    os.makedirs(os.path.dirname(OUT_RESULTS_CSV) or \".\", exist_ok=True)\n    pd.DataFrame([{\n        \"split\": \"test\",\n        \"threshold\": tau,\n        \"ROC_AUC\": calib_test[\"ROC_AUC\"],\n        \"PR_AUC\": calib_test[\"PR_AUC\"],\n        \"F1\": f1_score(yte, y_pred, zero_division=0),\n        \"Recall\": recall_score(yte, y_pred, zero_division=0),\n        \"Precision\": precision_score(yte, y_pred, zero_division=0),\n        \"Brier\": calib_test[\"Brier\"],\n        \"ECE\": calib_test[\"ECE\"],\n    }]).to_csv(OUT_RESULTS_CSV, index=False)\n    print(f\"üìë Saved calibrated test results ‚Üí {OUT_RESULTS_CSV}\")\n\n    return model, tau\n\n\n# ============================================================\n# SEQUENCE ‚Üí HOUR MAPPING  (shape-safe)\n# ============================================================\n\ndef build_sequence_index_map(hourly, split, seq_len):\n    h = hourly.copy()\n    h[\"hour\"] = pd.to_datetime(h[\"hour\"], errors=\"coerce\")\n    h[\"date\"] = h[\"hour\"].dt.normalize()\n\n    s = h[\"Split\"].astype(str).str.lower()\n    sub = (h[s==split.lower()]\n           .sort_values([\"patientID\",\"hour\"])\n           .reset_index()\n           .rename(columns={\"index\":\"row_idx\"}))\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        if len(grp) <= seq_len:\n            continue\n        for i in range(len(grp)-seq_len):\n            tgt = grp.loc[i+seq_len]\n            rows.append({\n                \"seq_idx\": len(rows),\n                \"patientID\": pid,\n                \"hour\": pd.to_datetime(tgt[\"hour\"]),\n                \"date\": pd.to_datetime(tgt[\"date\"]).normalize(),\n                \"row_idx\": int(tgt[\"row_idx\"]),\n            })\n    return pd.DataFrame(rows)\n\n\n# ============================================================\n# DAILY RISK (using calibrated œÑ)\n# ============================================================\n\ndef compute_daily_risk(model, data, hourly, split=\"test\", threshold=0.5):\n    X = data[split][\"X_seq\"]\n    S = data[split][\"X_stat\"]\n    y = np.asarray(data[split][\"y\"]).astype(int).ravel()\n\n    seq_len = X.shape[1]\n    p = predict_proba(model, X, S)\n    y_pred = (p >= float(threshold)).astype(int)\n\n    idx_map = build_sequence_index_map(hourly, split, seq_len)\n    print(\"len(idx_map) =\", len(idx_map), \"len(X)\", len(X))\n    assert len(idx_map) == len(X), \"‚ùå mapping length != #sequences (shape problem)\"\n\n    df_hourly = pd.DataFrame({\n        \"patientID\": idx_map[\"patientID\"].values,\n        \"hour\":      pd.to_datetime(idx_map[\"hour\"]),\n        # ‚úÖ FIX: keep as Series, then use .dt\n        \"date\":      pd.to_datetime(idx_map[\"date\"]).dt.normalize(),\n        \"y_true\":    y,\n        \"y_prob\":    p,\n        \"y_pred\":    y_pred,\n        \"threshold_used\": float(threshold),\n    })\n\n    g = df_hourly.groupby([\"patientID\",\"date\"], as_index=False)\n    df_daily = g.agg(\n        n_hours      = (\"y_true\",\"size\"),\n        true_events  = (\"y_true\",\"sum\"),\n        pred_events  = (\"y_pred\",\"sum\"),\n        risk_mean    = (\"y_prob\",\"mean\"),\n        risk_max     = (\"y_prob\",\"max\"),\n    )\n    df_daily[\"daily_true\"] = (df_daily[\"true_events\"] > 0).astype(int)\n    df_daily[\"daily_pred\"] = (df_daily[\"risk_max\"] >= float(threshold)).astype(int)\n    df_daily[\"threshold_used\"] = float(threshold)\n\n    df_hourly.to_csv(\"/kaggle/working/hourly_predictions_test.csv\", index=False)\n    df_daily.to_csv(OUT_DAILY_RISK, index=False)\n    print(f\"üíæ Saved hourly preds ‚Üí /kaggle/working/hourly_predictions_test.csv\")\n    print(f\"üíæ Saved daily risk  ‚Üí {OUT_DAILY_RISK}\")\n\n    return df_hourly, df_daily\n\n\n# ============================================================\n# MAIN\n# ============================================================\n\nif __name__ == \"__main__\":\n    # 1) Build hourly features\n    hourly = build_hourly_features()\n\n    # 2) Build sequences\n    data = build_sequences(hourly, seq_len=SEQ_LEN)\n\n    # 3) Train + calibrate threshold on VALIDATION\n    model, tau = train_and_calibrate(data)\n\n    # 4) Compute daily risk on TEST using calibrated œÑ\n    df_hourly, df_daily = compute_daily_risk(model, data, hourly, split=\"test\", threshold=tau)\n\n    print(\"\\nüîé Hourly predictions (head):\")\n    print(df_hourly.head())\n\n    print(\"\\nüìä Daily risk (head):\")\n    print(df_daily.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# SHAP explainability for leak-safe Ramadan BiLSTM\n#\n# Fixes:\n#   * Works with TF 2.15/2.16 + Keras 3 by monkey‚Äëpatching\n#     tf.keras.backend.learning_phase / set_learning_phase,\n#     which SHAP's GradientExplainer still calls.\n#\n# Assumes you have already run the training script and created:\n#   /kaggle/working/bilstm_best.keras\n#   /kaggle/working/sequences_leakfree_calibrated.npz\n#   /kaggle/working/dynamic_hourly_features_ramadan.csv\n#   /kaggle/working/results_calibrated.csv\n# ============================================================\n\nimport os\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport shap\n\n# ------------------------------------------------------------\n# SHAP / KERAS 3 COMPAT PATCH\n# ------------------------------------------------------------\n# Keras 3 removed backend.learning_phase & set_learning_phase,\n# but shap.GradientExplainer still uses them. We add light\n# stand‚Äëins to avoid AttributeError without changing behavior\n# in practice (we always evaluate in inference mode).\n#\n# This MUST run before calling shap.GradientExplainer.shap_values().\n# ------------------------------------------------------------\n\nif not hasattr(tf.keras.backend, \"learning_phase\"):\n    # SHAP expects this to be callable\n    def _fake_learning_phase():\n        # 0 -> inference / test mode\n        return 0\n    tf.keras.backend.learning_phase = _fake_learning_phase\n\nif not hasattr(tf.keras.backend, \"set_learning_phase\"):\n    # SHAP uses this as a context switch; we can no‚Äëop safely\n    def _fake_set_learning_phase(value):\n        # value is usually 0 or 1; we ignore it\n        return None\n    tf.keras.backend.set_learning_phase = _fake_set_learning_phase\n\n\n# ------------------------------------------------------------\n# CONFIG + SEED\n# ------------------------------------------------------------\n\nRANDOM_STATE = 42          # üîë keep same seed as training\nnp.random.seed(RANDOM_STATE)\nrandom.seed(RANDOM_STATE)\ntf.random.set_seed(RANDOM_STATE)\n\nUSE_STATIC      = True     # same as in training\n\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree_calibrated.npz\"\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_calibrated.csv\"\nMODEL_CKPT      = \"/kaggle/working/bilstm_best.keras\"\n\n\n# ------------------------------------------------------------\n# LOAD MODEL + DATA\n# ------------------------------------------------------------\n\nprint(\"Loading model and data artifacts...\")\nmodel = tf.keras.models.load_model(MODEL_CKPT)\n\nnpz = np.load(OUT_SEQ_NPZ, allow_pickle=True)\n\nXtr = npz[\"Xtr\"]\nXva = npz[\"Xva\"]\nXte = npz[\"Xte\"]\n\nXtr_stat = npz[\"Xtr_stat\"]\nXva_stat = npz[\"Xva_stat\"]\nXte_stat = npz[\"Xte_stat\"]\n\nytr = npz[\"ytr\"]\nyva = npz[\"yva\"]\nyte = npz[\"yte\"]\n\nseq_features_used    = npz[\"seq_features_used\"].tolist()\nstatic_features_used = npz[\"static_features_used\"].tolist()\n\n\ndef _maybe_static(arr: np.ndarray):\n    \"\"\"Convert empty (0,0) static arrays to None for convenience.\"\"\"\n    if arr.ndim != 2:\n        return None\n    if arr.shape[0] == 0 or arr.shape[1] == 0:\n        return None\n    return arr\n\n\nXtr_stat = _maybe_static(Xtr_stat)\nXva_stat = _maybe_static(Xva_stat)\nXte_stat = _maybe_static(Xte_stat)\n\ndata = {\n    \"train\": {\"X_seq\": Xtr, \"X_stat\": Xtr_stat, \"y\": ytr},\n    \"val\":   {\"X_seq\": Xva, \"X_stat\": Xva_stat, \"y\": yva},\n    \"test\":  {\"X_seq\": Xte, \"X_stat\": Xte_stat, \"y\": yte},\n    \"seq_features_used\": seq_features_used,\n    \"static_features_used\": static_features_used,\n}\n\nhourly  = pd.read_csv(OUT_HOURLY_CSV)\nresults = pd.read_csv(OUT_RESULTS_CSV)\ntau = float(results[\"threshold\"].iloc[0])\n\nprint(\"Loaded.\")\nprint(\"  Test shapes:\", Xte.shape, \"(seq),\",\n      \"None\" if Xte_stat is None else Xte_stat.shape, \"(static)\")\nprint(\"  Calibrated threshold œÑ =\", tau)\n\n\n# ------------------------------------------------------------\n# SMALL HELPERS\n# ------------------------------------------------------------\n\ndef predict_proba(model, X, S=None):\n    \"\"\"Return model probabilities with/without static input.\"\"\"\n    if USE_STATIC and S is not None:\n        return model.predict([X, S], verbose=0).ravel()\n    return model.predict(X, verbose=0).ravel()\n\n\ndef build_sequence_index_map(hourly_df: pd.DataFrame, split: str, seq_len: int):\n    \"\"\"\n    Rebuild mapping from sequence index (within split) ‚Üí (patientID, hour, date)\n    Must be consistent with how sequences were built for training.\n\n    For each patient:\n      - sort by hour\n      - create sliding windows of length `seq_len`\n      - predict at the next hour after the window\n    \"\"\"\n    h = hourly_df.copy()\n    h[\"hour\"] = pd.to_datetime(h[\"hour\"], errors=\"coerce\")\n    h[\"date\"] = pd.to_datetime(h[\"date\"], errors=\"coerce\").dt.normalize()\n\n    s = h[\"Split\"].astype(str).str.lower()\n    sub = (h[s == split.lower()]\n           .sort_values([\"patientID\", \"hour\"])\n           .reset_index()\n           .rename(columns={\"index\": \"row_idx\"}))\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        if len(grp) <= seq_len:\n            continue\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i + seq_len]\n            rows.append({\n                \"seq_idx\": len(rows),\n                \"patientID\": pid,\n                \"hour\": pd.to_datetime(tgt[\"hour\"]),\n                \"date\": pd.to_datetime(tgt[\"date\"]).normalize(),\n                \"row_idx\": int(tgt[\"row_idx\"]),\n            })\n    return pd.DataFrame(rows)\n\n\ndef _unpack_shap_values(shap_values, has_static: bool):\n    \"\"\"\n    Handle SHAP output structures for GradientExplainer, supporting:\n      * single-input, single-output\n      * multi-input (seq + static), single-output\n\n    For multi-input we might get:\n        [seq_array, static_array]\n    or:\n        [[seq_array, static_array]]  (one output, two inputs)\n    \"\"\"\n    # Multi-input case: sequence + static\n    if has_static:\n        if not isinstance(shap_values, list) or len(shap_values) == 0:\n            raise ValueError(\"Unexpected SHAP structure for multi-input model.\")\n\n        first = shap_values[0]\n\n        # Case: [[seq, static]] (list of outputs, here 1 output)\n        if isinstance(first, list):\n            input_vals = first\n        # Case: [seq, static] (direct list of inputs)\n        elif isinstance(first, np.ndarray):\n            input_vals = shap_values\n        else:\n            raise ValueError(\"Unexpected nested SHAP structure for multi-input model.\")\n\n        shap_seq = np.array(input_vals[0])\n        shap_stat = np.array(input_vals[1]) if len(input_vals) > 1 else None\n\n    # Single-input case: only sequence features\n    else:\n        if isinstance(shap_values, list):\n            if len(shap_values) == 1 and isinstance(shap_values[0], np.ndarray):\n                shap_seq = np.array(shap_values[0])\n            else:\n                # If multiple outputs, just take the first one\n                first = shap_values[0]\n                if isinstance(first, np.ndarray):\n                    shap_seq = np.array(first)\n                else:\n                    raise ValueError(\"Unexpected SHAP structure for single-input model.\")\n        else:\n            shap_seq = np.array(shap_values)\n\n        shap_stat = None\n\n    return shap_seq, shap_stat\n\n\n# ------------------------------------------------------------\n# SHAP EXPLAINER\n# ------------------------------------------------------------\n\ndef build_shap_explainer(model, data, bg_split=\"train\", bg_size=128):\n    \"\"\"\n    Build GradientExplainer with a small background set from the training split.\n\n    This uses our Keras‚Äë3‚Äëcompatible monkey‚Äëpatch above to avoid\n    the tf.keras.backend.learning_phase() AttributeError.\n    \"\"\"\n    X_bg = data[bg_split][\"X_seq\"]\n    S_bg = data[bg_split][\"X_stat\"]\n\n    n_bg = len(X_bg)\n    if bg_size is not None and n_bg > bg_size:\n        idx = np.random.choice(n_bg, size=bg_size, replace=False)\n        X_bg = X_bg[idx]\n        if S_bg is not None:\n            S_bg = S_bg[idx]\n\n    if USE_STATIC and S_bg is not None:\n        background = [X_bg, S_bg]\n    else:\n        background = X_bg\n\n    print(f\"Building GradientExplainer with background size = {len(X_bg)}\")\n    explainer = shap.GradientExplainer(model, background)\n    return explainer\n\n\n# ------------------------------------------------------------\n# GLOBAL SHAP (overall feature importance)\n# ------------------------------------------------------------\n\ndef global_shap(explainer, data, split=\"test\", nsamples=256):\n    \"\"\"\n    Compute global SHAP importance for sequence and static features.\n    Sequence SHAP is aggregated over time (sequence length) and samples.\n    \"\"\"\n    X = data[split][\"X_seq\"]\n    S = data[split][\"X_stat\"]\n    n = len(X)\n\n    if nsamples is not None and n > nsamples:\n        idx = np.random.choice(n, size=nsamples, replace=False)\n        X_sample = X[idx]\n        S_sample = S[idx] if S is not None else None\n    else:\n        X_sample = X\n        S_sample = S\n\n    # GradientExplainer with multi-input returns nested lists for outputs/inputs.\n    if USE_STATIC and S_sample is not None:\n        raw_shap = explainer.shap_values([X_sample, S_sample])\n        shap_seq, shap_stat = _unpack_shap_values(raw_shap, has_static=True)\n    else:\n        raw_shap = explainer.shap_values(X_sample)\n        shap_seq, shap_stat = _unpack_shap_values(raw_shap, has_static=False)\n\n    # ---- sequence features: aggregate over time + samples ----\n    seq_feat_names = data[\"seq_features_used\"]\n    # shap_seq shape: (N, T, F_seq)\n    mean_abs_seq = np.abs(shap_seq).mean(axis=(0, 1))\n    df_seq_imp = (\n        pd.DataFrame({\n            \"feature\": seq_feat_names,\n            \"mean_abs_shap\": mean_abs_seq\n        })\n        .sort_values(\"mean_abs_shap\", ascending=False)\n        .reset_index(drop=True)\n    )\n\n    print(\"\\nGlobal importance ‚Äì sequence features:\")\n    print(df_seq_imp)\n\n    plt.figure(figsize=(6, 4))\n    plt.barh(df_seq_imp[\"feature\"], df_seq_imp[\"mean_abs_shap\"])\n    plt.xlabel(\"Mean |SHAP| (over time & samples)\")\n    plt.title(f\"Global importance ({split}, sequence features)\")\n    plt.tight_layout()\n    plt.show()\n\n    # ---- static features (if present) ----\n    df_stat_imp = None\n    if shap_stat is not None and len(data[\"static_features_used\"]) > 0:\n        stat_feat_names = data[\"static_features_used\"]\n        # shap_stat shape: (N, F_stat)\n        mean_abs_stat = np.abs(shap_stat).mean(axis=0)\n        df_stat_imp = (\n            pd.DataFrame({\n                \"feature\": stat_feat_names,\n                \"mean_abs_shap\": mean_abs_stat\n            })\n            .sort_values(\"mean_abs_shap\", ascending=False)\n            .reset_index(drop=True)\n        )\n\n        print(\"\\nGlobal importance ‚Äì static features:\")\n        print(df_stat_imp)\n\n        plt.figure(figsize=(6, 4))\n        plt.barh(df_stat_imp[\"feature\"], df_stat_imp[\"mean_abs_shap\"])\n        plt.xlabel(\"Mean |SHAP|\")\n        plt.title(f\"Global importance ({split}, static features)\")\n        plt.tight_layout()\n        plt.show()\n\n    return {\n        \"X_sample\": X_sample,\n        \"S_sample\": S_sample,\n        \"shap_seq\": shap_seq,\n        \"shap_stat\": shap_stat,\n        \"seq_importance\": df_seq_imp,\n        \"static_importance\": df_stat_imp,\n    }\n\n\n# ------------------------------------------------------------\n# PATIENT-LEVEL SHAP\n# ------------------------------------------------------------\n\ndef explain_patient(explainer, model, data, hourly_df, split, patient_id, threshold):\n    \"\"\"\n    Get SHAP explanations and predictions for all sequences\n    belonging to a single patient in a given split.\n    \"\"\"\n    X = data[split][\"X_seq\"]\n    S = data[split][\"X_stat\"]\n    y = data[split][\"y\"].astype(int).ravel()\n\n    idx_map = build_sequence_index_map(hourly_df, split=split, seq_len=X.shape[1])\n    sub = idx_map[idx_map[\"patientID\"] == patient_id].copy()\n    if sub.empty:\n        raise ValueError(f\"No sequences found for patientID={patient_id} in split {split}.\")\n\n    seq_idx = sub[\"seq_idx\"].astype(int).values\n\n    X_p = X[seq_idx]\n    S_p = S[seq_idx] if S is not None else None\n    y_true = y[seq_idx]\n\n    p = predict_proba(model, X_p, S_p)\n    y_pred = (p >= float(threshold)).astype(int)\n\n    # SHAP values for this patient's sequences\n    if USE_STATIC and S_p is not None:\n        raw_shap = explainer.shap_values([X_p, S_p])\n        shap_seq, shap_stat = _unpack_shap_values(raw_shap, has_static=True)\n    else:\n        raw_shap = explainer.shap_values(X_p)\n        shap_seq, shap_stat = _unpack_shap_values(raw_shap, has_static=False)\n\n    df_p = (\n        pd.DataFrame({\n            \"seq_idx\": seq_idx,\n            \"patientID\": sub[\"patientID\"].values,\n            \"hour\": pd.to_datetime(sub[\"hour\"].values),\n            \"date\": pd.to_datetime(sub[\"date\"].values).normalize(),\n            \"y_true\": y_true,\n            \"y_prob\": p,\n            \"y_pred\": y_pred,\n        })\n        .sort_values(\"hour\")\n        .reset_index(drop=True)\n    )\n\n    return df_p, shap_seq, shap_stat\n\n\ndef plot_patient_risk_timeline(df_p, threshold, title=None):\n    \"\"\"\n    Plot predicted risk over time for one patient, with threshold\n    and vertical lines at true hypo hours.\n    \"\"\"\n    t = df_p[\"hour\"]\n    risk = df_p[\"y_prob\"]\n    hypo = df_p[\"y_true\"] == 1\n\n    if title is None:\n        pid = df_p[\"patientID\"].iloc[0]\n        title = f\"Patient {pid} ‚Äì hourly hypoglycaemia risk\"\n\n    plt.figure(figsize=(12, 4))\n    plt.plot(t, risk, marker=\"o\", label=\"Predicted risk\")\n    plt.axhline(threshold, linestyle=\"--\", label=f\"Threshold œÑ={threshold:.3f}\")\n    for ts in t[hypo]:\n        plt.axvline(ts, linestyle=\":\", alpha=0.3, color=\"red\")\n    plt.legend()\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Predicted probability\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_patient_shap_heatmap(df_p, shap_seq, seq_feature_names, top_k=6):\n    \"\"\"\n    Heatmap: x-axis = sequences (time), y-axis = features,\n    values = mean SHAP over window for each feature.\n    \"\"\"\n    # shap_seq shape: (n_seq, seq_len, n_feat)\n    shap_agg = shap_seq.mean(axis=1)  # (n_seq, n_feat)\n\n    mean_abs = np.abs(shap_agg).mean(axis=0)\n    # pick top_k features by global mean |SHAP|\n    top_idx = np.argsort(mean_abs)[::-1][:top_k]\n    shap_top = shap_agg[:, top_idx]\n    feat_top = [seq_feature_names[i] for i in top_idx]\n\n    plt.figure(figsize=(12, 0.6 * top_k + 3))\n    plt.imshow(shap_top.T, aspect=\"auto\", interpolation=\"nearest\")\n    plt.colorbar(label=\"Mean SHAP over window\")\n    plt.yticks(range(len(feat_top)), feat_top)\n    plt.xticks(\n        range(len(df_p)),\n        df_p[\"hour\"].dt.strftime(\"%m-%d %H:%M\"),\n        rotation=90\n    )\n    pid = df_p[\"patientID\"].iloc[0]\n    plt.title(f\"Per-window feature contributions ‚Äì patient {pid}\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef build_daily_shap_summary(df_p, shap_seq, seq_feature_names, top_k=5):\n    \"\"\"\n    Daily risk stats + daily mean |SHAP| for top-k sequence features.\n    \"\"\"\n    df = df_p.copy()\n    df[\"date\"] = df[\"hour\"].dt.normalize()\n\n    daily_stats = df.groupby(\"date\").agg(\n        n_windows=(\"y_prob\", \"size\"),\n        mean_risk=(\"y_prob\", \"mean\"),\n        max_risk=(\"y_prob\", \"max\"),\n        true_events=(\"y_true\", \"sum\"),\n        pred_events=(\"y_pred\", \"sum\"),\n    )\n\n    # aggregate SHAP per sequence (mean over time), then per day\n    shap_agg = shap_seq.mean(axis=1)  # (n_seq, n_feat)\n    shap_df = pd.DataFrame(shap_agg, columns=seq_feature_names)\n    shap_df[\"date\"] = df[\"date\"].values\n\n    mean_abs_global = np.abs(shap_agg).mean(axis=0)\n    top_idx = np.argsort(mean_abs_global)[::-1][:top_k]\n    top_features = [seq_feature_names[i] for i in top_idx]\n\n    daily_shap = shap_df.groupby(\"date\")[top_features].apply(\n        lambda g: np.abs(g).mean(axis=0)\n    )\n\n    return daily_stats, daily_shap\n\n\n# ------------------------------------------------------------\n# MAIN\n# ------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    # 1) Build SHAP explainer on training background\n    explainer = build_shap_explainer(model, data, bg_split=\"train\", bg_size=128)\n\n    # 2) Global SHAP on test set\n    global_info = global_shap(explainer, data, split=\"test\", nsamples=256)\n\n    # 3) Example patient from test split\n    test_mask = hourly[\"Split\"].astype(str).str.lower() == \"test\"\n    test_pids = hourly.loc[test_mask, \"patientID\"].unique()\n\n    if len(test_pids) > 0:\n        example_pid = test_pids[0]\n        print(f\"\\nExplaining patient {example_pid} in test split...\")\n\n        df_p, shap_seq_p, shap_stat_p = explain_patient(\n            explainer,\n            model,\n            data,\n            hourly_df=hourly,\n            split=\"test\",\n            patient_id=example_pid,\n            threshold=tau,\n        )\n\n        # risk timeline\n        plot_patient_risk_timeline(df_p, threshold=tau)\n\n        # SHAP heatmap over time\n        plot_patient_shap_heatmap(df_p, shap_seq_p, data[\"seq_features_used\"], top_k=6)\n\n        # per-day risk + SHAP summaries\n        daily_stats, daily_shap = build_daily_shap_summary(\n            df_p, shap_seq_p, data[\"seq_features_used\"], top_k=5\n        )\n        print(\"\\nDaily risk stats for example patient:\")\n        print(daily_stats)\n\n        print(\"\\nDaily mean |SHAP| for top sequence features:\")\n        print(daily_shap)\n    else:\n        print(\"No patients in test split. Check hourly['Split'] values.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# SHAP explainability for leak-safe Ramadan LSTM / BiLSTM\n#\n# Works with:\n#   - TensorFlow 2.15/2.16\n#   - Keras 3\n#   - shap (GradientExplainer)\n#\n# Fixes:\n#   * Monkey-patches tf.keras.backend.learning_phase and\n#     tf.keras.backend.set_learning_phase, which SHAP still calls.\n#\n# Assumes you already created:\n#   /kaggle/working/bilstm_best.keras\n#   /kaggle/working/sequences_leakfree_calibrated.npz\n#   /kaggle/working/dynamic_hourly_features_ramadan.csv\n#   /kaggle/working/results_calibrated.csv\n# ============================================================\n\nimport os\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport shap\n\n# ------------------------------------------------------------\n# SHAP / KERAS 3 COMPAT PATCH\n# ------------------------------------------------------------\n# Keras 3 removed backend.learning_phase & set_learning_phase,\n# but shap.GradientExplainer still uses them.\n# We define no-op stand-ins:\n#   - learning_phase() always returns 0 (inference).\n#   - set_learning_phase(value) does nothing.\n# ------------------------------------------------------------\n\nif not hasattr(tf.keras.backend, \"learning_phase\"):\n    def _fake_learning_phase():\n        return 0\n    tf.keras.backend.learning_phase = _fake_learning_phase\n\nif not hasattr(tf.keras.backend, \"set_learning_phase\"):\n    def _fake_set_learning_phase(value):\n        return None\n    tf.keras.backend.set_learning_phase = _fake_set_learning_phase\n\n\n# ------------------------------------------------------------\n# CONFIG + SEED\n# ------------------------------------------------------------\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nrandom.seed(RANDOM_STATE)\ntf.random.set_seed(RANDOM_STATE)\n\nUSE_STATIC      = True  # True for dual-input (seq + static) models\n\nOUT_SEQ_NPZ     = \"/kaggle/working/sequences_leakfree_calibrated.npz\"\nOUT_HOURLY_CSV  = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nOUT_RESULTS_CSV = \"/kaggle/working/results_calibrated.csv\"\nMODEL_CKPT      = \"/kaggle/working/bilstm_best.keras\"  # can be LSTM or BiLSTM\n\n\n# ------------------------------------------------------------\n# LOAD MODEL + DATA\n# ------------------------------------------------------------\n\nprint(\"Loading model and data artifacts...\")\nmodel = tf.keras.models.load_model(MODEL_CKPT)\n\nnpz = np.load(OUT_SEQ_NPZ, allow_pickle=True)\n\nXtr = npz[\"Xtr\"]\nXva = npz[\"Xva\"]\nXte = npz[\"Xte\"]\n\nXtr_stat = npz[\"Xtr_stat\"]\nXva_stat = npz[\"Xva_stat\"]\nXte_stat = npz[\"Xte_stat\"]\n\nytr = npz[\"ytr\"]\nyva = npz[\"yva\"]\nyte = npz[\"yte\"]\n\nseq_features_used    = npz[\"seq_features_used\"].tolist()\nstatic_features_used = npz[\"static_features_used\"].tolist()\n\ndef _maybe_static(arr: np.ndarray):\n    \"\"\"Convert empty (0,0) static arrays to None.\"\"\"\n    if arr.ndim != 2:\n        return None\n    if arr.shape[0] == 0 or arr.shape[1] == 0:\n        return None\n    return arr\n\nXtr_stat = _maybe_static(Xtr_stat)\nXva_stat = _maybe_static(Xva_stat)\nXte_stat = _maybe_static(Xte_stat)\n\ndata = {\n    \"train\": {\"X_seq\": Xtr, \"X_stat\": Xtr_stat, \"y\": ytr},\n    \"val\":   {\"X_seq\": Xva, \"X_stat\": Xva_stat, \"y\": yva},\n    \"test\":  {\"X_seq\": Xte, \"X_stat\": Xte_stat, \"y\": yte},\n    \"seq_features_used\": seq_features_used,\n    \"static_features_used\": static_features_used,\n}\n\nhourly  = pd.read_csv(OUT_HOURLY_CSV)\nresults = pd.read_csv(OUT_RESULTS_CSV)\ntau = float(results[\"threshold\"].iloc[0])\n\nprint(\"Loaded.\")\nprint(\"  Test shapes:\", Xte.shape, \"(seq),\",\n      \"None\" if Xte_stat is None else Xte_stat.shape, \"(static)\")\nprint(\"  Calibrated threshold œÑ =\", tau)\n\n\n# ------------------------------------------------------------\n# SMALL HELPERS\n# ------------------------------------------------------------\n\ndef predict_proba(model, X, S=None):\n    \"\"\"Return model probabilities with/without static input.\"\"\"\n    if USE_STATIC and S is not None:\n        return model.predict([X, S], verbose=0).ravel()\n    return model.predict(X, verbose=0).ravel()\n\n\ndef build_sequence_index_map(hourly_df: pd.DataFrame, split: str, seq_len: int):\n    \"\"\"\n    Rebuild mapping from sequence index (within split) ‚Üí (patientID, hour, date).\n    Must mirror the sequence-building logic used in training.\n\n    For each patient:\n      - sort by hour\n      - create sliding windows of length `seq_len`\n      - each window predicts the hour immediately after the window\n    \"\"\"\n    h = hourly_df.copy()\n    h[\"hour\"] = pd.to_datetime(h[\"hour\"], errors=\"coerce\")\n    h[\"date\"] = pd.to_datetime(h[\"date\"], errors=\"coerce\").dt.normalize()\n\n    s = h[\"Split\"].astype(str).str.lower()\n    sub = (h[s == split.lower()]\n           .sort_values([\"patientID\", \"hour\"])\n           .reset_index()\n           .rename(columns={\"index\": \"row_idx\"}))\n\n    rows = []\n    for pid, grp in sub.groupby(\"patientID\", sort=True):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        if len(grp) <= seq_len:\n            continue\n        for i in range(len(grp) - seq_len):\n            tgt = grp.loc[i + seq_len]\n            rows.append({\n                \"seq_idx\": len(rows),\n                \"patientID\": pid,\n                \"hour\": pd.to_datetime(tgt[\"hour\"]),\n                \"date\": pd.to_datetime(tgt[\"date\"]).normalize(),\n                \"row_idx\": int(tgt[\"row_idx\"]),\n            })\n    return pd.DataFrame(rows)\n\n\ndef _unpack_shap_values(shap_values, has_static: bool):\n    \"\"\"\n    Handle SHAP output structures for GradientExplainer:\n\n    Single-output model ‚Üí\n      - single-input: [array]\n      - multi-input: [[seq_array, static_array]] or [seq_array, static_array]\n    \"\"\"\n    if has_static:\n        if not isinstance(shap_values, list) or len(shap_values) == 0:\n            raise ValueError(\"Unexpected SHAP structure for multi-input model.\")\n\n        first = shap_values[0]\n\n        # [[seq, static]] (list of outputs -> list of inputs)\n        if isinstance(first, list):\n            input_vals = first\n        # [seq, static]\n        elif isinstance(first, np.ndarray):\n            input_vals = shap_values\n        else:\n            raise ValueError(\"Unexpected nested SHAP structure for multi-input model.\")\n\n        shap_seq = np.array(input_vals[0])\n        shap_stat = np.array(input_vals[1]) if len(input_vals) > 1 else None\n\n    else:\n        # single-input\n        if isinstance(shap_values, list):\n            if len(shap_values) == 1 and isinstance(shap_values[0], np.ndarray):\n                shap_seq = np.array(shap_values[0])\n            else:\n                first = shap_values[0]\n                if isinstance(first, np.ndarray):\n                    shap_seq = np.array(first)\n                else:\n                    raise ValueError(\"Unexpected SHAP structure for single-input model.\")\n        else:\n            shap_seq = np.array(shap_values)\n\n        shap_stat = None\n\n    return shap_seq, shap_stat\n\n\n# ------------------------------------------------------------\n# SHAP EXPLAINER\n# ------------------------------------------------------------\n\ndef build_shap_explainer(model, data, bg_split=\"train\", bg_size=128):\n    \"\"\"\n    Build GradientExplainer with a background set from the given split.\n    \"\"\"\n    X_bg = data[bg_split][\"X_seq\"]\n    S_bg = data[bg_split][\"X_stat\"]\n\n    n_bg = len(X_bg)\n    if bg_size is not None and n_bg > bg_size:\n        idx = np.random.choice(n_bg, size=bg_size, replace=False)\n        X_bg = X_bg[idx]\n        if S_bg is not None:\n            S_bg = S_bg[idx]\n\n    if USE_STATIC and S_bg is not None:\n        background = [X_bg, S_bg]\n    else:\n        background = X_bg\n\n    print(f\"Building GradientExplainer with background size = {len(X_bg)}\")\n    explainer = shap.GradientExplainer(model, background)\n    return explainer\n\n\n# ------------------------------------------------------------\n# GLOBAL SHAP (overall feature importance)\n# ------------------------------------------------------------\n\ndef global_shap(explainer, data, split=\"test\", nsamples=256):\n    \"\"\"\n    Compute global SHAP importance for sequence and static features.\n    Sequence SHAP is aggregated over time and samples.\n    \"\"\"\n    X = data[split][\"X_seq\"]\n    S = data[split][\"X_stat\"]\n    n = len(X)\n\n    if nsamples is not None and n > nsamples:\n        idx = np.random.choice(n, size=nsamples, replace=False)\n        X_sample = X[idx]\n        S_sample = S[idx] if S is not None else None\n    else:\n        X_sample = X\n        S_sample = S\n\n    if USE_STATIC and S_sample is not None:\n        raw_shap = explainer.shap_values([X_sample, S_sample])\n        shap_seq, shap_stat = _unpack_shap_values(raw_shap, has_static=True)\n    else:\n        raw_shap = explainer.shap_values(X_sample)\n        shap_seq, shap_stat = _unpack_shap_values(raw_shap, has_static=False)\n\n    # ---- sequence features: aggregate over time + samples ----\n    seq_feat_names = data[\"seq_features_used\"]\n    # shap_seq shape: (N, T, F_seq)\n    mean_abs_seq = np.abs(shap_seq).mean(axis=(0, 1))\n    df_seq_imp = (\n        pd.DataFrame({\n            \"feature\": seq_feat_names,\n            \"mean_abs_shap\": mean_abs_seq\n        })\n        .sort_values(\"mean_abs_shap\", ascending=False)\n        .reset_index(drop=True)\n    )\n\n    print(\"\\nGlobal importance ‚Äì sequence features:\")\n    print(df_seq_imp)\n\n    plt.figure(figsize=(6, 4))\n    plt.barh(df_seq_imp[\"feature\"], df_seq_imp[\"mean_abs_shap\"])\n    plt.xlabel(\"Mean |SHAP| (over time & samples)\")\n    plt.title(f\"Global importance ({split}, sequence features)\")\n    plt.tight_layout()\n    plt.show()\n\n    # ---- static features (if present) ----\n    df_stat_imp = None\n    if shap_stat is not None and len(data[\"static_features_used\"]) > 0:\n        stat_feat_names = data[\"static_features_used\"]\n        mean_abs_stat = np.abs(shap_stat).mean(axis=0)\n        df_stat_imp = (\n            pd.DataFrame({\n                \"feature\": stat_feat_names,\n                \"mean_abs_shap\": mean_abs_stat\n            })\n            .sort_values(\"mean_abs_shap\", ascending=False)\n            .reset_index(drop=True)\n        )\n\n        print(\"\\nGlobal importance ‚Äì static features:\")\n        print(df_stat_imp)\n\n        plt.figure(figsize=(6, 4))\n        plt.barh(df_stat_imp[\"feature\"], df_stat_imp[\"mean_abs_shap\"])\n        plt.xlabel(\"Mean |SHAP|\")\n        plt.title(f\"Global importance ({split}, static features)\")\n        plt.tight_layout()\n        plt.show()\n\n    return {\n        \"X_sample\": X_sample,\n        \"S_sample\": S_sample,\n        \"shap_seq\": shap_seq,\n        \"shap_stat\": shap_stat,\n        \"seq_importance\": df_seq_imp,\n        \"static_importance\": df_stat_imp,\n    }\n\n\n# ------------------------------------------------------------\n# PATIENT-LEVEL SHAP\n# ------------------------------------------------------------\n\ndef explain_patient(explainer, model, data, hourly_df, split, patient_id, threshold):\n    \"\"\"\n    SHAP explanations + predictions for all sequences\n    for a single patient in a given split.\n    \"\"\"\n    X = data[split][\"X_seq\"]\n    S = data[split][\"X_stat\"]\n    y = data[split][\"y\"].astype(int).ravel()\n\n    idx_map = build_sequence_index_map(hourly_df, split=split, seq_len=X.shape[1])\n    sub = idx_map[idx_map[\"patientID\"] == patient_id].copy()\n    if sub.empty:\n        raise ValueError(f\"No sequences found for patientID={patient_id} in split {split}.\")\n\n    seq_idx = sub[\"seq_idx\"].astype(int).values\n\n    X_p = X[seq_idx]\n    S_p = S[seq_idx] if S is not None else None\n    y_true = y[seq_idx]\n\n    p = predict_proba(model, X_p, S_p)\n    y_pred = (p >= float(threshold)).astype(int)\n\n    if USE_STATIC and S_p is not None:\n        raw_shap = explainer.shap_values([X_p, S_p])\n        shap_seq, shap_stat = _unpack_shap_values(raw_shap, has_static=True)\n    else:\n        raw_shap = explainer.shap_values(X_p)\n        shap_seq, shap_stat = _unpack_shap_values(raw_shap, has_static=False)\n\n    df_p = (\n        pd.DataFrame({\n            \"seq_idx\": seq_idx,\n            \"patientID\": sub[\"patientID\"].values,\n            \"hour\": pd.to_datetime(sub[\"hour\"].values),\n            \"date\": pd.to_datetime(sub[\"date\"].values).normalize(),\n            \"y_true\": y_true,\n            \"y_prob\": p,\n            \"y_pred\": y_pred,\n        })\n        .sort_values(\"hour\")\n        .reset_index(drop=True)\n    )\n\n    return df_p, shap_seq, shap_stat\n\n\ndef plot_patient_risk_timeline(df_p, threshold, title=None):\n    \"\"\"Plot risk over time for one patient, with threshold & true hypoglycaemia.\"\"\"\n    t = df_p[\"hour\"]\n    risk = df_p[\"y_prob\"]\n    hypo = df_p[\"y_true\"] == 1\n\n    if title is None:\n        pid = df_p[\"patientID\"].iloc[0]\n        title = f\"Patient {pid} ‚Äì hourly hypoglycaemia risk\"\n\n    plt.figure(figsize=(12, 4))\n    plt.plot(t, risk, marker=\"o\", label=\"Predicted risk\")\n    plt.axhline(threshold, linestyle=\"--\", label=f\"Threshold œÑ={threshold:.3f}\")\n    for ts in t[hypo]:\n        plt.axvline(ts, linestyle=\":\", alpha=0.3, color=\"red\")\n    plt.legend()\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Predicted probability\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_patient_shap_heatmap(df_p, shap_seq, seq_feature_names, top_k=6):\n    \"\"\"\n    Heatmap: x-axis = sequences (time), y-axis = features,\n    values = mean SHAP over window for each feature.\n    \"\"\"\n    shap_agg = shap_seq.mean(axis=1)  # (n_seq, n_feat)\n\n    mean_abs = np.abs(shap_agg).mean(axis=0)\n    top_idx = np.argsort(mean_abs)[::-1][:top_k]\n    shap_top = shap_agg[:, top_idx]\n    feat_top = [seq_feature_names[i] for i in top_idx]\n\n    plt.figure(figsize=(12, 0.6 * top_k + 3))\n    plt.imshow(shap_top.T, aspect=\"auto\", interpolation=\"nearest\")\n    plt.colorbar(label=\"Mean SHAP over window\")\n    plt.yticks(range(len(feat_top)), feat_top)\n    plt.xticks(\n        range(len(df_p)),\n        df_p[\"hour\"].dt.strftime(\"%m-%d %H:%M\"),\n        rotation=90\n    )\n    pid = df_p[\"patientID\"].iloc[0]\n    plt.title(f\"Per-window feature contributions ‚Äì patient {pid}\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef build_daily_shap_summary(df_p, shap_seq, seq_feature_names, top_k=5):\n    \"\"\"\n    Daily risk stats + daily mean |SHAP| for top-k sequence features.\n    \"\"\"\n    df = df_p.copy()\n    df[\"date\"] = df[\"hour\"].dt.normalize()\n\n    daily_stats = df.groupby(\"date\").agg(\n        n_windows=(\"y_prob\", \"size\"),\n        mean_risk=(\"y_prob\", \"mean\"),\n        max_risk=(\"y_prob\", \"max\"),\n        true_events=(\"y_true\", \"sum\"),\n        pred_events=(\"y_pred\", \"sum\"),\n    )\n\n    shap_agg = shap_seq.mean(axis=1)  # (n_seq, n_feat)\n    shap_df = pd.DataFrame(shap_agg, columns=seq_feature_names)\n    shap_df[\"date\"] = df[\"date\"].values\n\n    mean_abs_global = np.abs(shap_agg).mean(axis=0)\n    top_idx = np.argsort(mean_abs_global)[::-1][:top_k]\n    top_features = [seq_feature_names[i] for i in top_idx]\n\n    daily_shap = shap_df.groupby(\"date\")[top_features].apply(\n        lambda g: np.abs(g).mean(axis=0)\n    )\n\n    return daily_stats, daily_shap\n\n\n# ------------------------------------------------------------\n# MAIN\n# ------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    # 1) Build SHAP explainer on training background\n    explainer = build_shap_explainer(model, data, bg_split=\"train\", bg_size=128)\n\n    # 2) Global SHAP on test set\n    global_info = global_shap(explainer, data, split=\"test\", nsamples=256)\n\n    # 3) Example patient from test split\n    test_mask = hourly[\"Split\"].astype(str).str.lower() == \"test\"\n    test_pids = hourly.loc[test_mask, \"patientID\"].unique()\n\n    if len(test_pids) > 0:\n        example_pid = test_pids[0]\n        print(f\"\\nExplaining patient {example_pid} in test split...\")\n\n        df_p, shap_seq_p, shap_stat_p = explain_patient(\n            explainer,\n            model,\n            data,\n            hourly_df=hourly,\n            split=\"test\",\n            patient_id=example_pid,\n            threshold=tau,\n        )\n\n        # Risk timeline\n        plot_patient_risk_timeline(df_p, threshold=tau)\n\n        # SHAP heatmap over time\n        plot_patient_shap_heatmap(\n            df_p,\n            shap_seq_p,\n            data[\"seq_features_used\"],\n            top_k=6\n        )\n\n        # Per-day risk + SHAP summaries\n        daily_stats, daily_shap = build_daily_shap_summary(\n            df_p,\n            shap_seq_p,\n            data[\"seq_features_used\"],\n            top_k=5\n        )\n        print(\"\\nDaily risk stats for example patient:\")\n        print(daily_stats)\n\n        print(\"\\nDaily mean |SHAP| for top sequence features:\")\n        print(daily_shap)\n    else:\n        print(\"No patients in test split. Check hourly['Split'] values.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}