{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10739756,"sourceType":"datasetVersion","datasetId":6659625},{"sourceId":13049616,"sourceType":"datasetVersion","datasetId":8213963}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. ID mapping \n\nyour final_master_sheet_clean.xlsx has a unified patient ID column that lines up with your intraday data.\nwhat we‚Äôll do\n\n##### 1. load final_master_sheet_clean.xlsx.\n##### 2. build a mapping Code ‚Üí patientID (from the table you provided).\n##### 3. for every visit sheet (Ramadan, Visit 1 ‚Ä¶ Visit 7), replace/add a column PatientID (Huawei Data).\n##### 4. keep the original Code if you like, or drop it once you confirm the mapping is correct.\n##### 5. save a new Excel (e.g., final_master_sheet_clean_with_huawei.xlsx).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\n# input/output\nMASTER_PATH = Path(\"/kaggle/input/static-variables/final_master_sheet_clean.xlsx\")\nOUT_PATH = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\n\n# your mapping dictionary\nPATIENT_ID_MAP = {\n    \"R01\": 45, \"R02\": 46, \"R04\": 47, \"R05\": 48, \"R06\": 49, \"R07\": 53,\n    \"R10\": 54, \"R11\": 55, \"R12\": 57, \"R15\": 59, \"R16\": 60, \"R17\": 61,\n    \"R20\": 63, \"R21\": 64, \"R22\": 66, \"R23\": 67, \"R24\": 68, \"R25\": 69,\n    \"R26\": 70, \"R27\": 71, \"R28\": 72, \"R29\": 73, \"R30\": 74, \"R31\": 75,\n    \"R32\": 76, \"R33\": 77, \"R34\": 78, \"R35\": 79, \"R36\": 80, \"R37\": 81,\n    \"R39\": 82, \"R40\": 83, \"R41\": 84, \"R42\": 85, \"R43\": 86,\n}\nid_map_df = pd.DataFrame(list(PATIENT_ID_MAP.items()), columns=[\"Code\", \"PatientID (Huawei Data)\"])\n\n# load workbook\nxls = pd.ExcelFile(MASTER_PATH)\nsheets = {s: pd.read_excel(MASTER_PATH, sheet_name=s) for s in xls.sheet_names}\n\n# update each sheet that has \"Code\"\nupdated_sheets = {}\nfor name, df in sheets.items():\n    if \"Code\" in df.columns:\n        df = df.merge(id_map_df, on=\"Code\", how=\"left\")\n        # optional: drop old Code col and just keep Huawei ID\n        # df = df.drop(columns=[\"Code\"])\n        updated_sheets[name] = df\n    else:\n        updated_sheets[name] = df\n\n# save to new Excel\nwith pd.ExcelWriter(OUT_PATH, engine=\"openpyxl\") as writer:\n    for name, df in updated_sheets.items():\n        df.to_excel(writer, index=False, sheet_name=name)\n\nprint(f\"saved ‚Üí {OUT_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T13:52:43.639321Z","iopub.execute_input":"2025-09-06T13:52:43.639676Z","iopub.status.idle":"2025-09-06T13:52:45.164197Z","shell.execute_reply.started":"2025-09-06T13:52:43.639648Z","shell.execute_reply":"2025-09-06T13:52:45.163327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. subperiods + Ramadan/Shawwal periods\n\nSets main periods (Ramadan/Shawwal) with your exact dates.\n\nDefines visit subperiods as inclusive date ranges:\n\nV1 = 2023-03-13 ‚Üí 2023-03-26\nV2 = 2023-03-27 ‚Üí 2023-04-02\nV3 = 2023-04-03 ‚Üí 2023-04-09\nV4 = 2023-04-10 ‚Üí 2023-04-19 (ends at Ramadan end)\nV5 = 2023-04-20 ‚Üí 2023-04-26\nV6 = 2023-04-27 ‚Üí 2023-05-08\nV7 = 2023-05-09 ‚Üí 2023-05-19 (ends at Shawwal end)\n\n\nAdd explicit Visit subperiods + Ramadan/Shawwal periods:\n- Annotates intraday rows with `period_main` and `visit_assigned`\n- Writes spec sheets into master workbook\n- Saves:\n  /kaggle/working/intraday_with_visits.csv\n  /kaggle/working/final_master_sheet_clean_with_visits.xlsx\n\n","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nAnnotate intraday with Visit subperiods + Ramadan/Shawwal periods\n- Adds `visit_assigned` (inclusive visit windows) and `period_main` (Ramadan/Shawwal/outside)\n- Saves annotated intraday to CSV and **single-sheet Excel (Intraday_All)**\n- Injects spec sheets into the master workbook and adds Visit_Anchor_Date to visit sheets\n- Also writes a dynamic missingness-by-visit CSV (optional QA artifact)\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ---------------- CONFIG ----------------\nINTRADAY_CSV_PATH = Path(\"/kaggle/input/intraday/intraday.csv\")\nMASTER_XLSX_IN    = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\nMASTER_XLSX_OUT   = Path(\"/kaggle/working/final_master_sheet_clean_with_visits.xlsx\")\n\nINTRADAY_OUT_CSV  = Path(\"/kaggle/working/intraday_with_visits.csv\")\nINTRADAY_OUT_XLSX = Path(\"/kaggle/working/intraday_with_visits.xlsx\")\n\nDYN_VISIT_OUT     = Path(\"/kaggle/working/missingness_dynamic_by_visit.csv\")  # optional QA\n\n# Main periods (inclusive)\nI_RAMADAN_START = pd.Timestamp(\"2023-03-22\")\nI_RAMADAN_END   = pd.Timestamp(\"2023-04-19\")\nI_SHAWWAL_START = pd.Timestamp(\"2023-04-20\")\nI_SHAWWAL_END   = pd.Timestamp(\"2023-05-19\")\n\n# Visit subperiods (ALL INCLUSIVE ‚Äî exactly as specified)\nVISIT_SUBPERIODS = [\n    {\"Visit\": \"Visit 1\", \"start\": \"2023-03-13\", \"end\": \"2023-03-21\"},\n    {\"Visit\": \"Visit 2\", \"start\": \"2023-03-22\", \"end\": \"2023-03-30\"},\n    {\"Visit\": \"Visit 3\", \"start\": \"2023-03-31\", \"end\": \"2023-04-06\"},\n    {\"Visit\": \"Visit 4\", \"start\": \"2023-04-08\", \"end\": \"2023-04-16\"},\n    {\"Visit\": \"Visit 5\", \"start\": \"2023-04-17\", \"end\": \"2023-04-26\"},\n    {\"Visit\": \"Visit 6\", \"start\": \"2023-04-27\", \"end\": \"2023-05-08\"},\n    {\"Visit\": \"Visit 7\", \"start\": \"2023-05-09\", \"end\": \"2023-05-19\"},\n]\n\n# Sheets in master we do not modify\nSKIP_SHEETS = {\"HMC_map_patientID\"}\n\n\n# ---------------- HELPERS ----------------\ndef _normalize_date_col(s: pd.Series) -> pd.Series:\n    return pd.to_datetime(s, errors=\"coerce\").dt.normalize()\n\ndef build_visit_spec_df(rows: list[dict]) -> pd.DataFrame:\n    \"\"\"\n    Build inclusive visit windows as a tidy DataFrame, warn (don't crash) on overlaps or bad spans.\n    \"\"\"\n    df = pd.DataFrame(rows)\n    df[\"start\"] = _normalize_date_col(df[\"start\"])\n    df[\"end\"]   = _normalize_date_col(df[\"end\"])\n    df = df.sort_values([\"start\", \"end\"]).reset_index(drop=True)\n\n    bad_span = df[df[\"end\"] < df[\"start\"]]\n    if not bad_span.empty:\n        print(\"‚ö†Ô∏è Visit with end < start:\\n\", bad_span)\n\n    df[\"prev_end\"] = df[\"end\"].shift(1)\n    overlap = df[(df.index > 0) & (df[\"start\"] <= df[\"prev_end\"])]\n    if not overlap.empty:\n        print(\"‚ö†Ô∏è Overlapping visits detected (inclusive ranges). Ensure previous 'end' < next 'start'.\")\n        print(overlap[[\"Visit\", \"start\", \"end\", \"prev_end\"]])\n\n    return df.drop(columns=[\"prev_end\"], errors=\"ignore\")\n\ndef tag_main_period(ts: pd.Timestamp) -> str | float:\n    if pd.isna(ts):\n        return np.nan\n    if I_RAMADAN_START <= ts <= I_RAMADAN_END:\n        return \"Ramadan (Mar 22‚ÄìApr 19, 2023)\"\n    if I_SHAWWAL_START <= ts <= I_SHAWWAL_END:\n        return \"Shawwal (Apr 20‚ÄìMay 19, 2023)\"\n    return \"Outside Ramadan/Shawwal\"\n\ndef assign_visit_inclusive(dates: pd.Series, visit_spec: pd.DataFrame) -> pd.Series:\n    \"\"\"\n    Inclusive mapping: a date belongs to Visit i if start_i <= date <= end_i.\n    Dates outside all windows remain NaN.\n    \"\"\"\n    visits_cat = pd.CategoricalDtype(categories=list(visit_spec[\"Visit\"]), ordered=True)\n    out = pd.Series(pd.Categorical([None] * len(dates), dtype=visits_cat), index=dates.index)\n    for _, r in visit_spec.iterrows():\n        mask = (dates >= r[\"start\"]) & (dates <= r[\"end\"])\n        out.loc[mask] = r[\"Visit\"]\n    return out\n\ndef _order_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Put key columns first for readability if they exist.\"\"\"\n    cols = list(df.columns)\n    front = [c for c in [\"date\", \"visit_assigned\", \"period_main\", \"patientID\", \"huaweiID\"] if c in cols]\n    rest  = [c for c in cols if c not in front]\n    return df[front + rest]\n\n\n# ---------------- 1) LOAD & ANNOTATE INTRADAY ----------------\nprint(\"Annotating intraday with visit subperiods & main periods‚Ä¶\")\nintraday = pd.read_csv(INTRADAY_CSV_PATH)\nif \"date\" not in intraday.columns:\n    raise ValueError(\"intraday.csv must have a 'date' column\")\n\nintraday[\"date\"] = _normalize_date_col(intraday[\"date\"])\n\nvisit_spec_df = build_visit_spec_df(VISIT_SUBPERIODS)\nintraday[\"period_main\"]    = intraday[\"date\"].apply(tag_main_period)\nintraday[\"visit_assigned\"] = assign_visit_inclusive(intraday[\"date\"], visit_spec_df)\n\n# Save CSV once\nintraday.to_csv(INTRADAY_OUT_CSV, index=False)\nprint(f\"‚úÖ Saved annotated intraday CSV ‚Üí {INTRADAY_OUT_CSV}\")\n\n# ---------------- 2) UPDATE MASTER WORKBOOK ----------------\nprint(\"Injecting Visit_Subperiods_Spec & Period_Bounds into master‚Ä¶\")\nxls = pd.ExcelFile(MASTER_XLSX_IN)\nsheets = {s: pd.read_excel(MASTER_XLSX_IN, sheet_name=s) for s in xls.sheet_names}\n\n# optional: add a Visit_Anchor_Date to each Visit sheet (start of its inclusive subperiod)\nANCHORS = {row[\"Visit\"]: row[\"start\"] for _, row in visit_spec_df.iterrows()}\n\nupdated = {}\nfor name, df in sheets.items():\n    if name in SKIP_SHEETS:\n        updated[name] = df\n        continue\n    if name in ANCHORS and \"Visit_Anchor_Date\" not in df.columns:\n        df = df.copy()\n        df[\"Visit_Anchor_Date\"] = ANCHORS[name].date()\n    updated[name] = df\n\nperiod_bounds = pd.DataFrame({\n    \"Period\":    [\"Ramadan\", \"Shawwal\"],\n    \"Start\":     [I_RAMADAN_START.date(), I_SHAWWAL_START.date()],\n    \"End\":       [I_RAMADAN_END.date(),   I_SHAWWAL_END.date()],\n    \"Inclusive\": [True, True],\n})\n\nwith pd.ExcelWriter(MASTER_XLSX_OUT, engine=\"openpyxl\") as writer:\n    for name, df in updated.items():\n        df.to_excel(writer, index=False, sheet_name=name[:31])  # safe truncate\n    visit_spec_df.to_excel(writer, index=False, sheet_name=\"Visit_Subperiods_Spec\")\n    period_bounds.to_excel(writer, index=False, sheet_name=\"Period_Bounds\")\n\nprint(f\"‚úÖ Saved master with specs ‚Üí {MASTER_XLSX_OUT}\")\n\n# ---------------- 3) OPTIONAL QA: MISSINGNESS BY VISIT (CSV) ----------------\nEXCLUDE = {\"Unnamed: 0\", \"huaweiID\", \"date\", \"start\", \"period_main\", \"visit_assigned\"}\ndfv = intraday.copy()\nfeature_cols = [c for c in dfv.columns if c not in EXCLUDE and c != \"patientID\"]\n\nif feature_cols:\n    out = (\n        dfv.groupby(\"visit_assigned\")[feature_cols]\n           .apply(lambda g: g.isna().mean().mul(100).round(2))\n           .reset_index()\n           .rename(columns={\"visit_assigned\": \"Visit\"})\n    )\n    out.melt(id_vars=[\"Visit\"], var_name=\"Feature\", value_name=\"% Missing (Visit Window)\") \\\n       .to_csv(DYN_VISIT_OUT, index=False)\n    print(f\"‚úÖ Saved dynamic-by-visit missingness CSV ‚Üí {DYN_VISIT_OUT}\")\nelse:\n    print(\"‚ÑπÔ∏è Skipped missingness export (no feature columns found).\")\n\n# ---------------- 4) SINGLE-SHEET EXCEL EXPORT ----------------\nprint(\"Writing single-sheet annotated intraday Excel‚Ä¶\")\nintraday_ordered = _order_columns(intraday)\n\nwith pd.ExcelWriter(INTRADAY_OUT_XLSX, engine=\"openpyxl\") as writer:\n    intraday_ordered.to_excel(writer, index=False, sheet_name=\"Intraday_All\")\n\nprint(f\"‚úÖ Saved single-sheet intraday Excel ‚Üí {INTRADAY_OUT_XLSX}\")\n\n# ---------------- 5) QUICK CHECKS (stdout) ----------------\nprint(\"\\n‚Äî Quick checks ‚Äî\")\ntry:\n    print(\"Date coverage:\", intraday[\"date\"].min(), \"‚Üí\", intraday[\"date\"].max(), \"| rows:\", len(intraday))\n    print(\"\\nVisit assignment counts:\")\n    print(intraday[\"visit_assigned\"].value_counts(dropna=False))\n    print(\"\\nMain period counts:\")\n    print(intraday[\"period_main\"].value_counts(dropna=False))\nexcept Exception as e:\n    print(\"Sanity checks skipped:\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T17:29:33.610930Z","iopub.execute_input":"2025-09-06T17:29:33.611207Z","iopub.status.idle":"2025-09-06T17:29:44.286791Z","shell.execute_reply.started":"2025-09-06T17:29:33.611183Z","shell.execute_reply":"2025-09-06T17:29:44.285599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Overview about the Static and Dynamic feature \n\nUnified Missingness Pipeline (Qatar edition, percent-aware)\n- Normalized name alignment (cleaned + aliases + percent-aware)\n- Separate Static vs Dynamic outputs\n- Ramadan/Shawwal rollups using visit groups\n- QC conditional formatting (>30 warn, >50 alert)\n- Patient-level missingness per visit\n- Robust dictionary merge (canonical-first, strict percent-safe fallback)\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport json\nimport re\nimport unicodedata\n\n# =============== CONFIG ===============\n# Inputs\nMASTER_PATH = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\nDICT_PATH   = Path(\"/kaggle/input/static-variables/Categorized_Data_Dictionary.xlsx\")  # xlsx or csv\nINTRADAY_CSV_PATH = Path(\"/kaggle/input/intraday/intraday.csv\")  # optional dynamic\n\n# Patient ID column in the master visit sheets (after your mapping step)\nPATIENT_ID_COL = \"PatientID (Huawei Data)\"\n\n# Which sheets to treat as visits (the script will read all except SKIP)\nSKIP_SHEETS = {\"HMC_map_patientID\"}\n\n# Visit groups for rollups (edit as needed)\nRAMADAN_VISITS = {\"Ramadan\", \"Visit 1\", \"Visit 2\", \"Visit 3\", \"Visit 4 (whole Ramadan)\"}\nSHAWWAL_VISITS = {\"Visit 6 (Shawal)\"}  # add Visit 5 here if it belongs to Shawwal\n\n# Intraday options\nINCLUDE_INTRADAY = True\nRAMADAN_START = pd.to_datetime(\"2023-03-23\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-21\")\nSHAWWAL_START = pd.to_datetime(\"2023-04-22\")\nSHAWWAL_END   = pd.to_datetime(\"2023-05-21\")\n# keep patientID to align; exclude only non-feature fields\nINTRADAY_EXCLUDE_COLS = {\"Unnamed: 0\", \"huaweiID\", \"date\", \"start\"}\n\n# Optional stage-2 CSV that (if present) we add as a sheet\nDYN_VISIT_OUT = Path(\"/kaggle/working/missingness_dynamic_by_visit.csv\")\n\n# Outputs\nOUT_XLSX = Path(\"/kaggle/working/missingness_report_full.xlsx\")\nOUT_CSV_STATIC  = Path(\"/kaggle/working/missingness_static.csv\")\nOUT_CSV_DYNAMIC = Path(\"/kaggle/working/missingness_dynamic.csv\")\nOUT_CSV_PATIENT = Path(\"/kaggle/working/missingness_patient_level.csv\")\n\n# QC thresholds (%)\nWARN_THRES = 30.0\nALERT_THRES = 50.0\n# =====================================\n\n\n# ===== Percent-aware normalization =====\ndef _strip_accents(s: str) -> str:\n    s = unicodedata.normalize(\"NFKD\", s)\n    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n\ndef clean_name_base(s: str) -> str:\n    \"\"\"Base alnum key (lower, accents removed, spaces collapsed, punctuation removed).\"\"\"\n    if s is None or (isinstance(s, float) and pd.isna(s)):\n        return \"\"\n    s = str(s).lower().strip()\n    s = _strip_accents(s)\n    s = re.sub(r\"\\s+\", \" \", s)\n    s = re.sub(r\"[^a-z0-9]\", \"\", s)\n    return s\n\ndef has_percent(s: str) -> bool:\n    \"\"\"Detect percent variants like 'HbA1c (%)', 'hba1c%', 'hba1c%%'.\"\"\"\n    if s is None or (isinstance(s, float) and pd.isna(s)):\n        return False\n    s0 = str(s).lower()\n    if \"%\" in s0:\n        return True\n    if \"(%)\" in s0:\n        return True\n    # If you want the word 'percent' to count:\n    # if re.search(r\"\\bpercent\\b\", s0): return True\n    return False\n\ndef canonical_key(raw_name: str) -> str:\n    \"\"\"Percent-safe canonical key: base or base+'_pct'.\"\"\"\n    base = clean_name_base(raw_name)\n    return f\"{base}_pct\" if has_percent(raw_name) else base\n\n# Canonical targets (extend as needed)\nCANON = {\n    \"meals_per_day\": \"meals_per_day\",\n    \"carbs_per_day\": \"carbs_per_day\",\n    \"active_insulin_time_hours\": \"active_insulin_time_hours\",\n    \"average_sg_mgdl\": \"average_sg_mgdl\",\n    \"icr_1\": \"icr_1\",\n    \"icr_2\": \"icr_2\",\n    \"sg_sd_mgdl\": \"sg_sd_mgdl\",\n    \"total_daily_dose_units\": \"total_daily_dose_units\",\n    # example duals for percent/non-percent:\n    \"hba1c\": \"hba1c\",\n    \"hba1c_pct\": \"hba1c_pct\",\n}\n\n# Aliases on canonical_key(raw) -> canonical name\nALIASES = {\n    # meals\n    canonical_key(\"meals\"): \"meals_per_day\",\n    canonical_key(\"meal\"): \"meals_per_day\",\n    canonical_key(\"meals/day\"): \"meals_per_day\",\n    canonical_key(\"meals per day\"): \"meals_per_day\",\n    canonical_key(\"number of meals\"): \"meals_per_day\",\n\n    # carbs\n    canonical_key(\"carb\"): \"carbs_per_day\",\n    canonical_key(\"carbs\"): \"carbs_per_day\",\n    canonical_key(\"carb/day\"): \"carbs_per_day\",\n    canonical_key(\"carbs/day\"): \"carbs_per_day\",\n    canonical_key(\"carbohydrates per day\"): \"carbs_per_day\",\n\n    # specifics\n    canonical_key(\"Active insulin time (hours)\"): \"active_insulin_time_hours\",\n    canonical_key(\"Active insulin time hours\"): \"active_insulin_time_hours\",\n    canonical_key(\"AIT (hours)\"): \"active_insulin_time_hours\",\n\n    canonical_key(\"Average SG mg/dL\"): \"average_sg_mgdl\",\n    canonical_key(\"Avg SG mg/dL\"): \"average_sg_mgdl\",\n    canonical_key(\"Average sensor glucose (mg/dL)\"): \"average_sg_mgdl\",\n\n    canonical_key(\"ICR-1\"): \"icr_1\",\n    canonical_key(\"ICR 1\"): \"icr_1\",\n    canonical_key(\"insulin carb ratio 1\"): \"icr_1\",\n\n    canonical_key(\"ICR-2\"): \"icr_2\",\n    canonical_key(\"ICR 2\"): \"icr_2\",\n    canonical_key(\"insulin carb ratio 2\"): \"icr_2\",\n\n    canonical_key(\"SG SD mg/dL\"): \"sg_sd_mgdl\",\n    canonical_key(\"sensor glucose sd (mg/dL)\"): \"sg_sd_mgdl\",\n\n    canonical_key(\"Total daily dose (Unit)\"): \"total_daily_dose_units\",\n    canonical_key(\"Total daily dose (Units)\"): \"total_daily_dose_units\",\n    canonical_key(\"TDD (units)\"): \"total_daily_dose_units\",\n    canonical_key(\"TDD\"): \"total_daily_dose_units\",\n\n    # HbA1c (non-percent vs percent ‚Äî DO NOT MERGE)\n    canonical_key(\"HbA1c\"): \"hba1c\",\n    canonical_key(\"HBA1C\"): \"hba1c\",\n    canonical_key(\"HbA1c (%)\"): \"hba1c_pct\",\n    canonical_key(\"hba1c%\"): \"hba1c_pct\",\n    canonical_key(\"hba1c%%\"): \"hba1c_pct\",\n}\n\ndef normalize_to_canonical(raw_name: str) -> str:\n    \"\"\"Map raw -> canonical (percent-safe).\"\"\"\n    key = canonical_key(raw_name)\n    return ALIASES.get(key, key)\n\n\n# =========================\n# ====== HELPERS ==========\n# =========================\ndef _clean_df_strings(df: pd.DataFrame) -> pd.DataFrame:\n    df2 = df.copy()\n    for col in df2.columns:\n        if df2[col].dtype == object:\n            s = df2[col].astype(str).str.strip()\n            s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"NA\": np.nan, \"None\": np.nan})\n            df2[col] = s\n    return df2\n\ndef percent_missing_by_column(df: pd.DataFrame, visit_name: str) -> pd.DataFrame:\n    df2 = _clean_df_strings(df)\n    miss = df2.isna().mean().mul(100).round(2)\n    out = miss.reset_index()\n    out.columns = [\"Feature\", f\"% Missing {visit_name}\"]\n    return out\n\ndef missing_counts(df: pd.DataFrame, visit_name: str) -> pd.DataFrame:\n    df2 = _clean_df_strings(df)\n    total = len(df2)\n    miss = df2.isna().sum()\n    return pd.DataFrame({\n        \"Feature\": miss.index,\n        f\"missing_{visit_name}\": miss.values,\n        f\"total_{visit_name}\": total\n    })\n\ndef availability_for_features(features, visit_dfs):\n    avail = []\n    for feat in features:\n        present_in = [vname for vname, vdf in visit_dfs.items() if feat in vdf.columns]\n        avail.append(present_in)\n    return pd.Series(avail, name=\"Availability (Visits)\")\n\ndef _pick_feature_col(cols):\n    for c in [\"Feature\", \"Feature Name\", \"Variable\", \"Field\", \"Name\"]:\n        if c in cols:\n            return c\n    return None\n\ndef load_dictionary(dict_path: Path) -> pd.DataFrame | None:\n    if not dict_path.exists():\n        print(f\"[INFO] Dictionary not found at {dict_path}. Skipping metadata merge.\")\n        return None\n\n    try:\n        if dict_path.suffix.lower() == \".csv\":\n            raw = pd.read_csv(dict_path)\n        else:\n            xls = pd.ExcelFile(dict_path)\n            selected = None\n            for s in xls.sheet_names:\n                tmp = pd.read_excel(dict_path, sheet_name=s)\n                fcol = _pick_feature_col(tmp.columns)\n                if fcol and \"Category\" in tmp.columns:\n                    selected = tmp.copy()\n                    break\n            if selected is None:\n                selected = pd.read_excel(dict_path)\n            raw = selected\n    except Exception as e:\n        print(f\"[WARN] Failed to read dictionary: {e}. Skipping metadata merge.\")\n        return None\n\n    fcol = _pick_feature_col(raw.columns)\n    if fcol is None:\n        print(\"[WARN] No recognizable feature-name column in dictionary. Skipping metadata merge.\")\n        return None\n\n    df = raw.rename(columns={fcol: \"Feature\"})\n    if \"Definition\" in df.columns and \"Definition & Unit\" not in df.columns:\n        df = df.rename(columns={\"Definition\": \"Definition & Unit\"})\n    keep_cols = [c for c in [\n        \"Feature\", \"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"\n    ] if c in df.columns]\n    if \"Feature\" not in keep_cols or \"Category\" not in keep_cols:\n        print(\"[WARN] Dictionary missing 'Feature' or 'Category' after normalization. Skipping metadata merge.\")\n        return None\n\n    df = df[keep_cols].drop_duplicates()\n    # percent-aware merge keys for the dictionary side\n    df[\"Feature_base\"]      = df[\"Feature\"].map(clean_name_base)\n    df[\"Feature_is_pct\"]    = df[\"Feature\"].map(has_percent)\n    df[\"Feature_canonical\"] = df[\"Feature\"].map(normalize_to_canonical)\n    return df\n\ndef weighted_overall_pct(merged_counts: pd.DataFrame) -> pd.Series:\n    missing_cols = [c for c in merged_counts.columns if c.startswith(\"missing_\")]\n    total_cols   = [c for c in merged_counts.columns if c.startswith(\"total_\")]\n    missing_total = merged_counts[missing_cols].sum(axis=1)\n    total_total   = merged_counts[total_cols].sum(axis=1)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        pct = np.where(total_total > 0, (missing_total / total_total) * 100, np.nan)\n    return np.round(pct, 2)\n\ndef qc_format_excel(writer, sheet_name, warn=WARN_THRES, alert=ALERT_THRES):\n    \"\"\"Add conditional formatting for % columns (>30 yellow, >50 red).\"\"\"\n    try:\n        from openpyxl.styles import PatternFill\n        from openpyxl.formatting.rule import CellIsRule\n        wb = writer.book\n        ws = wb[sheet_name]\n        header = [cell.value for cell in ws[1]]\n        pct_cols = [i for i, h in enumerate(header, start=1) if isinstance(h, str) and \"Missing\" in h]\n        yellow = PatternFill(start_color=\"FFF3CD\", end_color=\"FFF3CD\", fill_type=\"solid\")  # warn\n        red    = PatternFill(start_color=\"F8D7DA\", end_color=\"F8D7DA\", fill_type=\"solid\")  # alert\n        max_row = ws.max_row\n        for col in pct_cols:\n            col_letter = ws.cell(row=1, column=col).column_letter\n            rng = f\"{col_letter}2:{col_letter}{max_row}\"\n            ws.conditional_formatting.add(rng, CellIsRule(operator=\"greaterThan\", formula=[str(warn)], fill=yellow))\n            ws.conditional_formatting.add(rng, CellIsRule(operator=\"greaterThan\", formula=[str(alert)], fill=red))\n    except Exception as e:\n        print(f\"[INFO] QC formatting skipped on {sheet_name}: {e}\")\n\n\n# ------------- Main -------------\ndef main():\n    # Ensure output dir exists\n    OUT_XLSX.parent.mkdir(parents=True, exist_ok=True)\n\n    # Load visit sheets\n    xls = pd.ExcelFile(MASTER_PATH)\n    visit_sheet_names = [s for s in xls.sheet_names if s not in SKIP_SHEETS]\n    visits = {name: pd.read_excel(MASTER_PATH, sheet_name=name) for name in visit_sheet_names}\n\n    # ---- Static: per-visit missingness ----\n    miss_tables = [percent_missing_by_column(df, name) for name, df in visits.items()]\n    from functools import reduce\n    merged_miss = reduce(lambda L, R: pd.merge(L, R, on=\"Feature\", how=\"outer\"), miss_tables) if miss_tables else pd.DataFrame(columns=[\"Feature\"])\n\n    count_tables = [missing_counts(df, name) for name, df in visits.items()]\n    merged_counts = reduce(lambda L, R: pd.merge(L, R, on=\"Feature\", how=\"outer\"), count_tables) if count_tables else pd.DataFrame(columns=[\"Feature\"])\n    for col in merged_counts.columns:\n        if col.startswith(\"missing_\") or col.startswith(\"total_\"):\n            merged_counts[col] = merged_counts[col].fillna(0)\n\n    final_static = merged_miss.copy()\n    if not merged_counts.empty:\n        final_static[\"Overall % Missing\"] = weighted_overall_pct(merged_counts)\n    else:\n        final_static[\"Overall % Missing\"] = np.nan\n\n    final_static[\"Availability (Visits)\"] = availability_for_features(final_static[\"Feature\"], visits).apply(lambda x: json.dumps(x))\n\n    # ---- Merge dictionary (percent-aware) ----\n    dict_core = load_dictionary(DICT_PATH)\n    final_static[\"Feature_base\"]      = final_static[\"Feature\"].map(clean_name_base)\n    final_static[\"Feature_is_pct\"]    = final_static[\"Feature\"].map(has_percent)\n    final_static[\"Feature_canonical\"] = final_static[\"Feature\"].map(normalize_to_canonical)\n\n    if dict_core is not None:\n        # Pass 1: canonical join\n        merged = final_static.merge(\n            dict_core.add_suffix(\"_dict\"),\n            left_on=\"Feature_canonical\",\n            right_on=\"Feature_canonical_dict\",\n            how=\"left\"\n        )\n        # Pass 2 (strict): fallback by (base, is_pct) for unmatched rows\n        need_fb = merged[\"Category_dict\"].isna() if \"Category_dict\" in merged.columns else pd.Series(False, index=merged.index)\n        if need_fb.any():\n            fb_left = final_static.loc[need_fb, [\"Feature\", \"Feature_base\", \"Feature_is_pct\"]].copy()\n            fb_right = dict_core.add_suffix(\"_dict\")[\n                [\"Feature_dict\", \"Feature_base_dict\", \"Feature_is_pct_dict\",\n                 \"Category_dict\", \"Definition & Unit_dict\", \"Unit/Type_dict\", \"Feature Type_dict\", \"Subtype_dict\"]\n            ]\n            fb_joined = fb_left.merge(\n                fb_right,\n                left_on=[\"Feature_base\", \"Feature_is_pct\"],\n                right_on=[\"Feature_base_dict\", \"Feature_is_pct_dict\"],\n                how=\"left\"\n            )\n            idx = merged.index[need_fb]\n            for col in fb_joined.columns:\n                if col.endswith(\"_dict\") and col in merged.columns:\n                    merged.loc[idx, col] = fb_joined[col].values\n\n        # Prefer dictionary metadata where available\n        for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"]:\n            lc, rc = c, f\"{c}_dict\"\n            if lc not in merged.columns:\n                merged[lc] = np.nan\n            if rc in merged.columns:\n                merged[lc] = merged[lc].combine_first(merged[rc])\n\n        merged[\"Matched Dictionary Feature\"] = merged.get(\"Feature_dict\", np.nan)\n        # Cleanup helper cols\n        drop_helpers = [c for c in merged.columns if c.endswith(\"_dict\")] + \\\n                       [\"Feature_base\", \"Feature_base_dict\", \"Feature_is_pct\", \"Feature_is_pct_dict\",\n                        \"Feature_canonical\", \"Feature_canonical_dict\"]\n        final_static = merged.drop(columns=[c for c in drop_helpers if c in merged.columns])\n    else:\n        for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\", \"Matched Dictionary Feature\"]:\n            if c not in final_static.columns:\n                final_static[c] = np.nan\n\n    # Order columns\n    visit_cols = [c for c in final_static.columns if c.startswith(\"% Missing \")]\n    final_static = final_static[\n        [\"Category\", \"Feature\", \"Matched Dictionary Feature\", \"Definition & Unit\",\n         \"Unit/Type\", \"Feature Type\", \"Subtype\"]\n        + visit_cols\n        + [\"Overall % Missing\", \"Availability (Visits)\"]\n    ]\n\n    # ---- Ramadan/Shawwal rollups (static) ----\n    counts_map = {name: missing_counts(df, name).set_index(\"Feature\") for name, df in visits.items()}\n\n    def pooled_group_pct(features, group_visits):\n        if len(features) == 0:\n            return np.array([])\n        miss_total = pd.Series(0, index=features, dtype=float)\n        tot_total  = pd.Series(0, index=features, dtype=float)\n        for v in group_visits:\n            if v not in counts_map:\n                continue\n            mc = counts_map[v]\n            miss_total = miss_total.add(mc.get(f\"missing_{v}\", pd.Series(0, index=features)).reindex(features).fillna(0), fill_value=0)\n            tot_total  = tot_total.add(mc.get(f\"total_{v}\", pd.Series(0, index=features)).reindex(features).fillna(0),   fill_value=0)\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            pct = np.where(tot_total > 0, (miss_total / tot_total) * 100, np.nan)\n        return np.round(pct, 2)\n\n    features_list = final_static[\"Feature\"]\n    final_static[\"% Missing Ramadan (rollup)\"] = pooled_group_pct(features_list, RAMADAN_VISITS)\n    final_static[\"% Missing Shawwal (rollup)\"] = pooled_group_pct(features_list, SHAWWAL_VISITS)\n\n    # ---- Patient-level missingness per visit (row-wise) ----\n    plm_list = []\n    for vname, vdf in visits.items():\n        if PATIENT_ID_COL not in vdf.columns:\n            continue\n        df2 = _clean_df_strings(vdf)\n        cols_eval = [c for c in df2.columns if c != PATIENT_ID_COL]\n        if not cols_eval:\n            continue\n        row_pct = df2[cols_eval].isna().mean(axis=1).mul(100).round(2)\n        tmp = pd.DataFrame({\n            \"Visit\": vname,\n            PATIENT_ID_COL: df2[PATIENT_ID_COL],\n            \"Row % Missing (all fields)\": row_pct,\n            \"Missing Cells\": df2[cols_eval].isna().sum(axis=1),\n            \"Total Cells\": len(cols_eval)\n        })\n        plm_list.append(tmp)\n    patient_level_missing = pd.concat(plm_list, ignore_index=True) if plm_list else pd.DataFrame()\n\n    # ---- Dynamic (intraday) Ramadan/Shawwal ----\n    final_dynamic = pd.DataFrame()\n    if INCLUDE_INTRADAY and INTRADAY_CSV_PATH.exists():\n        try:\n            intraday = pd.read_csv(INTRADAY_CSV_PATH)\n            if \"date\" in intraday.columns:\n                intraday[\"date\"] = pd.to_datetime(intraday[\"date\"], errors=\"coerce\")\n                ram = intraday[(intraday[\"date\"] >= RAMADAN_START) & (intraday[\"date\"] <= RAMADAN_END)]\n                shw = intraday[(intraday[\"date\"] >= SHAWWAL_START) & (intraday[\"date\"] <= SHAWWAL_END)]\n\n                dyn_cols = [c for c in intraday.columns if c not in INTRADAY_EXCLUDE_COLS]\n\n                rows = []\n                for feat in dyn_cols:\n                    r_miss = ram[feat].isna().mean() * 100 if len(ram) else np.nan\n                    s_miss = shw[feat].isna().mean() * 100 if len(shw) else np.nan\n                    both = pd.concat([ram[feat], shw[feat]]) if len(ram) or len(shw) else pd.Series(dtype=float)\n                    overall = both.isna().mean() * 100 if len(both) else np.nan\n                    rows.append({\n                        \"Feature\": feat,\n                        \"% Missing Ramadan\": round(r_miss, 2) if pd.notna(r_miss) else np.nan,\n                        \"% Missing Shawwal\": round(s_miss, 2) if pd.notna(s_miss) else np.nan,\n                        \"Overall % Missing\": round(overall, 2) if pd.notna(overall) else np.nan,\n                        \"Availability (Visits)\": json.dumps([\"Ramadan (intraday)\", \"Shawwal (intraday)\"])\n                    })\n                final_dynamic = pd.DataFrame(rows)\n\n                # Dictionary enrichment (percent-aware)\n                dict_core_dyn = dict_core  # reuse if available\n                if dict_core_dyn is not None and not final_dynamic.empty:\n                    final_dynamic[\"Feature_base\"]      = final_dynamic[\"Feature\"].map(clean_name_base)\n                    final_dynamic[\"Feature_is_pct\"]    = final_dynamic[\"Feature\"].map(has_percent)\n                    final_dynamic[\"Feature_canonical\"] = final_dynamic[\"Feature\"].map(normalize_to_canonical)\n\n                    merged_dyn = final_dynamic.merge(\n                        dict_core_dyn.add_suffix(\"_dict\"),\n                        left_on=\"Feature_canonical\",\n                        right_on=\"Feature_canonical_dict\",\n                        how=\"left\"\n                    )\n                    # strict fallback on (base, is_pct)\n                    need_fb = merged_dyn[\"Category_dict\"].isna() if \"Category_dict\" in merged_dyn.columns else pd.Series(False, index=merged_dyn.index)\n                    if need_fb.any():\n                        fb_left = final_dynamic.loc[need_fb, [\"Feature\", \"Feature_base\", \"Feature_is_pct\"]].copy()\n                        fb_right = dict_core_dyn.add_suffix(\"_dict\")[\n                            [\"Feature_dict\", \"Feature_base_dict\", \"Feature_is_pct_dict\",\n                             \"Category_dict\", \"Definition & Unit_dict\", \"Unit/Type_dict\", \"Feature Type_dict\", \"Subtype_dict\"]\n                        ]\n                        fb_joined = fb_left.merge(\n                            fb_right,\n                            left_on=[\"Feature_base\", \"Feature_is_pct\"],\n                            right_on=[\"Feature_base_dict\", \"Feature_is_pct_dict\"],\n                            how=\"left\"\n                        )\n                        idx = merged_dyn.index[need_fb]\n                        for col in fb_joined.columns:\n                            if col.endswith(\"_dict\") and col in merged_dyn.columns:\n                                merged_dyn.loc[idx, col] = fb_joined[col].values\n\n                    for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"]:\n                        lc, rc = c, f\"{c}_dict\"\n                        if lc not in merged_dyn.columns:\n                            merged_dyn[lc] = np.nan\n                        if rc in merged_dyn.columns:\n                            merged_dyn[lc] = merged_dyn[lc].combine_first(merged_dyn[rc])\n\n                    merged_dyn[\"Matched Dictionary Feature\"] = merged_dyn.get(\"Feature_dict\", np.nan)\n                    drop_helpers = [c for c in merged_dyn.columns if c.endswith(\"_dict\")] + \\\n                                   [\"Feature_base\", \"Feature_base_dict\", \"Feature_is_pct\", \"Feature_is_pct_dict\",\n                                    \"Feature_canonical\", \"Feature_canonical_dict\"]\n                    final_dynamic = merged_dyn.drop(columns=[c for c in drop_helpers if c in merged_dyn.columns])\n                else:\n                    for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\", \"Matched Dictionary Feature\"]:\n                        if c not in final_dynamic.columns:\n                            final_dynamic[c] = np.nan\n            else:\n                print(\"[INFO] intraday has no 'date' column; dynamic missingness skipped.\")\n        except Exception as e:\n            print(f\"[WARN] Intraday integration skipped: {e}\")\n\n    # ---- Save everything to Excel with QC formatting ----\n    with pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\") as writer:\n        # Static (always create at least one sheet to avoid openpyxl 'no visible sheet' error)\n        final_static.to_excel(writer, index=False, sheet_name=\"Missingness_Static\")\n        qc_format_excel(writer, \"Missingness_Static\")\n\n        # Dynamic (Ramadan/Shawwal)\n        if not final_dynamic.empty:\n            dyn_cols_order = [\"Category\", \"Feature\", \"Matched Dictionary Feature\", \"Definition & Unit\",\n                              \"Unit/Type\", \"Feature Type\", \"Subtype\",\n                              \"% Missing Ramadan\", \"% Missing Shawwal\", \"Overall % Missing\", \"Availability (Visits)\"]\n            dyn_cols_order = [c for c in dyn_cols_order if c in final_dynamic.columns] + \\\n                             [c for c in final_dynamic.columns if c not in dyn_cols_order]\n            final_dynamic[dyn_cols_order].to_excel(writer, index=False, sheet_name=\"Missingness_Dynamic\")\n            qc_format_excel(writer, \"Missingness_Dynamic\")\n\n        # Dynamic-by-Visit (optional stage 2 CSV)\n        if DYN_VISIT_OUT.exists():\n            try:\n                dyn_visit_df = pd.read_csv(DYN_VISIT_OUT)\n                dyn_visit_df.to_excel(writer, index=False, sheet_name=\"Missingness_Dynamic_By_Visit\")\n                qc_format_excel(writer, \"Missingness_Dynamic_By_Visit\")\n                print(\"‚úÖ Added sheet: Missingness_Dynamic_By_Visit\")\n            except Exception as e:\n                print(f\"[INFO] Skipped Missingness_Dynamic_By_Visit: {e}\")\n\n        # Patient-level\n        if not patient_level_missing.empty:\n            patient_level_missing.to_excel(writer, index=False, sheet_name=\"Patient_Level_Missingness\")\n\n        # Unmapped audits\n        if \"Category\" in final_static.columns:\n            unmapped_static = (\n                final_static[final_static[\"Category\"].isna()][[\"Feature\"]]\n                .drop_duplicates().sort_values(\"Feature\")\n            )\n            unmapped_static.to_excel(writer, index=False, sheet_name=\"Unmapped_Static\")\n\n        if not final_dynamic.empty and \"Category\" in final_dynamic.columns:\n            unmapped_dyn = (\n                final_dynamic[final_dynamic[\"Category\"].isna()][[\"Feature\"]]\n                .drop_duplicates().sort_values(\"Feature\")\n            )\n            unmapped_dyn.to_excel(writer, index=False, sheet_name=\"Unmapped_Dynamic\")\n\n        # Name mapping audits\n        if \"Matched Dictionary Feature\" in final_static.columns:\n            mapping_static = (\n                final_static[[\"Feature\", \"Matched Dictionary Feature\"]]\n                .drop_duplicates().sort_values([\"Feature\", \"Matched Dictionary Feature\"])\n            )\n            mapping_static.to_excel(writer, index=False, sheet_name=\"Name_Mapping_Static\")\n\n        if not final_dynamic.empty and \"Matched Dictionary Feature\" in final_dynamic.columns:\n            mapping_dyn = (\n                final_dynamic[[\"Feature\", \"Matched Dictionary Feature\"]]\n                .drop_duplicates().sort_values([\"Feature\", \"Matched Dictionary Feature\"])\n            )\n            mapping_dyn.to_excel(writer, index=False, sheet_name=\"Name_Mapping_Dynamic\")\n\n    # CSVs\n    final_static.to_csv(OUT_CSV_STATIC, index=False)\n    if not final_dynamic.empty:\n        final_dynamic.to_csv(OUT_CSV_DYNAMIC, index=False)\n    if not patient_level_missing.empty:\n        patient_level_missing.to_csv(OUT_CSV_PATIENT, index=False)\n\n    print(f\"Saved Excel report ‚Üí {OUT_XLSX}\")\n    print(f\"Saved static CSV   ‚Üí {OUT_CSV_STATIC}\")\n    if not final_dynamic.empty:\n        print(f\"Saved dynamic CSV  ‚Üí {OUT_CSV_DYNAMIC}\")\n    if not patient_level_missing.empty:\n        print(f\"Saved patient CSV  ‚Üí {OUT_CSV_PATIENT}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T11:45:57.930451Z","iopub.execute_input":"2025-09-06T11:45:57.930746Z","iopub.status.idle":"2025-09-06T11:46:00.106022Z","shell.execute_reply.started":"2025-09-06T11:45:57.930727Z","shell.execute_reply":"2025-09-06T11:46:00.105096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. CGM with Dynamic Features \n\n1. Correlation heatmap ‚Üí checks pairwise correlations of features vs CGM.\n2. VIF (Variance Inflation Factor) ‚Üí detects collinearity (e.g., steps ‚Üî distance ‚Üî calories).\n3. Random Forest regression ‚Üí finds the most important predictors of CGM.\n4. Partial Dependence + ICE plots with smoothing ‚Üí visualizes how the top predictors (heart_rate, calories, steps, distance, spo2 in your example) influence CGM, both on average and at the individual sample level.\n      Blue = ICE curves (individual sample effects).\n      Orange = average PDP.\n      Red = smoothed PDP (to reduce jaggedness).\n6. PCA (95% variance) ‚Üí reduces 10 correlated features to the minimum number of orthogonal components that still explain ‚â•95% of the total variance.\n\n## 1. VIF (Multicollinearity check)\n\nBoth periods show moderate collinearity between steps, calories, and distance (VIF ~3‚Äì5). That‚Äôs expected ‚Äî people who walk more burn more calories and cover more distance.\n\nOther features (heart rate, sleep stages, SpO‚ÇÇ, awake, nap) are clean (VIF ~1‚Äì1.3).\n\nüëâ Interpretation: nothing is screaming \"drop me immediately\", but steps/calories/distance are partially redundant. If you want a leaner model, you could collapse them (e.g., PCA or just pick one).\n\n## 2. Random Forest Feature Importance\n\nHeart rate + calories + steps are the big three drivers for CGM in both Ramadan & Shawwal.\n\nDistance still matters, but less once steps/calories are in.\n\nSpO‚ÇÇ shows some relevance (~7‚Äì8%). Sleep features (deep, light, REM, nap, awake) barely move the needle (<3%).\n\nüëâ Interpretation: CGM is being explained more by physical activity + cardio strain than sleep metrics.\n\n## 4. Partial Dependence + ICE plots with smoothing \n\n‚Üí visualizes how the top predictors (heart_rate, calories, steps, distance, spo2 in your example) influence CGM, both on average and at the individual sample level.\n\nBlue = ICE curves (individual sample effects). \nOrange = average PDP. \nRed = smoothed PDP (to reduce jaggedness).\n\n## 5. PCA (Dimensionality reduction)\n\nRamadan: 10 features ‚Üí 3 principal components capture 98.5% variance.\nShawwal: 10 features ‚Üí 3 components capture 97.9% variance.\n\nüëâ Interpretation: data is very compressible. Three latent ‚Äúaxes‚Äù explain basically everything. Likely:\n   Activity/energy (steps + calories + distance).\n   Physiological (heart rate + SpO‚ÇÇ).\n   ![](http://)Sleep/rest cycle (deep, REM, light, awake, nap).\n\n## 6. Correlation differences\n\nYou mentioned a file \"Live_CGM_Correlation_Ramadan_vs_Shawwal.xlsx\" where you want to sort properly. Probably you want to line up features side-by-side and order them consistently (e.g., by Ramadan strength or absolute correlation).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# ---------------- CONFIG ----------------\nPATH = \"intraday_with_visits.xlsx\"  # ‚Üê Update path if needed\n\nFEATURES = [\n    \"calories\", \"deep\", \"distance\", \"heart_rate\",\n    \"light\", \"nap\", \"rem\", \"spo2\", \"steps\", \"awake\"\n]\n\nTARGET = \"cgm\"\nPERIOD_COL_CANDIDATES = [\"period_main\", \"period\", \"ramadan_shawwal\", \"season\", \"phase\"]\nTOP_K = 5\n\n# ---------------- HELPERS ----------------\ndef find_period_col(df, candidates):\n    for c in candidates:\n        if c in df.columns:\n            return c\n    return None\n\ndef normalize_period(s):\n    if pd.isna(s): return np.nan\n    x = str(s).strip().lower()\n    if \"ramad\" in x: return \"Ramadan\"\n    if \"shaw\" in x: return \"Shawwal\"\n    return np.nan\n\ndef to_numeric(df, cols):\n    return df[cols].apply(pd.to_numeric, errors='coerce')\n\ndef safe_heatmap(mat, title):\n    mask = mat.isna()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(mat, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\ndef compute_vif(X_df):\n    vif = []\n    for i in range(X_df.shape[1]):\n        try:\n            v = variance_inflation_factor(X_df.values, i)\n        except Exception:\n            v = np.nan\n        vif.append(v)\n    return pd.DataFrame({'Feature': X_df.columns, 'VIF': vif}).sort_values('VIF', ascending=False)\n\ndef ensure_min_samples(df, nmin=10):\n    if len(df) < nmin:\n        print(f\"‚ö†Ô∏è Not enough samples: {len(df)} < {nmin}\")\n        return False\n    return True\n\n# ---------------- LOAD DATA ----------------\ndf = pd.read_excel(PATH)\ndf.columns = df.columns.str.lower().str.replace(\" \", \"_\")\nperiod_col = find_period_col(df, PERIOD_COL_CANDIDATES)\n\nif not period_col:\n    raise ValueError(\"No valid period column found.\")\n\ndf[period_col] = df[period_col].apply(normalize_period)\n\ndf_r = to_numeric(df[df[period_col] == \"Ramadan\"].copy(), FEATURES + [TARGET]).dropna(subset=[TARGET])\ndf_s = to_numeric(df[df[period_col] == \"Shawwal\"].copy(), FEATURES + [TARGET]).dropna(subset=[TARGET])\n\n# ---------------- ANALYSIS FUNCTION ----------------\ndef analyze_period(df_period, label):\n    print(f\"\\n=== {label} Analysis ===\")\n    if not ensure_min_samples(df_period): \n        return None\n\n    # --- Correlation ---\n    corr = df_period[FEATURES + [TARGET]].corr()\n    safe_heatmap(corr, f\"{label} ‚Äì Feature Correlation\")\n\n    # --- VIF ---\n    X = pd.DataFrame(\n        SimpleImputer(strategy=\"median\").fit_transform(df_period[FEATURES]),\n        columns=FEATURES\n    )\n    vif_df = compute_vif(X)\n    print(\"\\nVIF:\")\n    print(vif_df)\n\n    # --- Random Forest ---\n    y = df_period[TARGET].values\n    rf = RandomForestRegressor(n_estimators=300, random_state=42)\n    rf.fit(X, y)\n    importances = pd.DataFrame({\n        \"Feature\": FEATURES,\n        \"Importance (%)\": 100 * rf.feature_importances_ / rf.feature_importances_.sum()\n    }).sort_values(\"Importance (%)\", ascending=False)\n    print(\"\\nRandom Forest Importances:\")\n    print(importances)\n\n    # --- PDP + ICE with smoothing ---\n    from sklearn.inspection import partial_dependence\n    top_feats = importances[\"Feature\"].head(TOP_K).tolist()\n    valid_feats = [f for f in top_feats if X[f].nunique() > 2]\n\n    if valid_feats:\n        try:\n            fig, ax = plt.subplots(1, len(valid_feats), figsize=(5 * len(valid_feats), 4))\n            if len(valid_feats) == 1:\n                ax = [ax]\n\n            for i, feat in enumerate(valid_feats):\n                disp = PartialDependenceDisplay.from_estimator(\n                    rf, X, [feat],\n                    kind=\"both\", subsample=200,\n                    random_state=42, ax=ax[i],\n                    percentiles=(0.01, 0.99)\n                )\n\n                # Smoothing\n                pd_results = partial_dependence(\n                    rf, X, [feat], kind=\"average\", percentiles=(0.01, 0.99)\n                )\n                xx = pd_results['values'][0]\n                yy = pd_results['average'][0]\n                smooth_y = pd.Series(yy).rolling(window=10, min_periods=1, center=True).mean()\n\n                ax[i].plot(xx, smooth_y, color=\"red\", linewidth=2, label=\"Smoothed PDP\")\n                ax[i].legend()\n\n            plt.suptitle(f\"{label} ‚Äì PDP + ICE (Smoothed)\")\n            plt.tight_layout()\n            plt.show()\n\n        except Exception as e:\n            print(f\"PDP failed even after filtering: {e}\")\n    else:\n        print(f\"‚ö†Ô∏è No valid features for PDP in {label}\")\n\n    # --- PCA ---\n    pca = PCA(n_components=0.95, svd_solver='full')\n    pca.fit(X)\n\n\n    print(f\"\\nPCA ‚Äì 95% Variance (Raw Features): {X.shape[1]} ‚Üí {pca.n_components_}\")\n    print(\"Cumulative Variance:\", np.round(np.cumsum(pca.explained_variance_ratio_), 4))\n\n    # PCA loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,\n        index=FEATURES,\n        columns=[f\"PC{i+1}\" for i in range(pca.n_components_)]\n    )\n    print(\"\\nPCA Loadings (feature contributions per component):\")\n    print(loadings.round(3))\n\n \n    # --- Plot PCA loadings as heatmap ---\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(loadings, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n    plt.title(f\"{label} ‚Äì PCA Loadings Heatmap\")\n    plt.xlabel(\"Principal Components\")\n    plt.ylabel(\"Features\")\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        \"corr\": corr,\n        \"vif\": vif_df,\n        \"rf_importance\": importances,\n        \"pca\": pca,\n        \"pca_loadings\": loadings\n    }\n\n\n\n# ---------------- RUN ANALYSIS ----------------\nres_r = analyze_period(df_r, \"Ramadan\")\nres_s = analyze_period(df_s, \"Shawwal\")\n\n\n\n# ---------------- OPTIONAL: EXPORT OR PLOT DIFFERENCE ----------------\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Extract correlation of each feature with CGM from the results\ncorr_r = res_r[\"corr\"][\"cgm\"].drop(\"cgm\")   # Ramadan correlations\ncorr_s = res_s[\"corr\"][\"cgm\"].drop(\"cgm\")   # Shawwal correlations\n\n# Build dataframe\ndf_corr = pd.DataFrame({\n    \"Ramadan\": corr_r,\n    \"Shawwal\": corr_s\n})\n\n# Use absolute values so all bars are positive\ndf_corr = df_corr.abs()\n\n# --- Plot Ramadan only ---\ndf_corr[\"Ramadan\"].sort_values().plot(\n    kind=\"barh\", color=\"firebrick\", figsize=(8,6)\n)\nplt.title(\"Correlation with CGM ‚Äì Ramadan\")\nplt.xlabel(\"Absolute Pearson Correlation\")\nplt.tight_layout()\nplt.show()\n\n# --- Plot Shawwal only ---\ndf_corr[\"Shawwal\"].sort_values().plot(\n    kind=\"barh\", color=\"royalblue\", figsize=(8,6)\n)\nplt.title(\"Correlation with CGM ‚Äì Shawwal\")\nplt.xlabel(\"Absolute Pearson Correlation\")\nplt.tight_layout()\nplt.show()\n\n# --- Combined Comparison ---\ndf_corr.sort_values(\"Ramadan\", ascending=True)[[\"Ramadan\", \"Shawwal\"]].plot(\n    kind=\"barh\",\n    color=[\"firebrick\", \"royalblue\"],\n    figsize=(10,6)\n)\nplt.title(\"Correlation with CGM ‚Äì Ramadan vs. Shawwal\")\nplt.xlabel(\"Absolute Pearson Correlation\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T21:05:58.591388Z","iopub.execute_input":"2025-10-21T21:05:58.591610Z","iopub.status.idle":"2025-10-21T21:06:03.971898Z","shell.execute_reply.started":"2025-10-21T21:05:58.591587Z","shell.execute_reply":"2025-10-21T21:06:03.970276Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1079797553.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# ---------------- LOAD DATA ----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mperiod_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_period_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERIOD_COL_CANDIDATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'intraday_with_visits.xlsx'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'intraday_with_visits.xlsx'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# A: Correlation heatmap (already saved as .png)\nimg1 = mpimg.imread(\"Correlation with CGM ‚Äì Ramadanl.png\")\naxes[0].imshow(img1)\naxes[0].axis(\"off\")\naxes[0].set_title(\"(A) Feature Correlation\")\n\n# B: VIF bar chart\nvif = {\"steps\":4.99,\"distance\":2.99,\"calories\":2.70,\"heart_rate\":1.26,\"spo2\":1.17,\"light\":1.07,\n       \"rem\":1.04,\"deep\":1.04,\"nap\":1.01,\"awake\":1.01}\naxes[1].barh(list(vif.keys()), list(vif.values()), color=\"skyblue\")\naxes[1].axvline(5, color=\"r\", linestyle=\"--\", label=\"threshold\")\naxes[1].invert_yaxis()\naxes[1].set_title(\"(B) VIF < 5 ‚Äì Acceptable collinearity\")\naxes[1].set_xlabel(\"VIF\")\n\n# C: Random Forest importance\nrf_imp = {\"heart_rate\":29.35,\"calories\":26.78,\"steps\":17.13,\"distance\":11.03,\"spo2\":7.68,\n          \"light\":2.61,\"rem\":2.10,\"deep\":1.26,\"awake\":1.22,\"nap\":0.84}\naxes[2].barh(list(rf_imp.keys()), list(rf_imp.values()), color=\"orange\")\naxes[2].invert_yaxis()\naxes[2].set_title(\"(C) Random Forest Importance\")\naxes[2].set_xlabel(\"Importance (%)\")\n\nplt.tight_layout()\nplt.savefig(\"Feature_Stability_Importance_Figure_Sx.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PCA Components Conceptual Diagram ","metadata":{}},{"cell_type":"code","source":"# ================= PCA Components Conceptual Diagram ==================\nimport matplotlib.pyplot as plt\n\n# Define conceptual components\npoints = {\n    \"PC1: Activity / Energy\\n(steps, distance, calories)\": (0, 1),\n    \"PC2: Physiology\\n(heart_rate, SpO‚ÇÇ)\": (-0.87, -0.5),\n    \"PC3: Sleep / Rest Pattern\\n(deep, light, REM, nap, awake)\": (0.87, -0.5)\n}\n\n# Create plot\nfig, ax = plt.subplots(figsize=(6,6))\n\n# Plot points and labels\nfor (label, (x,y)) in points.items():\n    ax.scatter(x, y, s=500, marker=\"o\", color=\"skyblue\", edgecolors=\"k\", zorder=3)\n    ax.text(x, y+0.1, label, ha=\"center\", va=\"bottom\", fontsize=10, weight=\"bold\")\n\n# Connect points to form a triangle\ncoords = list(points.values())\nfor i in range(len(coords)):\n    x1, y1 = coords[i]\n    x2, y2 = coords[(i+1) % len(coords)]\n    ax.plot([x1, x2], [y1, y2], \"k-\", lw=1.5)\n\n# Formatting\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1, 1.2)\nax.axis(\"off\")\nax.set_title(\"PCA Components from CGM Features\", fontsize=14, weight=\"bold\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:42:10.724459Z","iopub.execute_input":"2025-09-13T13:42:10.724824Z","iopub.status.idle":"2025-09-13T13:42:10.995193Z","shell.execute_reply.started":"2025-09-13T13:42:10.724796Z","shell.execute_reply":"2025-09-13T13:42:10.994201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Great‚Äîsteps 1‚Äì4 are done and you now have:\n\nfinal_master_sheet_clean_with_visits.xlsx\n\nintraday_with_visits.csv\n\nThe next milestone is to materialize PATH_HOURLY (the hourly ML-ready dataset) and also produce the two helper tables PATH_VISIT (per‚Äëvisit features) and PATH_BASE (static baseline).\n\nKaggle note: you cannot write to /kaggle/input. Write to /kaggle/working and, if needed, create a dataset from those files afterward.\n\nBelow are drop‚Äëin cells you can paste into your Kaggle notebook. They will:\n\nBuild hourly CGM features (+ context) from intraday_with_visits.csv.\n\nExtract per‚Äëvisit features (carb, meals, TDD, fasting%) from the master workbook.\n\nExtract static baseline variables (Age, Gender, BMI, HbA1C, Cholesterol, LDL, HDL, Triglycerides, eGFR, Creatinine, Insulin_units_per_kg, SmartGuard_percent).\n\nMerge them (patientID & visit‚Äëaware) into a single hourly ML table.\n\nSave the three CSVs under /kaggle/working/‚Ä¶ and expose feature lists you‚Äôll pass to the XGB/BiLSTM cells.\n\nLeakage‚Äësafe PCA: we do not compute pca_cgm1‚Äì3 or lifestyle PCs here. You‚Äôll compute those inside the modeling pipeline on the training fold only, as we set up earlier. The hourly CSV will carry the raw columns (CGM dynamics + activity/sleep/HR) that PCA will be derived from.","metadata":{}},{"cell_type":"markdown","source":"# Cell A ‚Äî Paths & setup","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\n\n# ----- Inputs produced in your earlier steps -----\nINTRADAY_ANN = Path(\"/kaggle/working/intraday_with_visits.csv\")\nMASTER_XLSX  = Path(\"/kaggle/working/final_master_sheet_clean_with_visits.xlsx\")\n\n# ----- Outputs (write to /kaggle/working) -----\nPATH_HOURLY = Path(\"/kaggle/working/hourly_features_ml.csv\")\nPATH_VISIT  = Path(\"/kaggle/working/per_visit_features.csv\")\nPATH_BASE   = Path(\"/kaggle/working/static_baseline.csv\")\n\nprint(\"Reading:\", INTRADAY_ANN.exists(), MASTER_XLSX.exists())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:16:00.241063Z","iopub.execute_input":"2025-10-17T16:16:00.241256Z","iopub.status.idle":"2025-10-17T16:16:01.831537Z","shell.execute_reply.started":"2025-10-17T16:16:00.241236Z","shell.execute_reply":"2025-10-17T16:16:01.830847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell B ‚Äî Build hourly CGM features (+ context)","metadata":{}},{"cell_type":"code","source":"# Robust loader\nintraday = pd.read_csv(INTRADAY_ANN)\ncols = {c.lower(): c for c in intraday.columns}\n\n# unify ID\npid_col = None\nfor k in [\"patientid\", \"patient_id\", \"patientid_(huawei_data)\", \"patientid_(huawei data)\", \"huaweiid\"]:\n    if k in cols:\n        pid_col = cols[k]; break\nif pid_col is None:\n    raise ValueError(\"No patient ID column found in intraday_with_visits.csv\")\n\n# unify time ‚Üí 'start' and 'hour'\ntime_col = None\nfor k in [\"start\", \"datetime\", \"timestamp\", \"time\"]:\n    if k in cols:\n        time_col = cols[k]; break\nif time_col is None and \"date\" in cols:\n    # if only 'date' present, treat it as midnight timestamps and warn\n    intraday[\"start\"] = pd.to_datetime(intraday[cols[\"date\"]], errors=\"coerce\")\nelse:\n    intraday[\"start\"] = pd.to_datetime(intraday[time_col], errors=\"coerce\")\n\nintraday[\"hour\"] = intraday[\"start\"].dt.floor(\"h\")\n\n# unify CGM column (sensor glucose)\ncgm_col = None\nfor k in [\"cgm\", \"sg\", \"sensor_glucose\", \"glucose\"]:\n    if k in cols:\n        cgm_col = cols[k]; break\nif cgm_col is None:\n    raise ValueError(\"No CGM column found in intraday_with_visits.csv (expected one of: cgm, sg, sensor_glucose, glucose).\")\n\n# Optional lifestyle columns at intraday grain (aggregate later if present)\nlifestyle_candidates = [\n    \"steps\",\"calories\",\"distance\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\npresent_dyn = [cols[k] for k in lifestyle_candidates if k in cols]\n\n# Keep period/visit context if present\nvisit_col  = cols.get(\"visit_assigned\", None)\nperiod_col = cols.get(\"period_main\", None)\n\nkeep_cols = [pid_col, \"hour\", cgm_col] + [c for c in [visit_col, period_col] if c] + present_dyn\ndf = intraday[keep_cols].copy()\n\n# --- hourly CGM stats per patient-hour\ng = df.groupby([pid_col, \"hour\"], as_index=False)\n\nhourly_cgm = g.agg(\n    cgm_mean=(cgm_col, \"mean\"),\n    cgm_min =(cgm_col, \"min\"),\n    cgm_max =(cgm_col, \"max\"),\n    cgm_std =(cgm_col, \"std\")\n)\n\nhourly_cgm[\"cgm_mean_plus_std\"]  = hourly_cgm[\"cgm_mean\"] + hourly_cgm[\"cgm_std\"].fillna(0)\nhourly_cgm[\"cgm_mean_minus_std\"] = hourly_cgm[\"cgm_mean\"] - hourly_cgm[\"cgm_std\"].fillna(0)\n\n# Label: hypoglycemia within this hour (detection). (Forecast label is created later in modeling if you choose.)\nhypo_hour = (df[cgm_col] < 70).groupby([df[pid_col], df[\"hour\"]]).any().reset_index()\nhypo_hour.columns = [pid_col, \"hour\", \"hypo_label\"]\nhourly_cgm = hourly_cgm.merge(hypo_hour, on=[pid_col, \"hour\"], how=\"left\")\nhourly_cgm[\"hypo_label\"] = hourly_cgm[\"hypo_label\"].fillna(False).astype(int)\n\n# Re-attach visit/period context (take the most frequent within the hour if needed)\ndef mode_or_first(s):\n    try:\n        return s.mode(dropna=True).iloc[0]\n    except Exception:\n        return s.dropna().iloc[0] if s.notna().any() else np.nan\n\nctx = df.groupby([pid_col, \"hour\"]).agg(\n    visit_assigned=(visit_col, mode_or_first) if visit_col else (cgm_col, \"size\"),\n    period_main=(period_col, mode_or_first) if period_col else (cgm_col, \"size\")\n).reset_index()\n\nif visit_col is None:\n    ctx = ctx.drop(columns=[\"visit_assigned\"])\nif period_col is None:\n    ctx = ctx.drop(columns=[\"period_main\"])\n\nhourly = hourly_cgm.merge(ctx, on=[pid_col, \"hour\"], how=\"left\")\n\n# hour-of-day (0‚Äì23)\nhourly[\"hour_of_day\"] = pd.to_datetime(hourly[\"hour\"]).dt.hour\n\n# rename patientID column consistently\nhourly = hourly.rename(columns={pid_col: \"patientID\"})\n\nprint(\"Hourly CGM rows:\", len(hourly), \"| patients:\", hourly[\"patientID\"].nunique())\nhourly.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell C ‚Äî Aggregate lifestyle to hourly (if present)","metadata":{}},{"cell_type":"code","source":"# For activity-like streams, use sums per hour; for vitals, use means.\nSUM_COLS  = [c for c in [\"steps\",\"calories\",\"distance\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"] if c in cols]\nMEAN_COLS = [c for c in [\"heart_rate\",\"spo2\"] if c in cols]\n\nagg_parts = []\nif SUM_COLS:\n    agg_sum = df.groupby([pid_col,\"hour\"])[[cols[c] for c in SUM_COLS]].sum().reset_index()\n    # restore canonical names\n    agg_sum = agg_sum.rename(columns={cols[c]: c for c in SUM_COLS})\n    agg_parts.append(agg_sum)\n\nif MEAN_COLS:\n    agg_mean = df.groupby([pid_col,\"hour\"])[[cols[c] for c in MEAN_COLS]].mean().reset_index()\n    agg_mean = agg_mean.rename(columns={cols[c]: c for c in MEAN_COLS})\n    agg_parts.append(agg_mean)\n\nif agg_parts:\n    agg_all = agg_parts[0]\n    for k in agg_parts[1:]:\n        agg_all = agg_all.merge(k, on=[pid_col, \"hour\"], how=\"outer\")\n    agg_all = agg_all.rename(columns={pid_col: \"patientID\"})\n    hourly = hourly.merge(agg_all, on=[\"patientID\",\"hour\"], how=\"left\")\n\nprint(\"Added lifestyle columns:\", [c for c in [\"steps\",\"calories\",\"distance\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\",\"heart_rate\",\"spo2\"] if c in hourly.columns])\nhourly.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell D ‚Äî Build per‚Äëvisit table (PATH_VISIT)","metadata":{}},{"cell_type":"code","source":"import re\nfrom pandas import ExcelFile\n\ndef normalize_visit_name(x: str) -> str:\n    if not isinstance(x, str): return x\n    t = x.strip()\n    m = re.search(r\"visit\\s*(\\d+)\", t, flags=re.I)\n    if m: return f\"Visit {int(m.group(1))}\"\n    if \"ramadan\" in t.lower(): return \"Ramadan\"\n    if \"shaw\" in t.lower():    return \"Shawwal\"\n    return t\n\n# Columns we want (robust fuzzy find)\nwant_keys = {\n    \"carb\": r\"carb\",                # carb(s)\n    \"meals\": r\"meal\",               # meals\n    \"tdd_u\": r\"(total\\s*daily\\s*dose|tdd)\",  # total daily dose units\n    \"fasting_pct_29\": r\"fasting.*(29|%)\"     # fasting % (out of 29 days)\n}\n\nvis_rows = []\nxls = ExcelFile(MASTER_XLSX)\nfor sheet in xls.sheet_names:\n    if sheet in {\"HMC_map_patientID\"}: \n        continue\n    dfv = pd.read_excel(MASTER_XLSX, sheet_name=sheet)\n    # locate patient id\n    pidc = None\n    for cand in [\"PatientID (Huawei Data)\",\"patientID\",\"PatientID\",\"huaweiID\"]:\n        if cand in dfv.columns: pidc = cand; break\n    if pidc is None: \n        continue\n    # pick columns if present\n    sel = { \"Visit\": normalize_visit_name(sheet), \"SourceSheet\": sheet }\n    ok = False\n    for key, pat in want_keys.items():\n        hit = [c for c in dfv.columns if re.search(pat, str(c), flags=re.I)]\n        if hit:\n            sel[key] = dfv[hit[0]]\n            ok = True\n        else:\n            sel[key] = np.nan\n    if ok:\n        tmp = pd.DataFrame({\n            \"patientID\": dfv[pidc].values,\n            \"visit\": sel[\"Visit\"],\n            \"carb\": sel[\"carb\"],\n            \"meals\": sel[\"meals\"],\n            \"total_daily_dose_u\": sel[\"tdd_u\"],\n            \"fasting_pct_29\": sel[\"fasting_pct_29\"]\n        })\n        vis_rows.append(tmp)\n\nvisit_tbl = pd.concat(vis_rows, ignore_index=True) if vis_rows else pd.DataFrame(\n    columns=[\"patientID\",\"visit\",\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_pct_29\"]\n)\n\n# Clean types\nfor c in [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_pct_29\"]:\n    if c in visit_tbl.columns:\n        visit_tbl[c] = pd.to_numeric(visit_tbl[c], errors=\"coerce\")\n\n# Save (and also keep to merge later)\nvisit_tbl.to_csv(PATH_VISIT, index=False)\nprint(f\"Saved per-visit features ‚Üí {PATH_VISIT} | rows:\", len(visit_tbl), \"| visits:\", visit_tbl[\"visit\"].nunique())\nvisit_tbl.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell E ‚Äî Build static baseline table (PATH_BASE)","metadata":{}},{"cell_type":"code","source":"# We‚Äôll scan all sheets and pick the best-available columns per patient.\nSTATIC_WANT = {\n    \"Age\": [r\"^age$\"],\n    \"Gender\": [r\"^gender$\", r\"^sex$\"],\n    \"BMI\": [r\"^bmi$\"],\n    \"HbA1C\": [r\"^hba1c\\b(?!.*%)\", r\"^a1c$\"],\n    \"Cholesterol\": [r\"chol(?!\\w)\"],\n    \"LDL\": [r\"^ldl$\"],\n    \"HDL\": [r\"^hdl$\"],\n    \"Triglycerides\": [r\"^trig(lycerides)?$\",\"^tg$\"],\n    \"eGFR\": [r\"^egfr$\"],\n    \"Creatinine\": [r\"creat\"],\n    \"Insulin_units_per_kg\": [r\"units.*kg\", r\"tdd.*kg\", r\"^iu/kg$\"],\n    \"SmartGuard_percent\": [r\"smart.*guard\", r\"sg.*%\"]\n}\n\ndef first_match(df, pats):\n    for p in pats:\n        hit = [c for c in df.columns if re.search(p, str(c), flags=re.I)]\n        if hit: return hit[0]\n    return None\n\nstat_rows = []\nfor sheet in xls.sheet_names:\n    if sheet in {\"HMC_map_patientID\"}:\n        continue\n    d = pd.read_excel(MASTER_XLSX, sheet_name=sheet)\n    pidc = None\n    for cand in [\"PatientID (Huawei Data)\",\"patientID\",\"PatientID\",\"huaweiID\"]:\n        if cand in d.columns: pidc = cand; break\n    if pidc is None: \n        continue\n    rec = {\"patientID\": d[pidc]}\n    found_any = False\n    for out_name, patterns in STATIC_WANT.items():\n        col = first_match(d, patterns)\n        if col is not None:\n            rec[out_name] = pd.to_numeric(d[col], errors=\"coerce\") if d[col].dtype != object else d[col]\n            found_any = True\n    if found_any:\n        stat_rows.append(pd.DataFrame(rec))\n\nstatic_tbl = (pd.concat(stat_rows, ignore_index=True)\n              if stat_rows else pd.DataFrame({\"patientID\": []}))\n\n# collapse duplicates by patient (prefer last non-null)\nstatic_tbl = static_tbl.sort_values(\"patientID\").groupby(\"patientID\", as_index=False).agg(lambda s: s.dropna().iloc[-1] if s.notna().any() else np.nan)\n\n# Optional: derive Insulin_units_per_kg if missing and TDD/weight available (commented here; enable if you have columns)\n# if \"Insulin_units_per_kg\" not in static_tbl.columns and {\"total_daily_dose_u\",\"Weight_kg\"}.issubset(set(static_tbl.columns)):\n#     static_tbl[\"Insulin_units_per_kg\"] = static_tbl[\"total_daily_dose_u\"] / static_tbl[\"Weight_kg\"]\n\n# Save\nstatic_tbl.to_csv(PATH_BASE, index=False)\nprint(f\"Saved static baseline ‚Üí {PATH_BASE} | patients:\", static_tbl[\"patientID\"].nunique())\nstatic_tbl.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell F ‚Äî Merge everything ‚Üí PATH_HOURLY","metadata":{}},{"cell_type":"code","source":"# Left join hourly ‚Üê per-visit by (patientID, visit_assigned)\nhourly_merged = hourly.copy()\nif \"visit_assigned\" in hourly_merged.columns:\n    # normalize visit name in visit_tbl\n    visit_tbl_norm = visit_tbl.copy()\n    visit_tbl_norm[\"visit\"] = visit_tbl_norm[\"visit\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n    hourly_merged[\"visit_assigned\"] = hourly_merged[\"visit_assigned\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n    hourly_merged = hourly_merged.merge(\n        visit_tbl_norm.rename(columns={\"visit\":\"visit_assigned\"}),\n        on=[\"patientID\",\"visit_assigned\"], how=\"left\"\n    )\nelse:\n    print(\"‚ö†Ô∏è No visit_assigned in hourly ‚Äî skipping per-visit merge.\")\n\n# Left join hourly ‚Üê static baseline by patientID\nhourly_merged = hourly_merged.merge(static_tbl, on=\"patientID\", how=\"left\")\n\n# Final ordering\nfront = [\"patientID\",\"hour\",\"hour_of_day\"]\nctx   = [c for c in [\"visit_assigned\",\"period_main\"] if c in hourly_merged.columns]\ntarget= [\"hypo_label\"]\ncgm   = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\",\"cgm_mean_plus_std\",\"cgm_mean_minus_std\"]\nlifestyle = [c for c in [\"steps\",\"calories\",\"distance\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\",\"heart_rate\",\"spo2\"] if c in hourly_merged.columns]\npervisit  = [c for c in [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_pct_29\"] if c in hourly_merged.columns]\nstatic    = [c for c in [\"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"] if c in hourly_merged.columns]\n\ncol_order = front + ctx + target + cgm + lifestyle + pervisit + static\ncol_order += [c for c in hourly_merged.columns if c not in col_order]  # keep any extras at end\n\nhourly_merged = hourly_merged[col_order]\nhourly_merged.to_csv(PATH_HOURLY, index=False)\n\nprint(f\"‚úÖ Saved hourly ML table ‚Üí {PATH_HOURLY}\")\nprint(\"Rows:\", len(hourly_merged), \"patients:\", hourly_merged['patientID'].nunique())\nhourly_merged.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell G ‚Äî Feature lists for modeling (to reuse exactly in the XGB/BiLSTM cells)","metadata":{}},{"cell_type":"code","source":"# These lists will feed directly into the modeling cells you have.\nTARGET = \"hypo_label\"\n\ndyn_cols = [c for c in [\n    # CGM dynamics (raw; PCA comes later in the pipeline, train-only)\n    \"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\",\n    \"cgm_mean_plus_std\",\"cgm_mean_minus_std\",\n    # Lifestyle raw streams aggregated hourly (PCs later, train-only)\n    \"steps\",\"calories\",\"distance\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\",\"heart_rate\",\"spo2\"\n] if c in hourly_merged.columns]\n\nvisit_cols = [c for c in [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_pct_29\"] if c in hourly_merged.columns]\nstatic_cols = [c for c in [\"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"] if c in hourly_merged.columns]\ncontext_cols = [c for c in [\"hour_of_day\",\"visit_assigned\",\"period_main\"] if c in hourly_merged.columns]\n\nprint(\"dyn_cols:\", dyn_cols)\nprint(\"visit_cols:\", visit_cols)\nprint(\"static_cols:\", static_cols)\nprint(\"context_cols:\", context_cols)\n\n# Load the final CSV as your canonical \"hourly\" frame for the next cells:\nhourly = pd.read_csv(PATH_HOURLY, parse_dates=[\"hour\"])\nprint(\"hourly shape:\", hourly.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}