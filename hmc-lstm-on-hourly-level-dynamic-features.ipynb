{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11815141,"sourceType":"datasetVersion","datasetId":7421066}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# Step 1: Create a new environment\n#!python -m venv cleanenv\n\n# Step 2: Activate it\n# On Windows:\n#!cleanenv\\Scripts\\activate\n# On Mac/Linux:\n#source cleanenv/bin/activate\n\n# Step 3: Install only what you need\n#!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.0 imbalanced-learn==0.13.0 tensorflow==2.18.0\n# GOOD (pick one)\n# 1) Install into the running kernel\n!pip install --upgrade pip\n!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.1 imbalanced-learn==0.13.0 tensorflow==2.18.0\n\n\n!pip install tensorflow\n\n!pip install tensorflow==2.18.0\nimport torch                         # PyTorch only\n# OR\nimport tensorflow as tf              # TensorFlow only\n# OR\nimport jax                           # JAX only\n\n# Import TF first so cuDNN is registered once\nimport tensorflow as tf\nimport torch\n\nimport numpy, scipy, sklearn, imblearn, tensorflow as tf\n\nprint(\"numpy:\", numpy.__version__)\nprint(\"scipy:\", scipy.__version__)\nprint(\"scikit-learn:\", sklearn.__version__)\nprint(\"imbalanced-learn:\", imblearn.__version__)\nprint(\"tensorflow:\", tf.__version__)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom sklearn.utils import resample\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:31:47.132165Z","iopub.execute_input":"2025-08-23T11:31:47.132463Z","iopub.status.idle":"2025-08-23T11:32:01.513641Z","shell.execute_reply.started":"2025-08-23T11:31:47.132439Z","shell.execute_reply":"2025-08-23T11:32:01.512554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/hmcdataset/intraday.csv\")\n\n# Preprocessing\ndf['start'] = pd.to_datetime(df['start'], errors='coerce')\ndf['date'] = df['start'].dt.date\ndf['hour'] = df['start'].dt.floor('h')  # use lowercase 'h' to avoid deprecation warning\n\n# Filter relevant columns and define hypoglycemia\ndf_cgm = df[['patientID', 'date', 'hour', 'cgm', 'steps']].dropna(subset=['cgm'])\ndf_cgm['hypo'] = df_cgm['cgm'] < 70\n\n\n# Keep only relevant columns and drop rows without CGM\ndf_cgm = df[['patientID', 'hour', 'cgm']].dropna(subset=['cgm'])\n\n# STEP 1: Filter for complete hours (≥ 4 CGM readings)\ngrouped = df_cgm.groupby(['patientID', 'hour'])\nvalid_hours = grouped.filter(lambda x: len(x) >= 4)\n\n# STEP 2: Create features and label\nfeatures = valid_hours.groupby(['patientID', 'hour']).agg(\n    cgm_std=('cgm', 'std'),\n    cgm_min=('cgm', 'min'),\n   cgm_mean=('cgm', 'mean'),\n    cgm_max=('cgm', 'max'),\n   \n    hypo_label=('cgm', lambda x: int((x < 70).any()))\n).reset_index()\n\n# STEP 3: Sort for time-series modeling\nfeatures = features.sort_values(['patientID', 'hour']).reset_index(drop=True)\n\n# STEP 4: Display preview of the processed data\n#print(\"LSTM-ready features (preview):\")\n#print(features.head())\n\nimport numpy as np\n\n#Train/Test Split by Sample vs. by Patient\n# Assign entire patients to either the training or testing set.\n#Group and build sequences separately for each set to avoid data leakage.\n\nfrom sklearn.model_selection import train_test_split\n\n# Step 1: Get list of unique patients and split\nunique_patients = features['patientID'].unique()\ntrain_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)\n\n# Step 2: Split the features dataframe accordingly\ntrain_df = features[features['patientID'].isin(train_patients)]\ntest_df = features[features['patientID'].isin(test_patients)]\n\n# Configuration\nsequence_length = 24\nfeature_cols = ['cgm_mean']  # You can include others like 'cgm_min', 'cgm_std' if available\n\ndef build_sequences(df, feature_cols, label_col='hypo_label'):\n    X, y = [], []\n    for patient_id, group in df.groupby('patientID'):\n        group = group.sort_values('hour').reset_index(drop=True)\n        for i in range(len(group) - sequence_length):\n            seq_x = group.loc[i:i+sequence_length-1, feature_cols].values\n            seq_y = group.loc[i + sequence_length, label_col]\n            X.append(seq_x)\n            y.append(seq_y)\n    return np.array(X), np.array(y)\n\n# Build sequences for each subset\nX_train, y_train = build_sequences(train_df, feature_cols)\nX_test, y_test = build_sequences(test_df, feature_cols)\n\n#print(X_train[:5])   # Preview first 5 sequences from training set\n#print(y_train[:20])  # Preview first 20 labels from training set\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:36:45.551946Z","iopub.execute_input":"2025-08-23T11:36:45.552219Z","iopub.status.idle":"2025-08-23T11:37:06.748128Z","shell.execute_reply.started":"2025-08-23T11:36:45.552201Z","shell.execute_reply":"2025-08-23T11:37:06.747537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:37:19.796085Z","iopub.execute_input":"2025-08-23T11:37:19.796631Z","iopub.status.idle":"2025-08-23T11:37:19.800995Z","shell.execute_reply.started":"2025-08-23T11:37:19.796606Z","shell.execute_reply":"2025-08-23T11:37:19.800307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Balanced LSTM Pipeline (All-in-1)\n# ================================\n\n# ------ Optional installs (Kaggle/Colab) ------\n# !pip install --upgrade pip\n# !pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.1 imbalanced-learn==0.13.0 tensorflow==2.18.0\n\n# ------ Imports ------\nimport os, time, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout, TimeDistributed, Flatten\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# --------------------------\n# Config\n# --------------------------\nDATA_PATH         = \"/kaggle/input/hmcdataset/intraday.csv\"\nSEQUENCE_LENGTH   = 24\nFEATURE_COLS      = [\"cgm_mean\"]      # you can add: \"cgm_min\",\"cgm_std\",\"cgm_max\"\nTHR_MIN, THR_MAX  = 0.40, 0.60\nRANDOM_STATE      = 42\nAUGMENT_SIGMA     = 0.01              # small Gaussian jitter on train (optional); set None to disable\nRESAMPLE_METHODS  = [\n    \"none\",           # baseline (class_weight focal only)\n    \"oversample_seq\", # duplicate minority sequences\n    \"undersample_seq\",# downsample majority sequences\n    \"smote\",          # SMOTE on flattened sequences\n    \"smoteenn\",       # SMOTE+ENN on flattened sequences\n    \"smotetomek\"      # SMOTE+Tomek on flattened sequences\n]\n\n# --------------------------\n# Utilities\n# --------------------------\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask]))\n        idx    = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_from_cm(cm, pos_label=1):\n    tn, fp, fn, tp = cm.ravel()\n    if pos_label == 1:  # negatives are 0\n        return tn / (tn + fp + 1e-8)\n    else:               # negatives are 1\n        return tp / (tp + fn + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    # per-class\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_from_cm(cm, pos_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    # overall (hard preds)\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/MSE_pred\"]             = mse_pred\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    # prob-based\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/MSE_prob\"]       = mse_prob\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/MSE_prob\"] = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n\n    return metrics\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\n# --------------------------\n# Data loading & preprocessing\n# --------------------------\ndf = pd.read_csv(DATA_PATH)\ndf[\"start\"] = pd.to_datetime(df[\"start\"], errors=\"coerce\")\ndf[\"hour\"]  = df[\"start\"].dt.floor(\"h\")\n\n# keep & aggregate (≥4 CGM readings per hour)\ndf_cgm = df[[\"patientID\",\"hour\",\"cgm\"]].dropna(subset=[\"cgm\"])\nvalid  = df_cgm.groupby([\"patientID\",\"hour\"]).filter(lambda x: len(x) >= 4)\n\nfeatures = (valid.groupby([\"patientID\",\"hour\"])\n                 .agg(cgm_std=(\"cgm\",\"std\"),\n                      cgm_min=(\"cgm\",\"min\"),\n                      cgm_mean=(\"cgm\",\"mean\"),\n                      cgm_max=(\"cgm\",\"max\"),\n                      hypo_label=(\"cgm\", lambda x: int((x<70).any())))\n                 .reset_index()\n                 .sort_values([\"patientID\",\"hour\"])\n                 .reset_index(drop=True))\n\n# patient-level split\nunique_patients = features[\"patientID\"].unique()\ntrain_pat, test_pat = train_test_split(unique_patients, test_size=0.20, random_state=RANDOM_STATE)\ntrain_df = features[features[\"patientID\"].isin(train_pat)]\ntest_df  = features[features[\"patientID\"].isin(test_pat)]\n\n# --------------------------\n# Sequences\n# --------------------------\ndef build_sequences(df, feature_cols, label_col=\"hypo_label\", seq_len=SEQUENCE_LENGTH):\n    X, y = [], []\n    for pid, grp in df.groupby(\"patientID\"):\n        grp = grp.sort_values(\"hour\").reset_index(drop=True)\n        for i in range(len(grp) - seq_len):\n            X.append(grp.loc[i:i+seq_len-1, feature_cols].values)\n            y.append(int(grp.loc[i+seq_len, label_col]))\n    return np.array(X), np.array(y)\n\nX_train, y_train = build_sequences(train_df, FEATURE_COLS)\nX_test,  y_test  = build_sequences(test_df,  FEATURE_COLS)\n\n# sanity: no leakage\nassert set(train_df.patientID).isdisjoint(set(test_df.patientID))\n\n# tiny augmentation (optional)\ndef augment(X, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0: return X, y\n    noise = np.random.normal(0, sigma, X.shape)\n    return np.vstack([X, X+noise]), np.hstack([y, y])\n\nX_train, y_train = augment(X_train, y_train)\n\n# --------------------------\n# Resampling (sequence level & SMOTE family)\n# --------------------------\nRNG = np.random.default_rng(RANDOM_STATE)\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE):\n    \"\"\"\n    method ∈ {\n      \"none\",\n      \"oversample_seq\", \"undersample_seq\",        # window-level (no interpolation)\n      \"smote\", \"smoteenn\", \"smotetomek\"           # flattened window resampling\n    }\n    \"\"\"\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n\n    if method == \"none\":\n        return X, y\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0: return X, y\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = RNG.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = RNG.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:\n            if n0 > n1:\n                keep0 = RNG.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = RNG.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        RNG.shuffle(keep)\n        return X[keep], y[keep]\n\n    # SMOTE family on flattened sequences\n    Xf = X.reshape(n, -1)\n    if method == \"smote\":\n        minority_n = int((y==1).sum())\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n    return Xr.reshape(-1, T, F), yr\n\ndef make_balanced_test(X_test, y_test, random_state=RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0: return X_test, y_test\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    return X_test[keep], y_test[keep]\n\nX_test_bal, y_test_bal = make_balanced_test(X_test, y_test)\n\n# --------------------------\n# Models\n# --------------------------\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True), Dropout(0.2),\n            LSTM(50), Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True), Dropout(0.2),\n            LSTM(25), Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5)), Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(1e-5)), Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(1e-5)),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_25_L2\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5)), Dropout(0.2),\n            LSTM(25, kernel_regularizer=l2(1e-5)), Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l2(1e-5)),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"BiLSTM\": Sequential([\n            Input(shape=input_shape),\n            Bidirectional(LSTM(64, return_sequences=True)), Dropout(0.2),\n            Bidirectional(LSTM(32)), Dropout(0.2),\n            Dense(16, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ])\n    }\n\n# --------------------------\n# Train + evaluate\n# --------------------------\nos.makedirs(\"checkpoints\", exist_ok=True)\nos.makedirs(\"plots\", exist_ok=True)\nos.makedirs(\"outputs\", exist_ok=True)\n\nresults = {}       # key -> metrics dict\nroc_data = {}      # (method, model) -> (fpr, tpr, auc)\npr_data  = {}      # (method, model) -> (recall, precision, ap)\nbest_thresholds = {}  # (method, model) -> {\"youden\": t, \"f1\": t}\n\ndef train_eval_one(method_name, model_name, model, Xtr, ytr, Xte, yte, XteB, yteB):\n    tag = f\"{method_name}__{model_name}\"\n    print(f\"\\n🚀 Training [{tag}] with class-weighted focal loss\")\n    es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n    cp = ModelCheckpoint(f\"checkpoints/{tag}.h5\", save_best_only=True, monitor='val_loss', verbose=0)\n    model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n\n    class_weight = make_class_weight(ytr)\n    t0 = time.time()\n    model.fit(Xtr, ytr, epochs=5, batch_size=32,\n              validation_data=(Xte, yte),\n              callbacks=[es, cp], verbose=1,\n              class_weight=class_weight)\n    print(f\"⏱️ Training Time: {time.time()-t0:.2f}s\")\n\n    # probabilities\n    p_tr  = model.predict(Xtr,  verbose=0).ravel()\n    p_te  = model.predict(Xte,  verbose=0).ravel()\n    p_teB = model.predict(XteB, verbose=0).ravel()\n\n    # thresholds (on ORIGINAL test), constrained to [0.40, 0.60]\n    try:\n        fpr, tpr, thr_roc = roc_curve(yte, p_te); auc_roc = auc(fpr, tpr)\n    except ValueError:\n        fpr, tpr, thr_roc, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n    youden = tpr - fpr\n    t_roc, _ = _best_threshold_in_range(thr_roc, youden)\n\n    prec, rec, thr_pr = precision_recall_curve(yte, p_te)\n    f1s = 2*prec[:-1]*rec[:-1] / (prec[:-1]+rec[:-1]+1e-8)\n    t_pr, _ = _best_threshold_in_range(thr_pr, f1s)\n    ap_val  = average_precision_score(yte, p_te)\n\n    roc_data[(method_name, model_name)] = (fpr, tpr, auc_roc)\n    pr_data[(method_name, model_name)]  = (rec, prec, ap_val)\n    best_thresholds[(method_name, model_name)] = {\"youden\": t_roc, \"f1\": t_pr}\n    print(f\"📌 [{tag}] thresholds → Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{THR_MIN},{THR_MAX}])\")\n\n    eval_ts = sorted(set([THR_MIN, 0.50, THR_MAX, float(t_roc), float(t_pr)]))\n    # evaluate at all thresholds on train / test / testBalanced\n    for t in eval_ts:\n        yhat_tr  = (p_tr  >= t).astype(int)\n        yhat_te  = (p_te  >= t).astype(int)\n        yhat_teB = (p_teB >= t).astype(int)\n\n        results[f\"{tag}__thr_{t:.2f}__train\"]         = evaluate_full_metrics(ytr,  yhat_tr,  p_tr)\n        results[f\"{tag}__thr_{t:.2f}__test\"]          = evaluate_full_metrics(yte,  yhat_te,  p_te)\n        results[f\"{tag}__thr_{t:.2f}__testBalanced\"]  = evaluate_full_metrics(yteB, yhat_teB, p_teB)\n\n# run all methods x models\nmodels = define_models((X_train.shape[1], X_train.shape[2]))\n\nfor METHOD in RESAMPLE_METHODS:\n    Xtr_rs, ytr_rs = seq_resample(X_train, y_train, method=METHOD)\n    print(f\"\\n🔁 Resampling: {METHOD} → X={Xtr_rs.shape}, y={Counter(ytr_rs)}\")\n    for mname, model in define_models((X_train.shape[1], X_train.shape[2])).items():\n        train_eval_one(METHOD, mname, model, Xtr_rs, ytr_rs, X_test, y_test, X_test_bal, y_test_bal)\n\n# --------------------------\n# Curves (optional plots)\n# --------------------------\nplt.figure(figsize=(14,6))\n# ROC\nplt.subplot(1,2,1)\nfor (meth, mname), (fpr, tpr, auc_roc) in roc_data.items():\n    plt.plot(fpr, tpr, label=f'{meth}/{mname} (AUC={auc_roc:.3f})')\nplt.plot([0,1],[0,1],'--',label='Random')\nplt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC'); plt.legend(fontsize=8)\n# PR\nplt.subplot(1,2,2)\nfor (meth, mname), (rec, prec, ap) in pr_data.items():\n    plt.plot(rec, prec, label=f'{meth}/{mname} (AP={ap:.3f})')\nplt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR'); plt.legend(fontsize=8)\nplt.tight_layout(); plt.savefig(\"plots/combined_roc_pr_curves.png\", dpi=300); plt.show()\n\n# --------------------------\n# --------------------------\n# Summaries (robust)\n# --------------------------\nresults_df = pd.DataFrame(results).T\n\n# Keep the original key for debugging\nresults_df = results_df.reset_index().rename(columns={\"index\":\"Key\"})\n\n# Robust Split extraction (no regex fragility / trailing-space safe)\nk = results_df[\"Key\"].str.strip()\nsplit = np.where(k.str.endswith(\"__train\"), \"train\",\n         np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n         np.where(k.str.endswith(\"__test\"), \"test\", np.nan)))\nresults_df[\"Split\"] = split\n\n# Method, Model, Threshold extraction (robust)\n# Key format: \"<method>__<model>__thr_<thr>__<split>\"\nparts = k.str.split(\"__\")\nresults_df[\"Method\"] = parts.str[0]\nresults_df[\"Model\"]  = parts.str[1]\n# threshold lives in the third chunk like \"thr_0.40\"\nthr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\nwith np.errstate(all='ignore'):\n    results_df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n\n# Save\nresults_df.round(6).to_csv(\"outputs/results_summary_all.csv\", index=False)\nprint(\"\\n📁 Saved files:\")\nprint(\" - plots/combined_roc_pr_curves.png\")\nprint(\" - outputs/results_summary_all.csv\")\n\n# Quick sanity\nprint(\"\\nSplit counts:\")\nprint(results_df[\"Split\"].value_counts(dropna=False))\n\n# Leaderboards\neval_test_df  = results_df[results_df[\"Split\"]==\"test\"].copy()\neval_tbal_df  = results_df[results_df[\"Split\"]==\"testBalanced\"].copy()\n\ndef top_k(df, by_col, k=10, cols=None):\n    if df.empty:\n        return pd.DataFrame(columns=(cols or []))\n    if cols is None:\n        cols = ['Method','Model','Threshold','Overall/F1_weighted','Overall/Recall_weighted',\n                'Overall/Precision_weighted','Overall/ROC-AUC','Overall/PR-AUC','Overall/Accuracy']\n    present = [c for c in cols if c in df.columns]\n    return df.sort_values(by_col, ascending=False)[present].head(k).round(4)\n\nprint(\"\\n🔽 ORIGINAL TEST — top by Overall/F1_weighted\")\nprint(top_k(eval_test_df, 'Overall/F1_weighted'))\n\nprint(\"\\n🔽 BALANCED TEST — top by Overall/F1_weighted\")\nprint(top_k(eval_tbal_df, 'Overall/F1_weighted'))\n\ndef best_per_model(df):\n    if df.empty:\n        return df\n    idx = df.groupby(['Method','Model'])['Overall/F1_weighted'].idxmax()\n    return df.loc[idx].sort_values(['Overall/F1_weighted'], ascending=False)\n\noverall_cols = [\n    'Overall/Accuracy','Overall/Precision_macro','Overall/Recall_macro','Overall/F1_macro',\n    'Overall/Precision_weighted','Overall/Recall_weighted','Overall/F1_weighted',\n    'Overall/Specificity(label=1)','Overall/ROC-AUC','Overall/PR-AUC',\n    'Overall/MSE_pred','Overall/RMSE_pred','Overall/MSE_prob','Overall/RMSE_prob'\n]\nclass_cols = [\n    'Class0/Precision','Class0/Recall','Class0/F1','Class0/Specificity','Class0/Support',\n    'Class1/Precision','Class1/Recall','Class1/F1','Class1/Specificity','Class1/Support'\n]\n\nprint(\"\\n=== ORIGINAL TEST — best per (Method,Model) ===\")\nbest_test = best_per_model(eval_test_df)\nif best_test.empty:\n    print(\"⚠️ No TEST rows found — check Split counts above.\")\nelse:\n    print(best_test[['Method','Model','Threshold']+[c for c in overall_cols if c in best_test.columns]].round(4))\n    print(\"\\n--- Per-class breakdown:\")\n    print(best_test[['Method','Model','Threshold']+[c for c in class_cols if c in best_test.columns]].round(4))\n\nprint(\"\\n=== BALANCED TEST — best per (Method,Model) ===\")\nbest_tbal = best_per_model(eval_tbal_df)\nif best_tbal.empty:\n    print(\"⚠️ No BALANCED TEST rows found — check Split counts above.\")\nelse:\n    print(best_tbal[['Method','Model','Threshold']+[c for c in overall_cols if c in best_tbal.columns]].round(4))\n    print(\"\\n--- Per-class breakdown:\")\n    print(best_tbal[['Method','Model','Threshold']+[c for c in class_cols if c in best_tbal.columns]].round(4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== ✅ Step 7B: Inline displays (no Excel needed) =====\nfrom IPython.display import display\nimport pandas as pd\n\npd.set_option(\"display.max_rows\", 200)\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 180)\npd.set_option(\"display.precision\", 4)\n\n# --------------------------------------\n# Flatten results -> DataFrame\n# --------------------------------------\nresults_df = pd.DataFrame(results).T\nresults_df[\"Split\"] = results_df.index.str.extract(r'_(train|test)$')[0]\nresults_df[\"Model\"] = results_df.index.str.extract(r'^(.*?)_thr_')[0]\nresults_df[\"Threshold\"] = (\n    results_df.index.str.extract(r'thr_([0-9.]+)_(?:train|test)$')[0]\n    .astype(float)\n)\n\n# --------------------------------------\n# Define column groups (safe + flexible)\n# --------------------------------------\noverall_cols = [\n    'Overall/Accuracy','Overall/Precision_macro','Overall/Recall_macro','Overall/F1_macro',\n    'Overall/Precision_weighted','Overall/Recall_weighted','Overall/F1_weighted',\n    'Overall/ROC-AUC','Overall/PR-AUC',\n    'Overall/MSE_pred','Overall/RMSE_pred','Overall/MSE_prob','Overall/RMSE_prob'\n]\n\nclass_cols = [\n    'Class0/Precision','Class0/Recall','Class0/F1','Class0/Specificity','Class0/Support',\n    'Class1/Precision','Class1/Recall','Class1/F1','Class1/Specificity','Class1/Support'\n]\n\n# helper: keep only existing columns\ndef safe_cols(df, cols):\n    return [c for c in cols if c in df.columns]\n\n# --------------------------------------\n# Select best rows per model\n# --------------------------------------\ndef best_per_model(df):\n    idx = df.groupby('Model')['Overall/F1_weighted'].idxmax()\n    return df.loc[idx].sort_values('Overall/F1_weighted', ascending=False)\n\n# --------------------------------------\n# TEST — Top 20 by F1_weighted\n# --------------------------------------\ntest_all = results_df[results_df['Split'] == 'test'].copy()\ntest_sorted = test_all.sort_values('Overall/F1_weighted', ascending=False)\nprint(\"\\n🔎 TEST — All models & thresholds (Top 20 by F1_weighted)\")\ndisplay(test_sorted[['Model','Threshold'] + safe_cols(test_sorted, overall_cols)].head(20).round(4))\n\n# --------------------------------------\n# TRAIN — Top 20 by F1_weighted\n# --------------------------------------\ntrain_all = results_df[results_df['Split'] == 'train'].copy()\ntrain_sorted = train_all.sort_values('Overall/F1_weighted', ascending=False)\nprint(\"\\n🔎 TRAIN — All models & thresholds (Top 20 by F1_weighted)\")\ndisplay(train_sorted[['Model','Threshold'] + safe_cols(train_sorted, overall_cols)].head(20).round(4))\n\n# --------------------------------------\n# BEST PER MODEL (TEST)\n# --------------------------------------\ntest_best = best_per_model(test_all)\nprint(\"\\n🏁 TEST — Best per model (overall metrics @ best threshold)\")\ndisplay(test_best[['Model','Threshold'] + safe_cols(test_best, overall_cols)].round(4))\n\nprint(\"\\n🏁 TEST — Best per model (per-label metrics @ best threshold)\")\ndisplay(test_best[['Model','Threshold'] + safe_cols(test_best, class_cols)].round(4))\n\n# --------------------------------------\n# BEST PER MODEL (TRAIN)\n# --------------------------------------\ntrain_best = best_per_model(train_all)\nprint(\"\\n🏋️ TRAIN — Best per model (overall metrics @ best threshold)\")\ndisplay(train_best[['Model','Threshold'] + safe_cols(train_best, overall_cols)].round(4))\n\nprint(\"\\n🏋️ TRAIN — Best per model (per-label metrics @ best threshold)\")\ndisplay(train_best[['Model','Threshold'] + safe_cols(train_best, class_cols)].round(4))\n\n# --------------------------------------\n# Compact view: TEST\n# --------------------------------------\ncompact_cols = ['Overall/F1_weighted','Overall/Recall_weighted','Overall/PR-AUC','Overall/Accuracy']\nprint(\"\\n📋 TEST — Compact table per model & threshold (sorted by F1_weighted)\")\ncompact = (\n    test_all[['Model','Threshold'] + safe_cols(test_all, compact_cols)]\n    .sort_values(['Model','Overall/F1_weighted'], ascending=[True, False])\n)\ndisplay(compact.round(4).reset_index(drop=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\nimport numpy as np\nimport pandas as pd\n\ndef _to_1d_int(y):\n    y = np.asarray(y).ravel()\n    # cast to int if your labels are floats like 0.0/1.0\n    try:\n        return y.astype(int)\n    except Exception:\n        return y\n\ny_tr = _to_1d_int(y_train)\ny_te = _to_1d_int(y_test)\ny_all = np.concatenate([y_tr, y_te])\n\ndef class_counts(y):\n    c = Counter(y.tolist())\n    # sort by class label for stable printing\n    return {k: c[k] for k in sorted(c.keys())}\n\ndef class_props(y):\n    c = Counter(y.tolist())\n    total = sum(c.values())\n    return {k: f\"{c[k]} ({c[k]/total:.2%})\" for k in sorted(c.keys())}\n\nprint(\"🔢 Class Distribution Summary\")\nprint(f\"➡️ Full Dataset : {class_counts(y_all)}\")\nprint(f\"➡️ Training Set : {class_counts(y_tr)}\")\nprint(f\"➡️ Testing Set  : {class_counts(y_te)}\")\n\nprint(\"\\n📊 Class Proportions\")\nprint(f\"Full  : {class_props(y_all)}\")\nprint(f\"Train : {class_props(y_tr)}\")\nprint(f\"Test  : {class_props(y_te)}\")\nfrom IPython.display import display\n\ndef df_counts_props(y, split_name):\n    c = Counter(y.tolist()); total = sum(c.values())\n    rows = [{\"Split\": split_name, \"Class\": k, \"Count\": c[k], \"Proportion\": c[k]/total}\n            for k in sorted(c.keys())]\n    return pd.DataFrame(rows)\n\ndist_df = pd.concat([\n    df_counts_props(y_tr, \"Train\"),\n    df_counts_props(y_te, \"Test\"),\n    df_counts_props(y_all, \"Full\")\n], ignore_index=True)\n\ndisplay(dist_df.pivot(index=\"Class\", columns=\"Split\", values=[\"Count\",\"Proportion\"]).round(4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}