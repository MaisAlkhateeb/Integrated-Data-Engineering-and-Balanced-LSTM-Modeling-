{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10739756,"sourceType":"datasetVersion","datasetId":6659625},{"sourceId":13049616,"sourceType":"datasetVersion","datasetId":8213963}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. ID mapping \n\nyour final_master_sheet_clean.xlsx has a unified patient ID column that lines up with your intraday data.\nwhat we’ll do\n\n##### 1. load final_master_sheet_clean.xlsx.\n##### 2. build a mapping Code → patientID (from the table you provided).\n##### 3. for every visit sheet (Ramadan, Visit 1 … Visit 7), replace/add a column PatientID (Huawei Data).\n##### 4. keep the original Code if you like, or drop it once you confirm the mapping is correct.\n##### 5. save a new Excel (e.g., final_master_sheet_clean_with_huawei.xlsx).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\n# input/output\nMASTER_PATH = Path(\"/kaggle/input/static-variables/final_master_sheet_clean.xlsx\")\nOUT_PATH = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\n\n# your mapping dictionary\nPATIENT_ID_MAP = {\n    \"R01\": 45, \"R02\": 46, \"R04\": 47, \"R05\": 48, \"R06\": 49, \"R07\": 53,\n    \"R10\": 54, \"R11\": 55, \"R12\": 57, \"R15\": 59, \"R16\": 60, \"R17\": 61,\n    \"R20\": 63, \"R21\": 64, \"R22\": 66, \"R23\": 67, \"R24\": 68, \"R25\": 69,\n    \"R26\": 70, \"R27\": 71, \"R28\": 72, \"R29\": 73, \"R30\": 74, \"R31\": 75,\n    \"R32\": 76, \"R33\": 77, \"R34\": 78, \"R35\": 79, \"R36\": 80, \"R37\": 81,\n    \"R39\": 82, \"R40\": 83, \"R41\": 84, \"R42\": 85, \"R43\": 86,\n}\nid_map_df = pd.DataFrame(list(PATIENT_ID_MAP.items()), columns=[\"Code\", \"PatientID (Huawei Data)\"])\n\n# load workbook\nxls = pd.ExcelFile(MASTER_PATH)\nsheets = {s: pd.read_excel(MASTER_PATH, sheet_name=s) for s in xls.sheet_names}\n\n# update each sheet that has \"Code\"\nupdated_sheets = {}\nfor name, df in sheets.items():\n    if \"Code\" in df.columns:\n        df = df.merge(id_map_df, on=\"Code\", how=\"left\")\n        # optional: drop old Code col and just keep Huawei ID\n        # df = df.drop(columns=[\"Code\"])\n        updated_sheets[name] = df\n    else:\n        updated_sheets[name] = df\n\n# save to new Excel\nwith pd.ExcelWriter(OUT_PATH, engine=\"openpyxl\") as writer:\n    for name, df in updated_sheets.items():\n        df.to_excel(writer, index=False, sheet_name=name)\n\nprint(f\"saved → {OUT_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T13:52:43.639321Z","iopub.execute_input":"2025-09-06T13:52:43.639676Z","iopub.status.idle":"2025-09-06T13:52:45.164197Z","shell.execute_reply.started":"2025-09-06T13:52:43.639648Z","shell.execute_reply":"2025-09-06T13:52:45.163327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. subperiods + Ramadan/Shawwal periods\n\nSets main periods (Ramadan/Shawwal) with your exact dates.\n\nDefines visit subperiods as inclusive date ranges:\n\nV1 = 2023-03-13 → 2023-03-26\nV2 = 2023-03-27 → 2023-04-02\nV3 = 2023-04-03 → 2023-04-09\nV4 = 2023-04-10 → 2023-04-19 (ends at Ramadan end)\nV5 = 2023-04-20 → 2023-04-26\nV6 = 2023-04-27 → 2023-05-08\nV7 = 2023-05-09 → 2023-05-19 (ends at Shawwal end)\n\n\nAdd explicit Visit subperiods + Ramadan/Shawwal periods:\n- Annotates intraday rows with `period_main` and `visit_assigned`\n- Writes spec sheets into master workbook\n- Saves:\n  /kaggle/working/intraday_with_visits.csv\n  /kaggle/working/final_master_sheet_clean_with_visits.xlsx\n\n","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nAnnotate intraday with Visit subperiods + Ramadan/Shawwal periods\n- Adds `visit_assigned` (inclusive visit windows) and `period_main` (Ramadan/Shawwal/outside)\n- Saves annotated intraday to CSV and **single-sheet Excel (Intraday_All)**\n- Injects spec sheets into the master workbook and adds Visit_Anchor_Date to visit sheets\n- Also writes a dynamic missingness-by-visit CSV (optional QA artifact)\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ---------------- CONFIG ----------------\nINTRADAY_CSV_PATH = Path(\"/kaggle/input/intraday/intraday.csv\")\nMASTER_XLSX_IN    = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\nMASTER_XLSX_OUT   = Path(\"/kaggle/working/final_master_sheet_clean_with_visits.xlsx\")\n\nINTRADAY_OUT_CSV  = Path(\"/kaggle/working/intraday_with_visits.csv\")\nINTRADAY_OUT_XLSX = Path(\"/kaggle/working/intraday_with_visits.xlsx\")\n\nDYN_VISIT_OUT     = Path(\"/kaggle/working/missingness_dynamic_by_visit.csv\")  # optional QA\n\n# Main periods (inclusive)\nI_RAMADAN_START = pd.Timestamp(\"2023-03-22\")\nI_RAMADAN_END   = pd.Timestamp(\"2023-04-19\")\nI_SHAWWAL_START = pd.Timestamp(\"2023-04-20\")\nI_SHAWWAL_END   = pd.Timestamp(\"2023-05-19\")\n\n# Visit subperiods (ALL INCLUSIVE — exactly as specified)\nVISIT_SUBPERIODS = [\n    {\"Visit\": \"Visit 1\", \"start\": \"2023-03-13\", \"end\": \"2023-03-21\"},\n    {\"Visit\": \"Visit 2\", \"start\": \"2023-03-22\", \"end\": \"2023-03-30\"},\n    {\"Visit\": \"Visit 3\", \"start\": \"2023-03-31\", \"end\": \"2023-04-06\"},\n    {\"Visit\": \"Visit 4\", \"start\": \"2023-04-08\", \"end\": \"2023-04-16\"},\n    {\"Visit\": \"Visit 5\", \"start\": \"2023-04-17\", \"end\": \"2023-04-26\"},\n    {\"Visit\": \"Visit 6\", \"start\": \"2023-04-27\", \"end\": \"2023-05-08\"},\n    {\"Visit\": \"Visit 7\", \"start\": \"2023-05-09\", \"end\": \"2023-05-19\"},\n]\n\n# Sheets in master we do not modify\nSKIP_SHEETS = {\"HMC_map_patientID\"}\n\n\n# ---------------- HELPERS ----------------\ndef _normalize_date_col(s: pd.Series) -> pd.Series:\n    return pd.to_datetime(s, errors=\"coerce\").dt.normalize()\n\ndef build_visit_spec_df(rows: list[dict]) -> pd.DataFrame:\n    \"\"\"\n    Build inclusive visit windows as a tidy DataFrame, warn (don't crash) on overlaps or bad spans.\n    \"\"\"\n    df = pd.DataFrame(rows)\n    df[\"start\"] = _normalize_date_col(df[\"start\"])\n    df[\"end\"]   = _normalize_date_col(df[\"end\"])\n    df = df.sort_values([\"start\", \"end\"]).reset_index(drop=True)\n\n    bad_span = df[df[\"end\"] < df[\"start\"]]\n    if not bad_span.empty:\n        print(\"⚠️ Visit with end < start:\\n\", bad_span)\n\n    df[\"prev_end\"] = df[\"end\"].shift(1)\n    overlap = df[(df.index > 0) & (df[\"start\"] <= df[\"prev_end\"])]\n    if not overlap.empty:\n        print(\"⚠️ Overlapping visits detected (inclusive ranges). Ensure previous 'end' < next 'start'.\")\n        print(overlap[[\"Visit\", \"start\", \"end\", \"prev_end\"]])\n\n    return df.drop(columns=[\"prev_end\"], errors=\"ignore\")\n\ndef tag_main_period(ts: pd.Timestamp) -> str | float:\n    if pd.isna(ts):\n        return np.nan\n    if I_RAMADAN_START <= ts <= I_RAMADAN_END:\n        return \"Ramadan (Mar 22–Apr 19, 2023)\"\n    if I_SHAWWAL_START <= ts <= I_SHAWWAL_END:\n        return \"Shawwal (Apr 20–May 19, 2023)\"\n    return \"Outside Ramadan/Shawwal\"\n\ndef assign_visit_inclusive(dates: pd.Series, visit_spec: pd.DataFrame) -> pd.Series:\n    \"\"\"\n    Inclusive mapping: a date belongs to Visit i if start_i <= date <= end_i.\n    Dates outside all windows remain NaN.\n    \"\"\"\n    visits_cat = pd.CategoricalDtype(categories=list(visit_spec[\"Visit\"]), ordered=True)\n    out = pd.Series(pd.Categorical([None] * len(dates), dtype=visits_cat), index=dates.index)\n    for _, r in visit_spec.iterrows():\n        mask = (dates >= r[\"start\"]) & (dates <= r[\"end\"])\n        out.loc[mask] = r[\"Visit\"]\n    return out\n\ndef _order_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Put key columns first for readability if they exist.\"\"\"\n    cols = list(df.columns)\n    front = [c for c in [\"date\", \"visit_assigned\", \"period_main\", \"patientID\", \"huaweiID\"] if c in cols]\n    rest  = [c for c in cols if c not in front]\n    return df[front + rest]\n\n\n# ---------------- 1) LOAD & ANNOTATE INTRADAY ----------------\nprint(\"Annotating intraday with visit subperiods & main periods…\")\nintraday = pd.read_csv(INTRADAY_CSV_PATH)\nif \"date\" not in intraday.columns:\n    raise ValueError(\"intraday.csv must have a 'date' column\")\n\nintraday[\"date\"] = _normalize_date_col(intraday[\"date\"])\n\nvisit_spec_df = build_visit_spec_df(VISIT_SUBPERIODS)\nintraday[\"period_main\"]    = intraday[\"date\"].apply(tag_main_period)\nintraday[\"visit_assigned\"] = assign_visit_inclusive(intraday[\"date\"], visit_spec_df)\n\n# Save CSV once\nintraday.to_csv(INTRADAY_OUT_CSV, index=False)\nprint(f\"✅ Saved annotated intraday CSV → {INTRADAY_OUT_CSV}\")\n\n# ---------------- 2) UPDATE MASTER WORKBOOK ----------------\nprint(\"Injecting Visit_Subperiods_Spec & Period_Bounds into master…\")\nxls = pd.ExcelFile(MASTER_XLSX_IN)\nsheets = {s: pd.read_excel(MASTER_XLSX_IN, sheet_name=s) for s in xls.sheet_names}\n\n# optional: add a Visit_Anchor_Date to each Visit sheet (start of its inclusive subperiod)\nANCHORS = {row[\"Visit\"]: row[\"start\"] for _, row in visit_spec_df.iterrows()}\n\nupdated = {}\nfor name, df in sheets.items():\n    if name in SKIP_SHEETS:\n        updated[name] = df\n        continue\n    if name in ANCHORS and \"Visit_Anchor_Date\" not in df.columns:\n        df = df.copy()\n        df[\"Visit_Anchor_Date\"] = ANCHORS[name].date()\n    updated[name] = df\n\nperiod_bounds = pd.DataFrame({\n    \"Period\":    [\"Ramadan\", \"Shawwal\"],\n    \"Start\":     [I_RAMADAN_START.date(), I_SHAWWAL_START.date()],\n    \"End\":       [I_RAMADAN_END.date(),   I_SHAWWAL_END.date()],\n    \"Inclusive\": [True, True],\n})\n\nwith pd.ExcelWriter(MASTER_XLSX_OUT, engine=\"openpyxl\") as writer:\n    for name, df in updated.items():\n        df.to_excel(writer, index=False, sheet_name=name[:31])  # safe truncate\n    visit_spec_df.to_excel(writer, index=False, sheet_name=\"Visit_Subperiods_Spec\")\n    period_bounds.to_excel(writer, index=False, sheet_name=\"Period_Bounds\")\n\nprint(f\"✅ Saved master with specs → {MASTER_XLSX_OUT}\")\n\n# ---------------- 3) OPTIONAL QA: MISSINGNESS BY VISIT (CSV) ----------------\nEXCLUDE = {\"Unnamed: 0\", \"huaweiID\", \"date\", \"start\", \"period_main\", \"visit_assigned\"}\ndfv = intraday.copy()\nfeature_cols = [c for c in dfv.columns if c not in EXCLUDE and c != \"patientID\"]\n\nif feature_cols:\n    out = (\n        dfv.groupby(\"visit_assigned\")[feature_cols]\n           .apply(lambda g: g.isna().mean().mul(100).round(2))\n           .reset_index()\n           .rename(columns={\"visit_assigned\": \"Visit\"})\n    )\n    out.melt(id_vars=[\"Visit\"], var_name=\"Feature\", value_name=\"% Missing (Visit Window)\") \\\n       .to_csv(DYN_VISIT_OUT, index=False)\n    print(f\"✅ Saved dynamic-by-visit missingness CSV → {DYN_VISIT_OUT}\")\nelse:\n    print(\"ℹ️ Skipped missingness export (no feature columns found).\")\n\n# ---------------- 4) SINGLE-SHEET EXCEL EXPORT ----------------\nprint(\"Writing single-sheet annotated intraday Excel…\")\nintraday_ordered = _order_columns(intraday)\n\nwith pd.ExcelWriter(INTRADAY_OUT_XLSX, engine=\"openpyxl\") as writer:\n    intraday_ordered.to_excel(writer, index=False, sheet_name=\"Intraday_All\")\n\nprint(f\"✅ Saved single-sheet intraday Excel → {INTRADAY_OUT_XLSX}\")\n\n# ---------------- 5) QUICK CHECKS (stdout) ----------------\nprint(\"\\n— Quick checks —\")\ntry:\n    print(\"Date coverage:\", intraday[\"date\"].min(), \"→\", intraday[\"date\"].max(), \"| rows:\", len(intraday))\n    print(\"\\nVisit assignment counts:\")\n    print(intraday[\"visit_assigned\"].value_counts(dropna=False))\n    print(\"\\nMain period counts:\")\n    print(intraday[\"period_main\"].value_counts(dropna=False))\nexcept Exception as e:\n    print(\"Sanity checks skipped:\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T17:29:33.61093Z","iopub.execute_input":"2025-09-06T17:29:33.611207Z","iopub.status.idle":"2025-09-06T17:29:44.286791Z","shell.execute_reply.started":"2025-09-06T17:29:33.611183Z","shell.execute_reply":"2025-09-06T17:29:44.285599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Comprehensive data-quality and missingness analysis pipeline\n\nUnified Missingness Pipeline (Qatar edition, percent-aware)\n- Normalized name alignment (cleaned + aliases + percent-aware)\n- Separate Static vs Dynamic outputs\n- Ramadan/Shawwal rollups using visit groups\n- QC conditional formatting (>30 warn, >50 alert)\n- Patient-level missingness per visit\n- Robust dictionary merge (canonical-first, strict percent-safe fallback)\n\nThis Python script is a **comprehensive data-quality and missingness analysis pipeline** tailored for a multi-sheet Excel dataset (like your *final_master_sheet_clean_with_huawei.xlsx*) used in clinical research (e.g., your Ramadan–Shawwal T1D study). Below is a full explanation of its structure and logic:\n\n---\n\n## 🧩 1. Purpose\n\nIt automatically:\n\n* Reads **multiple Excel sheets** (each representing a study visit or phase),\n* Calculates **missing-data statistics** per feature, visit, and patient,\n* Optionally merges a **data dictionary** for metadata categories (e.g., clinical domains),\n* Summarizes **missingness by Ramadan vs. Shawwal**, and\n* Produces a **color-coded Excel report** and CSVs for static, dynamic (intraday), and patient-level missingness.\n\n---\n\n## 📁 2. Input Files\n\n### Required\n\n* `MASTER_PATH` → Excel file containing all study visits.\n\n  * Each sheet = one visit (e.g., *Visit 2, Visit 3, Visit 6 Shawwal*).\n* `PATIENT_ID_COL` → patient identifier column (used for patient-level summaries).\n\n### Optional\n\n* `DICT_PATH` → data dictionary mapping features to categories, units, and definitions.\n* `INTRADAY_CSV_PATH` → Huawei intraday file (continuous wearable data).\n* `DYN_VISIT_OUT` → optional CSV for dynamic missingness per visit.\n\n---\n\n## 🧠 3. Core Functions\n\n### (a) **String Normalization**\n\nFunctions like `_strip_accents`, `clean_name_base`, and `canonical_key` standardize column names:\n\n* Remove accents, punctuation, and spacing.\n* Detect if a variable is percentage-based (e.g., `HbA1c (%)` → `hba1c_pct`).\n* Build a consistent key system to match feature names even with minor formatting differences.\n\nThis ensures reliable merging across sheets and with the data dictionary.\n\n---\n\n### (b) **Missingness Computation**\n\nFor each sheet (visit):\n\n* `percent_missing_by_column(df, visit_name)`\n  → computes % missing values per feature.\n* `missing_counts(df, visit_name)`\n  → counts missing and total entries per feature.\n\nThese are then merged across all visits using Pandas `reduce()`.\n\n---\n\n### (c) **Weighted Overall Missingness**\n\n`weighted_overall_pct()` combines the visit-level counts to yield an overall % missing per feature, weighting by total entries rather than raw averages (so small visits don’t dominate).\n\n---\n\n### (d) **Dictionary Merge**\n\nIf the data dictionary exists:\n\n* Matches each feature to its category, unit, and definition using both:\n\n  1. canonical name (exact match),\n  2. fallback on base name + “is percent” flag.\n* Enriches the report with metadata columns:\n\n  * `Category`, `Definition & Unit`, `Unit/Type`, `Feature Type`, etc.\n\nThis links missingness to clinical domains (e.g., “CGM metrics,” “Lipid profile”).\n\n---\n\n### (e) **Roll-up by Ramadan vs. Shawwal**\n\nGroups visits into `RAMADAN_VISITS` and `SHAWWAL_VISITS`.\nComputes combined % missing within each group:\n\n```python\n% Missing Ramadan (rollup)\n% Missing Shawwal (rollup)\n```\n\n— weighted by total cells.\n\n---\n\n### (f) **Patient-Level Missingness**\n\nFor each visit:\n\n* Calculates per-patient row missingness across all variables.\n* Reports percentage and number of missing cells:\n\n  ```\n  Row % Missing (all fields)\n  Missing Cells\n  Total Cells\n  ```\n\nThis identifies patients with systematically incomplete records.\n\n---\n\n### (g) **Dynamic (Intraday) Missingness**\n\nIf `INCLUDE_INTRADAY=True` and a CSV exists:\n\n* Splits Huawei or CGM data by date ranges:\n\n  * Ramadan: March 23 – April 21 2023\n  * Shawwal: April 22 – May 21 2023\n* For each feature (e.g., HR, steps, SpO₂):\n\n  * % missing per phase + overall.\n* Merges metadata from the dictionary if possible.\n\n---\n\n## 🧾 4. Output Files\n\n| Output                              | Description                                |\n| ----------------------------------- | ------------------------------------------ |\n| **`missingness_report_full.xlsx`**  | Master Excel with conditional formatting.  |\n| **`missingness_static.csv`**        | Visit-level missingness summary.           |\n| **`missingness_dynamic.csv`**       | Intraday (Huawei/CGM) missingness summary. |\n| **`missingness_patient_level.csv`** | Patient-wise missingness per visit.        |\n\n### Excel Sheets in `OUT_XLSX`\n\n* `Missingness_Static` (main summary, color-coded)\n* `Missingness_Dynamic` (if available)\n* `Patient_Level_Missingness`\n* `Unmapped_Static` / `Unmapped_Dynamic` → features not found in dictionary\n* `Name_Mapping_*` → audit of feature–dictionary mappings\n\nConditional formatting flags:\n\n* Yellow if > 30% missing,\n* Red if > 50%.\n\n---\n\n## 📊 5. Key Outputs and Interpretation\n\n| Column                                 | Meaning                                        |\n| -------------------------------------- | ---------------------------------------------- |\n| `% Missing Visit X`                    | Missing fraction per feature per visit.        |\n| `Overall % Missing`                    | Weighted total missingness across all visits.  |\n| `% Missing Ramadan / Shawwal (rollup)` | Aggregated phase-specific missingness.         |\n| `Availability (Visits)`                | JSON list of visits where feature is present.  |\n| `Category`                             | Domain (e.g., “Insulin metrics,” “Lifestyle”). |\n\n---\n\n## ⚙️ 6. Why It’s Useful for Your T1D Ramadan–Shawwal Dataset\n\nThis script ensures:\n\n* **Transparent data completeness tracking** across visits.\n* **Automated integration** with your clinical variable dictionary.\n* **Separate summaries** for:\n\n  * Static features (e.g., HbA1c, BMI),\n  * Dynamic wearable metrics (HR, steps, sleep),\n  * Patient-level data quality.\n* Facilitates decisions like:\n\n  * Which features or visits need imputation,\n  * Which patient records to exclude,\n  * How much data loss differs between Ramadan and Shawwal.\n\n---\n\n### ✅ In Short\n\n> **This code is a reproducible, phase-aware missing-data audit tool** for clinical and wearable datasets, producing publication-ready Excel reports and per-domain summaries that quantify data completeness across visits, patients, and phases (Ramadan vs. Shawwal).\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport json\nimport re\nimport unicodedata\n\n# =============== CONFIG ===============\n# Inputs\nMASTER_PATH = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\nDICT_PATH   = Path(\"/kaggle/input/static-variables/Categorized_Data_Dictionary.xlsx\")  # xlsx or csv\nINTRADAY_CSV_PATH = Path(\"/kaggle/input/intraday/intraday.csv\")  # optional dynamic\n\n# Patient ID column in the master visit sheets (after your mapping step)\nPATIENT_ID_COL = \"PatientID (Huawei Data)\"\n\n# Which sheets to treat as visits (the script will read all except SKIP)\nSKIP_SHEETS = {\"HMC_map_patientID\"}\n\n# Visit groups for rollups (edit as needed)\nRAMADAN_VISITS = {\"Ramadan\", \"Visit 1\", \"Visit 2\", \"Visit 3\", \"Visit 4 (whole Ramadan)\"}\nSHAWWAL_VISITS = {\"Visit 6 (Shawal)\"}  # add Visit 5 here if it belongs to Shawwal\n\n# Intraday options\nINCLUDE_INTRADAY = True\nRAMADAN_START = pd.to_datetime(\"2023-03-23\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-21\")\nSHAWWAL_START = pd.to_datetime(\"2023-04-22\")\nSHAWWAL_END   = pd.to_datetime(\"2023-05-21\")\n# keep patientID to align; exclude only non-feature fields\nINTRADAY_EXCLUDE_COLS = {\"Unnamed: 0\", \"huaweiID\", \"date\", \"start\"}\n\n# Optional stage-2 CSV that (if present) we add as a sheet\nDYN_VISIT_OUT = Path(\"/kaggle/working/missingness_dynamic_by_visit.csv\")\n\n# Outputs\nOUT_XLSX = Path(\"/kaggle/working/missingness_report_full.xlsx\")\nOUT_CSV_STATIC  = Path(\"/kaggle/working/missingness_static.csv\")\nOUT_CSV_DYNAMIC = Path(\"/kaggle/working/missingness_dynamic.csv\")\nOUT_CSV_PATIENT = Path(\"/kaggle/working/missingness_patient_level.csv\")\n\n# QC thresholds (%)\nWARN_THRES = 30.0\nALERT_THRES = 50.0\n# =====================================\n\n\n# ===== Percent-aware normalization =====\ndef _strip_accents(s: str) -> str:\n    s = unicodedata.normalize(\"NFKD\", s)\n    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n\ndef clean_name_base(s: str) -> str:\n    \"\"\"Base alnum key (lower, accents removed, spaces collapsed, punctuation removed).\"\"\"\n    if s is None or (isinstance(s, float) and pd.isna(s)):\n        return \"\"\n    s = str(s).lower().strip()\n    s = _strip_accents(s)\n    s = re.sub(r\"\\s+\", \" \", s)\n    s = re.sub(r\"[^a-z0-9]\", \"\", s)\n    return s\n\ndef has_percent(s: str) -> bool:\n    \"\"\"Detect percent variants like 'HbA1c (%)', 'hba1c%', 'hba1c%%'.\"\"\"\n    if s is None or (isinstance(s, float) and pd.isna(s)):\n        return False\n    s0 = str(s).lower()\n    if \"%\" in s0:\n        return True\n    if \"(%)\" in s0:\n        return True\n    # If you want the word 'percent' to count:\n    # if re.search(r\"\\bpercent\\b\", s0): return True\n    return False\n\ndef canonical_key(raw_name: str) -> str:\n    \"\"\"Percent-safe canonical key: base or base+'_pct'.\"\"\"\n    base = clean_name_base(raw_name)\n    return f\"{base}_pct\" if has_percent(raw_name) else base\n\n# Canonical targets (extend as needed)\nCANON = {\n    \"meals_per_day\": \"meals_per_day\",\n    \"carbs_per_day\": \"carbs_per_day\",\n    \"active_insulin_time_hours\": \"active_insulin_time_hours\",\n    \"average_sg_mgdl\": \"average_sg_mgdl\",\n    \"icr_1\": \"icr_1\",\n    \"icr_2\": \"icr_2\",\n    \"sg_sd_mgdl\": \"sg_sd_mgdl\",\n    \"total_daily_dose_units\": \"total_daily_dose_units\",\n    # example duals for percent/non-percent:\n    \"hba1c\": \"hba1c\",\n    \"hba1c_pct\": \"hba1c_pct\",\n}\n\n# Aliases on canonical_key(raw) -> canonical name\nALIASES = {\n    # meals\n    canonical_key(\"meals\"): \"meals_per_day\",\n    canonical_key(\"meal\"): \"meals_per_day\",\n    canonical_key(\"meals/day\"): \"meals_per_day\",\n    canonical_key(\"meals per day\"): \"meals_per_day\",\n    canonical_key(\"number of meals\"): \"meals_per_day\",\n\n    # carbs\n    canonical_key(\"carb\"): \"carbs_per_day\",\n    canonical_key(\"carbs\"): \"carbs_per_day\",\n    canonical_key(\"carb/day\"): \"carbs_per_day\",\n    canonical_key(\"carbs/day\"): \"carbs_per_day\",\n    canonical_key(\"carbohydrates per day\"): \"carbs_per_day\",\n\n    # specifics\n    canonical_key(\"Active insulin time (hours)\"): \"active_insulin_time_hours\",\n    canonical_key(\"Active insulin time hours\"): \"active_insulin_time_hours\",\n    canonical_key(\"AIT (hours)\"): \"active_insulin_time_hours\",\n\n    canonical_key(\"Average SG mg/dL\"): \"average_sg_mgdl\",\n    canonical_key(\"Avg SG mg/dL\"): \"average_sg_mgdl\",\n    canonical_key(\"Average sensor glucose (mg/dL)\"): \"average_sg_mgdl\",\n\n    canonical_key(\"ICR-1\"): \"icr_1\",\n    canonical_key(\"ICR 1\"): \"icr_1\",\n    canonical_key(\"insulin carb ratio 1\"): \"icr_1\",\n\n    canonical_key(\"ICR-2\"): \"icr_2\",\n    canonical_key(\"ICR 2\"): \"icr_2\",\n    canonical_key(\"insulin carb ratio 2\"): \"icr_2\",\n\n    canonical_key(\"SG SD mg/dL\"): \"sg_sd_mgdl\",\n    canonical_key(\"sensor glucose sd (mg/dL)\"): \"sg_sd_mgdl\",\n\n    canonical_key(\"Total daily dose (Unit)\"): \"total_daily_dose_units\",\n    canonical_key(\"Total daily dose (Units)\"): \"total_daily_dose_units\",\n    canonical_key(\"TDD (units)\"): \"total_daily_dose_units\",\n    canonical_key(\"TDD\"): \"total_daily_dose_units\",\n\n    # HbA1c (non-percent vs percent — DO NOT MERGE)\n    canonical_key(\"HbA1c\"): \"hba1c\",\n    canonical_key(\"HBA1C\"): \"hba1c\",\n    canonical_key(\"HbA1c (%)\"): \"hba1c_pct\",\n    canonical_key(\"hba1c%\"): \"hba1c_pct\",\n    canonical_key(\"hba1c%%\"): \"hba1c_pct\",\n}\n\ndef normalize_to_canonical(raw_name: str) -> str:\n    \"\"\"Map raw -> canonical (percent-safe).\"\"\"\n    key = canonical_key(raw_name)\n    return ALIASES.get(key, key)\n\n\n# =========================\n# ====== HELPERS ==========\n# =========================\ndef _clean_df_strings(df: pd.DataFrame) -> pd.DataFrame:\n    df2 = df.copy()\n    for col in df2.columns:\n        if df2[col].dtype == object:\n            s = df2[col].astype(str).str.strip()\n            s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"NA\": np.nan, \"None\": np.nan})\n            df2[col] = s\n    return df2\n\ndef percent_missing_by_column(df: pd.DataFrame, visit_name: str) -> pd.DataFrame:\n    df2 = _clean_df_strings(df)\n    miss = df2.isna().mean().mul(100).round(2)\n    out = miss.reset_index()\n    out.columns = [\"Feature\", f\"% Missing {visit_name}\"]\n    return out\n\ndef missing_counts(df: pd.DataFrame, visit_name: str) -> pd.DataFrame:\n    df2 = _clean_df_strings(df)\n    total = len(df2)\n    miss = df2.isna().sum()\n    return pd.DataFrame({\n        \"Feature\": miss.index,\n        f\"missing_{visit_name}\": miss.values,\n        f\"total_{visit_name}\": total\n    })\n\ndef availability_for_features(features, visit_dfs):\n    avail = []\n    for feat in features:\n        present_in = [vname for vname, vdf in visit_dfs.items() if feat in vdf.columns]\n        avail.append(present_in)\n    return pd.Series(avail, name=\"Availability (Visits)\")\n\ndef _pick_feature_col(cols):\n    for c in [\"Feature\", \"Feature Name\", \"Variable\", \"Field\", \"Name\"]:\n        if c in cols:\n            return c\n    return None\n\ndef load_dictionary(dict_path: Path) -> pd.DataFrame | None:\n    if not dict_path.exists():\n        print(f\"[INFO] Dictionary not found at {dict_path}. Skipping metadata merge.\")\n        return None\n\n    try:\n        if dict_path.suffix.lower() == \".csv\":\n            raw = pd.read_csv(dict_path)\n        else:\n            xls = pd.ExcelFile(dict_path)\n            selected = None\n            for s in xls.sheet_names:\n                tmp = pd.read_excel(dict_path, sheet_name=s)\n                fcol = _pick_feature_col(tmp.columns)\n                if fcol and \"Category\" in tmp.columns:\n                    selected = tmp.copy()\n                    break\n            if selected is None:\n                selected = pd.read_excel(dict_path)\n            raw = selected\n    except Exception as e:\n        print(f\"[WARN] Failed to read dictionary: {e}. Skipping metadata merge.\")\n        return None\n\n    fcol = _pick_feature_col(raw.columns)\n    if fcol is None:\n        print(\"[WARN] No recognizable feature-name column in dictionary. Skipping metadata merge.\")\n        return None\n\n    df = raw.rename(columns={fcol: \"Feature\"})\n    if \"Definition\" in df.columns and \"Definition & Unit\" not in df.columns:\n        df = df.rename(columns={\"Definition\": \"Definition & Unit\"})\n    keep_cols = [c for c in [\n        \"Feature\", \"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"\n    ] if c in df.columns]\n    if \"Feature\" not in keep_cols or \"Category\" not in keep_cols:\n        print(\"[WARN] Dictionary missing 'Feature' or 'Category' after normalization. Skipping metadata merge.\")\n        return None\n\n    df = df[keep_cols].drop_duplicates()\n    # percent-aware merge keys for the dictionary side\n    df[\"Feature_base\"]      = df[\"Feature\"].map(clean_name_base)\n    df[\"Feature_is_pct\"]    = df[\"Feature\"].map(has_percent)\n    df[\"Feature_canonical\"] = df[\"Feature\"].map(normalize_to_canonical)\n    return df\n\ndef weighted_overall_pct(merged_counts: pd.DataFrame) -> pd.Series:\n    missing_cols = [c for c in merged_counts.columns if c.startswith(\"missing_\")]\n    total_cols   = [c for c in merged_counts.columns if c.startswith(\"total_\")]\n    missing_total = merged_counts[missing_cols].sum(axis=1)\n    total_total   = merged_counts[total_cols].sum(axis=1)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        pct = np.where(total_total > 0, (missing_total / total_total) * 100, np.nan)\n    return np.round(pct, 2)\n\ndef qc_format_excel(writer, sheet_name, warn=WARN_THRES, alert=ALERT_THRES):\n    \"\"\"Add conditional formatting for % columns (>30 yellow, >50 red).\"\"\"\n    try:\n        from openpyxl.styles import PatternFill\n        from openpyxl.formatting.rule import CellIsRule\n        wb = writer.book\n        ws = wb[sheet_name]\n        header = [cell.value for cell in ws[1]]\n        pct_cols = [i for i, h in enumerate(header, start=1) if isinstance(h, str) and \"Missing\" in h]\n        yellow = PatternFill(start_color=\"FFF3CD\", end_color=\"FFF3CD\", fill_type=\"solid\")  # warn\n        red    = PatternFill(start_color=\"F8D7DA\", end_color=\"F8D7DA\", fill_type=\"solid\")  # alert\n        max_row = ws.max_row\n        for col in pct_cols:\n            col_letter = ws.cell(row=1, column=col).column_letter\n            rng = f\"{col_letter}2:{col_letter}{max_row}\"\n            ws.conditional_formatting.add(rng, CellIsRule(operator=\"greaterThan\", formula=[str(warn)], fill=yellow))\n            ws.conditional_formatting.add(rng, CellIsRule(operator=\"greaterThan\", formula=[str(alert)], fill=red))\n    except Exception as e:\n        print(f\"[INFO] QC formatting skipped on {sheet_name}: {e}\")\n\n\n# ------------- Main -------------\ndef main():\n    # Ensure output dir exists\n    OUT_XLSX.parent.mkdir(parents=True, exist_ok=True)\n\n    # Load visit sheets\n    xls = pd.ExcelFile(MASTER_PATH)\n    visit_sheet_names = [s for s in xls.sheet_names if s not in SKIP_SHEETS]\n    visits = {name: pd.read_excel(MASTER_PATH, sheet_name=name) for name in visit_sheet_names}\n\n    # ---- Static: per-visit missingness ----\n    miss_tables = [percent_missing_by_column(df, name) for name, df in visits.items()]\n    from functools import reduce\n    merged_miss = reduce(lambda L, R: pd.merge(L, R, on=\"Feature\", how=\"outer\"), miss_tables) if miss_tables else pd.DataFrame(columns=[\"Feature\"])\n\n    count_tables = [missing_counts(df, name) for name, df in visits.items()]\n    merged_counts = reduce(lambda L, R: pd.merge(L, R, on=\"Feature\", how=\"outer\"), count_tables) if count_tables else pd.DataFrame(columns=[\"Feature\"])\n    for col in merged_counts.columns:\n        if col.startswith(\"missing_\") or col.startswith(\"total_\"):\n            merged_counts[col] = merged_counts[col].fillna(0)\n\n    final_static = merged_miss.copy()\n    if not merged_counts.empty:\n        final_static[\"Overall % Missing\"] = weighted_overall_pct(merged_counts)\n    else:\n        final_static[\"Overall % Missing\"] = np.nan\n\n    final_static[\"Availability (Visits)\"] = availability_for_features(final_static[\"Feature\"], visits).apply(lambda x: json.dumps(x))\n\n    # ---- Merge dictionary (percent-aware) ----\n    dict_core = load_dictionary(DICT_PATH)\n    final_static[\"Feature_base\"]      = final_static[\"Feature\"].map(clean_name_base)\n    final_static[\"Feature_is_pct\"]    = final_static[\"Feature\"].map(has_percent)\n    final_static[\"Feature_canonical\"] = final_static[\"Feature\"].map(normalize_to_canonical)\n\n    if dict_core is not None:\n        # Pass 1: canonical join\n        merged = final_static.merge(\n            dict_core.add_suffix(\"_dict\"),\n            left_on=\"Feature_canonical\",\n            right_on=\"Feature_canonical_dict\",\n            how=\"left\"\n        )\n        # Pass 2 (strict): fallback by (base, is_pct) for unmatched rows\n        need_fb = merged[\"Category_dict\"].isna() if \"Category_dict\" in merged.columns else pd.Series(False, index=merged.index)\n        if need_fb.any():\n            fb_left = final_static.loc[need_fb, [\"Feature\", \"Feature_base\", \"Feature_is_pct\"]].copy()\n            fb_right = dict_core.add_suffix(\"_dict\")[\n                [\"Feature_dict\", \"Feature_base_dict\", \"Feature_is_pct_dict\",\n                 \"Category_dict\", \"Definition & Unit_dict\", \"Unit/Type_dict\", \"Feature Type_dict\", \"Subtype_dict\"]\n            ]\n            fb_joined = fb_left.merge(\n                fb_right,\n                left_on=[\"Feature_base\", \"Feature_is_pct\"],\n                right_on=[\"Feature_base_dict\", \"Feature_is_pct_dict\"],\n                how=\"left\"\n            )\n            idx = merged.index[need_fb]\n            for col in fb_joined.columns:\n                if col.endswith(\"_dict\") and col in merged.columns:\n                    merged.loc[idx, col] = fb_joined[col].values\n\n        # Prefer dictionary metadata where available\n        for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"]:\n            lc, rc = c, f\"{c}_dict\"\n            if lc not in merged.columns:\n                merged[lc] = np.nan\n            if rc in merged.columns:\n                merged[lc] = merged[lc].combine_first(merged[rc])\n\n        merged[\"Matched Dictionary Feature\"] = merged.get(\"Feature_dict\", np.nan)\n        # Cleanup helper cols\n        drop_helpers = [c for c in merged.columns if c.endswith(\"_dict\")] + \\\n                       [\"Feature_base\", \"Feature_base_dict\", \"Feature_is_pct\", \"Feature_is_pct_dict\",\n                        \"Feature_canonical\", \"Feature_canonical_dict\"]\n        final_static = merged.drop(columns=[c for c in drop_helpers if c in merged.columns])\n    else:\n        for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\", \"Matched Dictionary Feature\"]:\n            if c not in final_static.columns:\n                final_static[c] = np.nan\n\n    # Order columns\n    visit_cols = [c for c in final_static.columns if c.startswith(\"% Missing \")]\n    final_static = final_static[\n        [\"Category\", \"Feature\", \"Matched Dictionary Feature\", \"Definition & Unit\",\n         \"Unit/Type\", \"Feature Type\", \"Subtype\"]\n        + visit_cols\n        + [\"Overall % Missing\", \"Availability (Visits)\"]\n    ]\n\n    # ---- Ramadan/Shawwal rollups (static) ----\n    counts_map = {name: missing_counts(df, name).set_index(\"Feature\") for name, df in visits.items()}\n\n    def pooled_group_pct(features, group_visits):\n        if len(features) == 0:\n            return np.array([])\n        miss_total = pd.Series(0, index=features, dtype=float)\n        tot_total  = pd.Series(0, index=features, dtype=float)\n        for v in group_visits:\n            if v not in counts_map:\n                continue\n            mc = counts_map[v]\n            miss_total = miss_total.add(mc.get(f\"missing_{v}\", pd.Series(0, index=features)).reindex(features).fillna(0), fill_value=0)\n            tot_total  = tot_total.add(mc.get(f\"total_{v}\", pd.Series(0, index=features)).reindex(features).fillna(0),   fill_value=0)\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            pct = np.where(tot_total > 0, (miss_total / tot_total) * 100, np.nan)\n        return np.round(pct, 2)\n\n    features_list = final_static[\"Feature\"]\n    final_static[\"% Missing Ramadan (rollup)\"] = pooled_group_pct(features_list, RAMADAN_VISITS)\n    final_static[\"% Missing Shawwal (rollup)\"] = pooled_group_pct(features_list, SHAWWAL_VISITS)\n\n    # ---- Patient-level missingness per visit (row-wise) ----\n    plm_list = []\n    for vname, vdf in visits.items():\n        if PATIENT_ID_COL not in vdf.columns:\n            continue\n        df2 = _clean_df_strings(vdf)\n        cols_eval = [c for c in df2.columns if c != PATIENT_ID_COL]\n        if not cols_eval:\n            continue\n        row_pct = df2[cols_eval].isna().mean(axis=1).mul(100).round(2)\n        tmp = pd.DataFrame({\n            \"Visit\": vname,\n            PATIENT_ID_COL: df2[PATIENT_ID_COL],\n            \"Row % Missing (all fields)\": row_pct,\n            \"Missing Cells\": df2[cols_eval].isna().sum(axis=1),\n            \"Total Cells\": len(cols_eval)\n        })\n        plm_list.append(tmp)\n    patient_level_missing = pd.concat(plm_list, ignore_index=True) if plm_list else pd.DataFrame()\n\n    # ---- Dynamic (intraday) Ramadan/Shawwal ----\n    final_dynamic = pd.DataFrame()\n    if INCLUDE_INTRADAY and INTRADAY_CSV_PATH.exists():\n        try:\n            intraday = pd.read_csv(INTRADAY_CSV_PATH)\n            if \"date\" in intraday.columns:\n                intraday[\"date\"] = pd.to_datetime(intraday[\"date\"], errors=\"coerce\")\n                ram = intraday[(intraday[\"date\"] >= RAMADAN_START) & (intraday[\"date\"] <= RAMADAN_END)]\n                shw = intraday[(intraday[\"date\"] >= SHAWWAL_START) & (intraday[\"date\"] <= SHAWWAL_END)]\n\n                dyn_cols = [c for c in intraday.columns if c not in INTRADAY_EXCLUDE_COLS]\n\n                rows = []\n                for feat in dyn_cols:\n                    r_miss = ram[feat].isna().mean() * 100 if len(ram) else np.nan\n                    s_miss = shw[feat].isna().mean() * 100 if len(shw) else np.nan\n                    both = pd.concat([ram[feat], shw[feat]]) if len(ram) or len(shw) else pd.Series(dtype=float)\n                    overall = both.isna().mean() * 100 if len(both) else np.nan\n                    rows.append({\n                        \"Feature\": feat,\n                        \"% Missing Ramadan\": round(r_miss, 2) if pd.notna(r_miss) else np.nan,\n                        \"% Missing Shawwal\": round(s_miss, 2) if pd.notna(s_miss) else np.nan,\n                        \"Overall % Missing\": round(overall, 2) if pd.notna(overall) else np.nan,\n                        \"Availability (Visits)\": json.dumps([\"Ramadan (intraday)\", \"Shawwal (intraday)\"])\n                    })\n                final_dynamic = pd.DataFrame(rows)\n\n                # Dictionary enrichment (percent-aware)\n                dict_core_dyn = dict_core  # reuse if available\n                if dict_core_dyn is not None and not final_dynamic.empty:\n                    final_dynamic[\"Feature_base\"]      = final_dynamic[\"Feature\"].map(clean_name_base)\n                    final_dynamic[\"Feature_is_pct\"]    = final_dynamic[\"Feature\"].map(has_percent)\n                    final_dynamic[\"Feature_canonical\"] = final_dynamic[\"Feature\"].map(normalize_to_canonical)\n\n                    merged_dyn = final_dynamic.merge(\n                        dict_core_dyn.add_suffix(\"_dict\"),\n                        left_on=\"Feature_canonical\",\n                        right_on=\"Feature_canonical_dict\",\n                        how=\"left\"\n                    )\n                    # strict fallback on (base, is_pct)\n                    need_fb = merged_dyn[\"Category_dict\"].isna() if \"Category_dict\" in merged_dyn.columns else pd.Series(False, index=merged_dyn.index)\n                    if need_fb.any():\n                        fb_left = final_dynamic.loc[need_fb, [\"Feature\", \"Feature_base\", \"Feature_is_pct\"]].copy()\n                        fb_right = dict_core_dyn.add_suffix(\"_dict\")[\n                            [\"Feature_dict\", \"Feature_base_dict\", \"Feature_is_pct_dict\",\n                             \"Category_dict\", \"Definition & Unit_dict\", \"Unit/Type_dict\", \"Feature Type_dict\", \"Subtype_dict\"]\n                        ]\n                        fb_joined = fb_left.merge(\n                            fb_right,\n                            left_on=[\"Feature_base\", \"Feature_is_pct\"],\n                            right_on=[\"Feature_base_dict\", \"Feature_is_pct_dict\"],\n                            how=\"left\"\n                        )\n                        idx = merged_dyn.index[need_fb]\n                        for col in fb_joined.columns:\n                            if col.endswith(\"_dict\") and col in merged_dyn.columns:\n                                merged_dyn.loc[idx, col] = fb_joined[col].values\n\n                    for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"]:\n                        lc, rc = c, f\"{c}_dict\"\n                        if lc not in merged_dyn.columns:\n                            merged_dyn[lc] = np.nan\n                        if rc in merged_dyn.columns:\n                            merged_dyn[lc] = merged_dyn[lc].combine_first(merged_dyn[rc])\n\n                    merged_dyn[\"Matched Dictionary Feature\"] = merged_dyn.get(\"Feature_dict\", np.nan)\n                    drop_helpers = [c for c in merged_dyn.columns if c.endswith(\"_dict\")] + \\\n                                   [\"Feature_base\", \"Feature_base_dict\", \"Feature_is_pct\", \"Feature_is_pct_dict\",\n                                    \"Feature_canonical\", \"Feature_canonical_dict\"]\n                    final_dynamic = merged_dyn.drop(columns=[c for c in drop_helpers if c in merged_dyn.columns])\n                else:\n                    for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\", \"Matched Dictionary Feature\"]:\n                        if c not in final_dynamic.columns:\n                            final_dynamic[c] = np.nan\n            else:\n                print(\"[INFO] intraday has no 'date' column; dynamic missingness skipped.\")\n        except Exception as e:\n            print(f\"[WARN] Intraday integration skipped: {e}\")\n\n    # ---- Save everything to Excel with QC formatting ----\n    with pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\") as writer:\n        # Static (always create at least one sheet to avoid openpyxl 'no visible sheet' error)\n        final_static.to_excel(writer, index=False, sheet_name=\"Missingness_Static\")\n        qc_format_excel(writer, \"Missingness_Static\")\n\n        # Dynamic (Ramadan/Shawwal)\n        if not final_dynamic.empty:\n            dyn_cols_order = [\"Category\", \"Feature\", \"Matched Dictionary Feature\", \"Definition & Unit\",\n                              \"Unit/Type\", \"Feature Type\", \"Subtype\",\n                              \"% Missing Ramadan\", \"% Missing Shawwal\", \"Overall % Missing\", \"Availability (Visits)\"]\n            dyn_cols_order = [c for c in dyn_cols_order if c in final_dynamic.columns] + \\\n                             [c for c in final_dynamic.columns if c not in dyn_cols_order]\n            final_dynamic[dyn_cols_order].to_excel(writer, index=False, sheet_name=\"Missingness_Dynamic\")\n            qc_format_excel(writer, \"Missingness_Dynamic\")\n\n        # Dynamic-by-Visit (optional stage 2 CSV)\n        if DYN_VISIT_OUT.exists():\n            try:\n                dyn_visit_df = pd.read_csv(DYN_VISIT_OUT)\n                dyn_visit_df.to_excel(writer, index=False, sheet_name=\"Missingness_Dynamic_By_Visit\")\n                qc_format_excel(writer, \"Missingness_Dynamic_By_Visit\")\n                print(\"✅ Added sheet: Missingness_Dynamic_By_Visit\")\n            except Exception as e:\n                print(f\"[INFO] Skipped Missingness_Dynamic_By_Visit: {e}\")\n\n        # Patient-level\n        if not patient_level_missing.empty:\n            patient_level_missing.to_excel(writer, index=False, sheet_name=\"Patient_Level_Missingness\")\n\n        # Unmapped audits\n        if \"Category\" in final_static.columns:\n            unmapped_static = (\n                final_static[final_static[\"Category\"].isna()][[\"Feature\"]]\n                .drop_duplicates().sort_values(\"Feature\")\n            )\n            unmapped_static.to_excel(writer, index=False, sheet_name=\"Unmapped_Static\")\n\n        if not final_dynamic.empty and \"Category\" in final_dynamic.columns:\n            unmapped_dyn = (\n                final_dynamic[final_dynamic[\"Category\"].isna()][[\"Feature\"]]\n                .drop_duplicates().sort_values(\"Feature\")\n            )\n            unmapped_dyn.to_excel(writer, index=False, sheet_name=\"Unmapped_Dynamic\")\n\n        # Name mapping audits\n        if \"Matched Dictionary Feature\" in final_static.columns:\n            mapping_static = (\n                final_static[[\"Feature\", \"Matched Dictionary Feature\"]]\n                .drop_duplicates().sort_values([\"Feature\", \"Matched Dictionary Feature\"])\n            )\n            mapping_static.to_excel(writer, index=False, sheet_name=\"Name_Mapping_Static\")\n\n        if not final_dynamic.empty and \"Matched Dictionary Feature\" in final_dynamic.columns:\n            mapping_dyn = (\n                final_dynamic[[\"Feature\", \"Matched Dictionary Feature\"]]\n                .drop_duplicates().sort_values([\"Feature\", \"Matched Dictionary Feature\"])\n            )\n            mapping_dyn.to_excel(writer, index=False, sheet_name=\"Name_Mapping_Dynamic\")\n\n    # CSVs\n    final_static.to_csv(OUT_CSV_STATIC, index=False)\n    if not final_dynamic.empty:\n        final_dynamic.to_csv(OUT_CSV_DYNAMIC, index=False)\n    if not patient_level_missing.empty:\n        patient_level_missing.to_csv(OUT_CSV_PATIENT, index=False)\n\n    print(f\"Saved Excel report → {OUT_XLSX}\")\n    print(f\"Saved static CSV   → {OUT_CSV_STATIC}\")\n    if not final_dynamic.empty:\n        print(f\"Saved dynamic CSV  → {OUT_CSV_DYNAMIC}\")\n    if not patient_level_missing.empty:\n        print(f\"Saved patient CSV  → {OUT_CSV_PATIENT}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T11:45:57.930451Z","iopub.execute_input":"2025-09-06T11:45:57.930746Z","iopub.status.idle":"2025-09-06T11:46:00.106022Z","shell.execute_reply.started":"2025-09-06T11:45:57.930727Z","shell.execute_reply":"2025-09-06T11:46:00.105096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load selected columns from the full sheet\ndf = pd.read_excel(\n    \"/kaggle/working/intraday_with_visits.xlsx\",  # ← change to your real input folder\",  # ← change to your real input folder\n    sheet_name=\"Intraday_All\",\n    usecols=[\"patientID\", \"start\", \"cgm\"]\n)\n\n# Convert start to datetime and filter for Ramadan\ndf[\"start\"] = pd.to_datetime(df[\"start\"], errors=\"coerce\")\ndf = df[(df[\"start\"] >= \"2023-03-22\") & (df[\"start\"] <= \"2023-04-19\")]\n\n# Drop missing rows\ndf = df.dropna(subset=[\"patientID\", \"cgm\"])\n\n# Sample 60,000 rows\ndf_sample = df.sample(n=min(60000, len(df)), random_state=42)\n\n# Save to Excel for later upload\ndf_sample.to_excel(\"/kaggle/working/intraday_ramadan_subset.xlsx\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. static baseline covariates (patient-level features)","metadata":{}},{"cell_type":"markdown","source":"This code analyzes static baseline covariates (patient-level features) from the Excel file final_master_sheet_clean.xlsx.\nIt performs the following steps:\n\nLoads and merges all sheets from the master Excel into one dataset.\n\nCleans column names and automatically detects key baseline features (age, gender, BMI, HbA1C, cholesterol, LDL, HDL, triglycerides, SBP, DBP, eGFR, creatinine, insulin/kg, SmartGuard %).\n\n1. Computes descriptive statistics (mean, median, SD, min, max) for all numeric variables.\n\n2. Calculates a correlation matrix to identify relationships among covariates.\n\n3. Visualizes relationships with a pairplot (scatter and KDE) and a heatmap of correlations.\n\n4. Exports results as two CSV files:\n\n5. Static_Baseline_Descriptive_Stats.csv\n\n6. Static_Baseline_Correlation.csv\n\nIn summary, it provides a full exploratory and correlation analysis of baseline clinical and metabolic variables across all patient visits.\n\n\nThis code performs a **cleaned, patient-level analysis** of static baseline covariates from the Excel file `final_master_sheet_clean.xlsx`.\n\n### 🔍 Step-by-step summary\n\n## 1. **Load all visits**\n\n   * Reads every sheet from the Excel file.\n   * Combines them into one DataFrame, adding a `visit` column to track each visit.\n\n## 2. **Normalize column names**\n\n   * Converts all headers to lowercase and removes extra spaces for consistency.\n\n## 3. **Identify the patient identifier**\n\n   * Automatically detects the column containing patient IDs (e.g., “patientid” or “patient code”).\n\n## 4. **Select only static baseline covariates**\n\n   * Keeps columns related to demographics and lab results such as:\n\n     * Age, gender/sex, BMI\n     * HbA1C\n     * Lipids (cholesterol, LDL, HDL, triglycerides)\n     * Blood pressure (SBP, DBP)\n     * Kidney markers (eGFR, creatinine)\n     * Insulin/kg, SmartGuard %\n\n## 5. **Aggregate to one row per patient**\n\n   * For each patient:\n\n     * Numeric features → median value across visits\n     * Categorical features (like gender) → most frequent value (mode)\n\n##  6. **Descriptive statistics**\n\n   * Prints summary metrics (mean, median, standard deviation, etc.) for all baseline variables.\n\n## 7. **Correlation heatmap**\n\n   * Computes correlations among numeric features (e.g., LDL vs. cholesterol, eGFR vs. creatinine).\n   * Displays them visually using a color-coded heatmap.\n\n---\n\n### ✅ **In short**\n\nThe script consolidates all visit data into a **single baseline record per patient**, summarizes their demographic and clinical characteristics, and visualizes **inter-variable correlations** to understand relationships among core metabolic and renal covariates.\n","metadata":{}},{"cell_type":"code","source":"# Study the Static Baseline Covariates (patient-level) from the master Excel file.\n# We'll compute descriptive statistics, correlations, and visualize relationships.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Load the uploaded Excel\nfile_path = \"/kaggle/input/static-variables/final_master_sheet_clean.xlsx\"\n\n# Read all sheets\nxls = pd.ExcelFile(file_path)\nsheets = xls.sheet_names\nprint(\"Sheets found:\", sheets)\n\n# Aggregate all visits into one DataFrame for static features\ndfs = []\nfor s in sheets:\n    df = pd.read_excel(file_path, sheet_name=s)\n    df[\"visit_sheet\"] = s\n    dfs.append(df)\n\ndf_all = pd.concat(dfs, ignore_index=True)\n\n# Normalize column names for consistency\ndf_all.columns = df_all.columns.str.strip().str.lower()\n\n# Identify key features (match by lowercase name fragments)\nfeatures = {\n    \"age\": [c for c in df_all.columns if \"age\" in c],\n    \"gender\": [c for c in df_all.columns if \"gender\" in c or \"sex\" in c],\n    \"bmi\": [c for c in df_all.columns if \"bmi\" in c],\n    \"hba1c\": [c for c in df_all.columns if \"hba1\" in c],\n    \"cholesterol\": [c for c in df_all.columns if \"cholesterol\" in c],\n    \"ldl\": [c for c in df_all.columns if \"ldl\" in c],\n    \"hdl\": [c for c in df_all.columns if \"hdl\" in c],\n    \"triglyceride\": [c for c in df_all.columns if \"trig\" in c],\n    \"sbp\": [c for c in df_all.columns if \"sbp\" in c or \"systolic\" in c],\n    \"dbp\": [c for c in df_all.columns if \"dbp\" in c or \"diastolic\" in c],\n    \"egfr\": [c for c in df_all.columns if \"egfr\" in c],\n    \"creatinine\": [c for c in df_all.columns if \"creat\" in c],\n    \"insulin_kg\": [c for c in df_all.columns if \"insulin\" in c and \"kg\" in c],\n    \"smartguard\": [c for c in df_all.columns if \"smartguard\" in c],\n}\n\n# Flatten to a feature list and extract relevant subset\nselected_cols = [col[0] for col in features.values() if len(col) > 0]\nstatic_df = df_all[selected_cols].copy()\n\n# Descriptive statistics\ndesc = static_df.describe(include=\"all\").T\n\n# Correlation matrix for numeric columns\nnumeric_df = static_df.select_dtypes(include=[np.number])\ncorr = numeric_df.corr()\n\n# --- Visualization ---\nsns.set(style=\"whitegrid\", context=\"notebook\")\n\n# 1. Pairplot for main features\npair_cols = [\"bmi\", \"hba1c\", \"cholesterol\", \"ldl\", \"hdl\", \"triglyceride\", \"egfr\", \"creatinine\"]\npair_cols = [c for c in pair_cols if c in numeric_df.columns]\nsns.pairplot(static_df[pair_cols], diag_kind=\"kde\")\nplt.suptitle(\"Static Baseline Covariates Relationships\", y=1.02)\nplt.tight_layout()\nplt.show()\n\n# 2. Correlation heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'Correlation'})\nplt.title(\"Correlation Heatmap – Static Baseline Covariates\")\nplt.tight_layout()\nplt.show()\n\n# Save outputs\nout_dir = Path(\"/kaggle/working/static_baseline_analysis\")\nout_dir.mkdir(parents=True, exist_ok=True)\ndesc_path = out_dir / \"Static_Baseline_Descriptive_Stats.csv\"\ncorr_path = out_dir / \"Static_Baseline_Correlation.csv\"\n\ndesc.to_csv(desc_path)\ncorr.to_csv(corr_path)\n\ndesc.head(), corr.head(), {\"descriptive_csv\": str(desc_path), \"correlation_csv\": str(corr_path)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:41:31.392978Z","iopub.execute_input":"2025-10-15T19:41:31.393431Z","iopub.status.idle":"2025-10-15T19:41:50.253090Z","shell.execute_reply.started":"2025-10-15T19:41:31.393395Z","shell.execute_reply":"2025-10-15T19:41:50.252242Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n\n# --- Load the Excel ---\nxls = pd.ExcelFile(\"/kaggle/input/static-variables/final_master_sheet_clean.xlsx\")\n\n# Combine all visits (so we can take per-patient medians later)\ndf_all = pd.concat([pd.read_excel(xls, s).assign(visit=s) for s in xls.sheet_names], ignore_index=True)\n\n# Normalize column names\ndf_all.columns = df_all.columns.str.strip().str.lower()\n\n# Identify the patient ID column\npid_col = [c for c in df_all.columns if \"patient\" in c or \"code\" in c][0]\n\n# --- Select only static baseline covariates ---\nkeep_patterns = [\n    \"age\", \"gender\", \"sex\", \"bmi\", \"hba1\", \"cholesterol\", \"ldl\", \"hdl\", \"trig\",\n    \"sbp\", \"dbp\", \"egfr\", \"creat\", \"insulin\", \"smartguard\"\n]\ncols = [c for c in df_all.columns if any(p in c for p in keep_patterns)]\nstatic_df = df_all[[pid_col] + cols].copy()\n\n# --- Aggregate to one row per patient (median for numeric, mode for gender) ---\nstatic_df = static_df.groupby(pid_col).agg(lambda x: pd.to_numeric(x, errors=\"coerce\").median(skipna=True)\n                                           if np.issubdtype(x.dtype, np.number) or str(x.dtype).startswith(\"float\")\n                                           else x.mode().iloc[0] if not x.mode().empty else x.iloc[0]).reset_index()\n\nprint(\"Number of unique patients:\", static_df.shape[0])\n\n# --- Descriptive statistics ---\ndesc = static_df.describe(include='all').T\nprint(\"\\nDescriptive summary:\\n\", desc)\n\n# --- Correlation heatmap for numeric features ---\nnum_df = static_df.select_dtypes(include=[np.number])\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(num_df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'Correlation'})\nplt.title(\"Static Baseline Covariates – Unique Patients (n ≈ 33–35)\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:47:44.728461Z","iopub.execute_input":"2025-10-15T19:47:44.728912Z","iopub.status.idle":"2025-10-15T19:47:46.975349Z","shell.execute_reply.started":"2025-10-15T19:47:44.728858Z","shell.execute_reply":"2025-10-15T19:47:46.974062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Dynamic Features during Ramadan Visits (Visits 2–5) f","metadata":{}},{"cell_type":"markdown","source":"This script extracts and summarizes the **dynamic features during Ramadan visits (Visits 2–5)** from your `final_master_sheet_clean.xlsx`.\n\nHere’s what it does, step by step:\n\n---\n\n## 🧭 **1. Load and Filter Ramadan Visits**\n\n* Reads every sheet in the Excel file.\n* Keeps only those whose sheet names include **2, 3, 4, 5** (Ramadan visits).\n* Combines them into one DataFrame with a `visit` column.\n\n```python\nRamadan sheets: ['Visit 2', 'Visit 3', 'Visit 4', 'Visit 5']\n```\n\n---\n\n## 🧹 **2. Normalize Column Names**\n\nAll column names are converted to lowercase and stripped of spaces so matching is consistent.\n\n---\n\n## ⚙️ **3. Define Feature Groups**\n\nDefines three clinical domains (based on your 1.3 variable specification):\n\n| Domain               | Example Features Detected                                          |\n| -------------------- | ------------------------------------------------------------------ |\n| **Glycemia**         | `TIR 70–180`, `Total T<70`, `SG SD`, `CV %`, `GRI`                 |\n| **Insulin / Device** | `Total Daily Dose`, `Bolus %`, `Auto Basal %`, `Auto Correction %` |\n| **Lifestyle**        | `Meals/Day`, `Carb/Day`, `Fasting %`, `Fasting Days`               |\n\nThe helper function `find_columns()` scans column names and collects all matches for these patterns.\n\n\n### ✅ **In short**\n\n> This code isolates and prepares **Ramadan-phase dynamic clinical and behavioral variables** (glycemia, insulin/device metrics, and lifestyle factors) for downstream modeling—ensuring clean numeric formatting, per-visit structure, and quick statistical and correlation summaries.\n\nHere’s a **comprehensive, unified summary** of all the major steps — combining the three scripts into a single clear workflow overview 👇\n\n---\n\n## 🌙 **Ramadan Dynamic Feature Extraction, Cleaning, and Modeling Pipeline**\n\nThis complete workflow extracts, prepares, and models **dynamic clinical and behavioral features** from Ramadan visits (Visits 2–5) in the study dataset `final_master_sheet_clean.xlsx`.\nIt transforms raw visit-level data into a **clean, numeric, and modeling-ready dataset** and uses machine learning to predict **hypoglycemia burden (Total T<70%)** during Ramadan.\n\n---\n\n### 🧭 **1️⃣ Extract and Combine Ramadan Visits**\n\n* Reads all sheets in the master Excel file.\n* Keeps only **Visits 2, 3, 4, and 5** (Ramadan phase).\n* Merges them into one unified DataFrame and adds a `visit` column.\n  👉 *Purpose:* Consolidate all Ramadan data across patients into one dataset.\n\n---\n\n### 🧹 **2️⃣ Clean and Normalize Columns**\n\n* Converts all column names to lowercase and removes spaces or symbols.\n* Ensures consistent column matching across visits regardless of formatting differences.\n  👉 *Purpose:* Standardize variable names for accurate identification.\n\n---\n\n### ⚙️ **3️⃣ Define and Identify Dynamic Feature Groups**\n\nAutomatically detects features belonging to three major **clinical domains**:\n\n| Domain             | Example Features                                                   | Description                                             |\n| :----------------- | :----------------------------------------------------------------- | :------------------------------------------------------ |\n| **Glycemia**       | `TIR 70–180`, `Total T<70`, `SG SD`, `CV %`, `GRI`                 | Continuous glucose and variability metrics.             |\n| **Insulin/Device** | `Total Daily Dose`, `Bolus %`, `Auto Basal %`, `Auto Correction %` | Insulin delivery and pump metrics.                      |\n| **Lifestyle**      | `Meals/Day`, `Carb/Day`, `Fasting %`, `Fasting Days`               | Meal frequency, carbohydrate intake, fasting adherence. |\n\nA helper function scans all columns and collects matches for each group automatically.\n👉 *Purpose:* Automatically categorize clinical and behavioral variables for analysis.\n\n---\n\n### 🔍 **4️⃣ Select, Clean, and Save Dynamic Features**\n\n* Automatically identifies the **patient ID** column.\n* Keeps only patient ID, visit, and matched dynamic variables.\n* Converts all numeric-like columns to numeric dtype.\n* Saves the clean Ramadan dynamic dataset to:\n\n  ```\n  /kaggle/working/Dynamic_Features_Ramadan.csv\n  ```\n\n👉 *Purpose:* Produce a structured, numeric dataset per patient/visit for statistical and modeling tasks.\n\n---\n\n### 📊 **5️⃣ Descriptive Statistics and Correlations**\n\n* Generates descriptive summaries (count, mean, SD, min, quartiles, max).\n* Computes a correlation matrix between all dynamic variables to check inter-feature relationships.\n  👉 *Purpose:* Understand data distribution and detect collinearity between variables.\n\n---\n\n### 🧩 **6️⃣ Feature Selection for Modeling**\n\n* Groups features into:\n\n  * **Keep** → Insulin/Device + Lifestyle\n  * **Drop** → Glycemia (to prevent information leakage into glycemic outcomes).\n* Visualizes these decisions with a color-coded scatterplot (green = keep, red = drop).\n* Exports a refined modeling dataset:\n\n  ```\n  /kaggle/working/Dynamic_Features_Ramadan_Clean.csv\n  ```\n\n👉 *Purpose:* Retain only independent behavioral and insulin-related predictors for modeling.\n\n---\n\n### 🤖 **7️⃣ Machine Learning Modeling (Hypoglycemia Prediction)**\n\nTwo ensemble regressors are trained to predict **Total T<70 %** (time below 70 mg/dL):\n\n| Model                       | Key Parameters                                 | Evaluation Metrics                                         |\n| :-------------------------- | :--------------------------------------------- | :--------------------------------------------------------- |\n| **Random Forest Regressor** | 300 trees, random state = 42                   | R² and RMSE on test data; feature importance = split gain. |\n| **XGBoost Regressor**       | 300 trees, learning rate = 0.05, max depth = 3 | R² and RMSE; feature importance = gradient gain.           |\n\n* Data split: 75 % train / 25 % test.\n* Only numeric variables are used.\n* Both models report:\n\n  * **R²** → fraction of explained variance.\n  * **RMSE** → prediction error magnitude.\n* Feature importances are ranked and visualized side-by-side for RF vs XGB.\n  👉 *Purpose:* Evaluate how well insulin and lifestyle patterns explain hypoglycemia risk and identify top predictive features.\n\n---\n\n### 📈 **8️⃣ Outputs**\n\n| Output                                               | Description                                                                     |\n| :--------------------------------------------------- | :------------------------------------------------------------------------------ |\n| `/kaggle/working/Dynamic_Features_Ramadan.csv`       | Full Ramadan dynamic dataset.                                                   |\n| `/kaggle/working/Dynamic_Features_Ramadan_Clean.csv` | Modeling-ready subset (insulin + lifestyle features).                           |\n| Model Metrics                                        | R² and RMSE for Random Forest and XGBoost.                                      |\n| Feature Importance Plots                             | Top predictors of hypoglycemia (e.g., Bolus %, Auto Basal %, Total Daily Dose). |\n\n---\n\n## ✅ **Overall Summary**\n\n> The complete pipeline extracts Ramadan-phase data from multi-visit Excel sheets, standardizes and classifies dynamic variables into glycemia, insulin/device, and lifestyle domains, selects relevant predictors, and trains Random Forest and XGBoost models to estimate **hypoglycemia exposure (Total T<70 %)**.\n> It produces clean, interpretable data, quantitative summaries, and feature-importance insights for subsequent clinical interpretation and modeling.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np\n\n# --- Load workbook with all visits ---\nxls = pd.ExcelFile(\"/kaggle/input/static-variables/final_master_sheet_clean.xlsx\")\n\n# --- Restrict to Ramadan visits (2–5) ---\nramadan_sheets = [s for s in xls.sheet_names if any(str(i) in s for i in [2,3,4,5])]\nprint(\"Ramadan sheets:\", ramadan_sheets)\n\n# Combine visits into one DataFrame\nramadan_df = pd.concat([pd.read_excel(xls, s).assign(visit=s) for s in ramadan_sheets], ignore_index=True)\n\n# Normalize column names\nramadan_df.columns = ramadan_df.columns.str.strip().str.lower()\n\n# --- Define feature groups based on your 1.3 spec ---\nglycemia_vars = [\"tir 70 - 180\", \"tir70-180\", \"tir_70_180\", \"total t<70\", \"t<70\",\n                 \"sg sd\", \"sgsd\", \"coefficient of variation\", \"coefficientofvariation\", \"gri\"]\n\ninsulin_device_vars = [\"total daily dose\", \"tdd\", \"bolus %\", \"bolus%\", \n                       \"auto basal %\", \"autobasal%\", \"auto correction %\", \"autocorrection%\"]\n\nlifestyle_vars = [\"meals/day\", \"meals\", \"carb/day\", \"carb\", \"fasting %\", \"fasting days\"]\n\n# Helper: find first matching column per pattern\ndef find_columns(df, patterns):\n    found = []\n    for c in df.columns:\n        for p in patterns:\n            if p.replace(\" \", \"\") in c.replace(\" \", \"\"):\n                found.append(c)\n                break\n    return list(dict.fromkeys(found))  # unique, preserve order\n\ngly_cols = find_columns(ramadan_df, glycemia_vars)\ninsulin_cols = find_columns(ramadan_df, insulin_device_vars)\nlife_cols = find_columns(ramadan_df, lifestyle_vars)\n\ndynamic_cols = gly_cols + insulin_cols + life_cols\nprint(\"Selected dynamic feature columns:\", dynamic_cols)\n\n# --- Build final modeling DataFrame ---\npid_col = [c for c in ramadan_df.columns if \"patient\" in c or \"code\" in c][0]\ndyn_df = ramadan_df[[pid_col, \"visit\"] + dynamic_cols].copy()\n\n# Convert all numeric features to numeric dtype\nfor c in dynamic_cols:\n    dyn_df[c] = pd.to_numeric(dyn_df[c], errors=\"coerce\")\n\n# --- Descriptive summary & correlation check ---\ndesc = dyn_df[dynamic_cols].describe().T\ncorr = dyn_df[dynamic_cols].corr()\n\nprint(\"\\nDynamic feature descriptive summary (median ± IQR):\\n\", desc)\nprint(\"\\nPairwise correlations:\\n\", corr.round(2))\n\n# --- Optional: save for modeling ---\nout_path = \"/kaggle/working/Dynamic_Features_Ramadan.csv\"\ndyn_df.to_csv(out_path, index=False)\nprint(f\"\\nSaved Ramadan dynamic feature dataset → {out_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:02:43.427312Z","iopub.execute_input":"2025-10-15T20:02:43.427705Z","iopub.status.idle":"2025-10-15T20:02:43.605355Z","shell.execute_reply.started":"2025-10-15T20:02:43.427678Z","shell.execute_reply":"2025-10-15T20:02:43.604177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Load your Ramadan dynamic dataset\ndf = pd.read_csv(\"/kaggle/working/Dynamic_Features_Ramadan.csv\")\n\n# --- Define Glycemia subset ---\nglycemia = [\"tir 70 - 180\", \"total t<70\", \"sg sd\", \"coefficient of variation\", \"gri\"]\nglycemia = [g for g in glycemia if g in df.columns]\n\n# Drop rows with all NaN in glycemia block\ngly_df = df[glycemia].dropna(how='all')\n\n# --- 1️⃣ Correlation Heatmap ---\nplt.figure(figsize=(7,5))\nsns.heatmap(gly_df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, cbar_kws={'label':'Correlation'})\nplt.title(\"Correlation Heatmap – Glycemia Features (Ramadan)\")\nplt.tight_layout()\nplt.show()\n\n# --- 2️⃣ VIF (Variance Inflation Factor) ---\n# Standardize numeric matrix\nX = StandardScaler().fit_transform(gly_df.dropna())\nvif_df = pd.DataFrame()\nvif_df[\"Feature\"] = glycemia\nvif_df[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\nprint(\"\\nVariance Inflation Factors (VIF):\\n\", vif_df)\n\n# --- 3️⃣ Diagram / annotation of keep vs drop ---\n# Rule: drop features with |r| > 0.9 OR VIF > 10\ncorr_mat = gly_df.corr().abs()\nupper = corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n\n# Highlight decisions\nkeep_features = [f for f in glycemia if f not in to_drop]\ndrop_features = to_drop\n\nprint(\"\\n🧮 Features to KEEP:\", keep_features)\nprint(\"🚫 Features to DROP (highly redundant):\", drop_features)\n\n# --- 4️⃣ Diagrammatic summary ---\nfig, ax = plt.subplots(figsize=(6,4))\nsns.scatterplot(x=[\"keep\"]*len(keep_features)+[\"drop\"]*len(drop_features),\n                y=keep_features+drop_features,\n                hue=[\"Keep\"]*len(keep_features)+[\"Drop\"]*len(drop_features),\n                palette={\"Keep\":\"green\",\"Drop\":\"red\"}, s=200)\nfor i, f in enumerate(keep_features+drop_features):\n    ax.text(-0.1 if i < len(keep_features) else 0.9, i, f, va=\"center\", fontsize=10)\nax.set_title(\"Feature Selection Decision – Glycemia Features (Ramadan)\")\nax.set_xlabel(\"Decision Category\")\nax.set_ylabel(\"\")\nax.legend(title=\"Status\", loc=\"best\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:10:38.353651Z","iopub.execute_input":"2025-10-15T20:10:38.354099Z","iopub.status.idle":"2025-10-15T20:10:39.390150Z","shell.execute_reply.started":"2025-10-15T20:10:38.354068Z","shell.execute_reply":"2025-10-15T20:10:39.388224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n\n# Load the Ramadan dataset\ndf = pd.read_csv(\"/kaggle/working/Dynamic_Features_Ramadan.csv\")\n\n# Define feature categories\nglycemia_features = [\"tir 70 - 180\", \"total t<70\", \"sg sd\", \"coefficient of variation\", \"gri\"]\ninsulin_device_features = [\"total daily dose (u)\", \"bolus %\", \"auto basal %\", \"auto correction %\"]\nlifestyle_features = [\"meals\", \"carb\", \"fasting % (out of 29 days)\"]\n\n# Keep only the features that exist in your dataset\nkeep_features = [f for f in insulin_device_features + lifestyle_features if f in df.columns]\ndrop_features = [f for f in glycemia_features if f in df.columns]\n\n# Assign group labels only for features that exist\ngroup_map = {}\nfor f in keep_features:\n    group_map[f] = \"Insulin/Device\" if f in insulin_device_features else \"Lifestyle/Meals\"\nfor f in drop_features:\n    group_map[f] = \"Glycemia\"\n\n# Build the decision DataFrame\nselection_df = pd.DataFrame({\n    \"Feature\": keep_features + drop_features,\n    \"Group\": [group_map[f] for f in keep_features + drop_features],\n    \"Decision\": [\"Keep\"] * len(keep_features) + [\"Drop\"] * len(drop_features)\n})\n\n# --- Diagram ---\nplt.figure(figsize=(7, 5))\nsns.scatterplot(\n    data=selection_df,\n    x=\"Decision\", y=\"Feature\",\n    hue=\"Decision\", style=\"Group\",\n    palette={\"Keep\": \"green\", \"Drop\": \"red\"},\n    s=200\n)\nplt.title(\"Feature Selection Decision – Ramadan Dynamic Features\")\nplt.xlabel(\"Decision Category\")\nplt.ylabel(\"\")\nplt.legend(title=\"Decision / Group\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n# --- Export clean dataset ---\npid_cols = [c for c in df.columns if \"patient\" in c or \"code\" in c] + [\"visit\"]\ndf_clean = df[pid_cols + keep_features]\nout_path = \"/kaggle/working/Dynamic_Features_Ramadan_Clean.csv\"\ndf_clean.to_csv(out_path, index=False)\n\nprint(\"✅ Kept features for modeling:\", keep_features)\nprint(\"🚫 Dropped glycemia features:\", drop_features)\nprint(f\"Cleaned feature dataset saved → {out_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:38:03.453717Z","iopub.execute_input":"2025-10-15T20:38:03.454114Z","iopub.status.idle":"2025-10-15T20:38:03.791160Z","shell.execute_reply.started":"2025-10-15T20:38:03.454089Z","shell.execute_reply":"2025-10-15T20:38:03.790013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# --- Load your clean Ramadan dataset ---\ndf = pd.read_csv(\"/kaggle/working/Dynamic_Features_Ramadan_Clean.csv\")\n\n# Load original dataset to bring back target (total T<70)\norig = pd.read_csv(\"/kaggle/working/Dynamic_Features_Ramadan.csv\")\n\n# Merge so we can predict total T<70 %\ntarget_col = \"total t<70\"\ndf = pd.merge(df, orig[[target_col]], left_index=True, right_index=True, how=\"left\")\n\n# Drop rows with any missing values (NaN)\ndf = df.dropna()\n\n# --- Define features (X) and target (y) ---\nX = df.drop(columns=[target_col])\ny = df[target_col]\n\n# Keep only numeric columns for modeling\nX = X.select_dtypes(include=[np.number])\n\n# Split data (train/test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# --- 1️⃣ Random Forest Regressor ---\nrf = RandomForestRegressor(n_estimators=300, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict & evaluate\ny_pred_rf = rf.predict(X_test)\nprint(f\"Random Forest R²: {r2_score(y_test, y_pred_rf):.3f} | RMSE: {mean_squared_error(y_test, y_pred_rf, squared=False):.3f}\")\n\n# Feature importance\nfi_rf = pd.DataFrame({\"Feature\": X.columns, \"Importance\": rf.feature_importances_}).sort_values(\"Importance\", ascending=False)\nprint(\"\\nRandom Forest Feature Importance:\\n\", fi_rf)\n\n# --- 2️⃣ XGBoost Regressor ---\nxgb = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=3, random_state=42)\nxgb.fit(X_train, y_train)\n\n# Predict & evaluate\ny_pred_xgb = xgb.predict(X_test)\nprint(f\"\\nXGBoost R²: {r2_score(y_test, y_pred_xgb):.3f} | RMSE: {mean_squared_error(y_test, y_pred_xgb, squared=False):.3f}\")\n\n# Feature importance\nfi_xgb = pd.DataFrame({\"Feature\": X.columns, \"Importance\": xgb.feature_importances_}).sort_values(\"Importance\", ascending=False)\nprint(\"\\nXGBoost Feature Importance:\\n\", fi_xgb)\n\n# --- 📊 Plot both importance sets ---\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nsns.barplot(data=fi_rf, x=\"Importance\", y=\"Feature\", ax=axes[0], palette=\"Blues_d\")\naxes[0].set_title(\"Random Forest – Feature Importance\")\n\nsns.barplot(data=fi_xgb, x=\"Importance\", y=\"Feature\", ax=axes[1], palette=\"Greens_d\")\naxes[1].set_title(\"XGBoost – Feature Importance\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T20:49:00.694429Z","iopub.execute_input":"2025-10-15T20:49:00.694802Z","iopub.status.idle":"2025-10-15T20:49:01.894482Z","shell.execute_reply.started":"2025-10-15T20:49:00.694775Z","shell.execute_reply":"2025-10-15T20:49:01.893329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np, re, unicodedata\nfrom pathlib import Path\n\n# ========= CONFIG =========\nIN_XLSX  = Path(\"/kaggle/working/final_master_sheet_clean_with_visits.xlsx\")\nOUT_XLSX = Path(\"/kaggle/working/outcomes_split_static_vs_visit.xlsx\")\nOUT_STATIC_CSV      = Path(\"/kaggle/working/outcome_static.csv\")\nOUT_VISIT_LONG_CSV  = Path(\"/kaggle/working/outcome_visit_long.csv\")\nOUT_VISIT_WIDE_CSV  = Path(\"/kaggle/working/outcome_visit_wide_by_variable.csv\")\n\n# Visit order preference (anything not found falls to the end)\nVISIT_PRIORITY = [\n    \"Visit 1\", \"Ramadan\", \"Visit 2\", \"Visit 3\", \"Visit 4 (whole Ramadan)\",\n    \"Visit 5\", \"Visit 6 (Shawal)\", \"Visit 7\"\n]\n\n# Sheets we shouldn't treat as visits\nSKIP_SHEETS = {\"HMC_map_patientID\", \"Visit_Subperiods_Spec\", \"Period_Bounds\"}\n\n# ========= LOAD ALL VISIT SHEETS =========\nxls = pd.ExcelFile(IN_XLSX)\nvisit_sheets = [s for s in xls.sheet_names if s not in SKIP_SHEETS]\nif not visit_sheets:\n    raise RuntimeError(\"No visit-like sheets found. Check SKIP_SHEETS or input file.\")\n\ndfs = []\nfor s in visit_sheets:\n    df = pd.read_excel(IN_XLSX, sheet_name=s)\n    df[\"Visit\"] = s\n    dfs.append(df)\n\nall_df = pd.concat(dfs, ignore_index=True)\n# Keep original header case in the output; just trim whitespace\nall_df.columns = [str(c).strip() for c in all_df.columns]\n\n# Find a patient-id column robustly\nPID_CANDIDATES = [\n    \"PatientID (Huawei Data)\", \"patientID\", \"patientid\", \"Patient ID\",\n    \"patient id\", \"Code\", \"code\"\n]\npid_col = next((c for c in PID_CANDIDATES if c in all_df.columns), None)\nif pid_col is None:\n    lower = {c.lower(): c for c in all_df.columns}\n    pid_col = next((lower[k] for k in lower if \"patient\" in k or k == \"code\"), None)\nif pid_col is None:\n    raise ValueError(\"Could not find a patient ID column. Add it to PID_CANDIDATES.\")\n\n# Visit order for choosing baseline static values\ndef visit_rank(v):\n    return VISIT_PRIORITY.index(v) if v in VISIT_PRIORITY else len(VISIT_PRIORITY) + 99\nall_df[\"__visit_order__\"] = all_df[\"Visit\"].map(visit_rank)\n\n# ========= YOUR EXACT OUTPUT COLUMNS =========\nVISIT_TARGETS = [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_percent_29\"]\nSTATIC_TARGETS = [\n    \"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\n    \"Triglycerides\",\"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"\n]\n\n# ========= ALIAS RESOLUTION (maps messy headers → canonical names) =========\ndef _norm(s: str) -> str:\n    s = unicodedata.normalize(\"NFKD\", str(s)).lower().strip()\n    s = re.sub(r\"\\s+\", \" \", s)\n    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n    return s\n\nALIASES = {\n    # --- VISIT VARS ---\n    \"carb\": [\n        \"carb\",\"carbs\",\"carb/day\",\"carbs/day\",\"carbohydrate\",\"carbohydrates\",\"carbohydrates per day\"\n    ],\n    \"meals\": [\n        \"meals\",\"meals/day\",\"number of meals\",\"meals per day\",\"meal count\"\n    ],\n    \"total_daily_dose_u\": [\n        \"total daily dose (u)\",\"total daily dose (units)\",\"total daily dose (unit)\",\n        \"total daily dose\",\"tdd\",\"tdd (units)\",\"tdd units\",\"tdd u\"\n    ],\n    \"fasting_percent_29\": [\n        \"fasting % (out of 29 days)\",\"fasting percent (out of 29 days)\",\n        \"fasting %\",\"fasting percent\",\"fasting pct\",\"fastingpct\",\"fasting%\",\"fastingpercent29\",\n        \"fasting % out of 29\"\n    ],\n\n    # --- STATIC VARS ---\n    \"Age\": [\"age\",\"age (years)\",\"age years\"],\n    \"Gender\": [\"gender\",\"gender (m / f)\",\"sex\"],\n    \"BMI\": [\"bmi\"],\n    \"HbA1C\": [\"hba1c\",\"hba1c %\",\"hba1c%\"],\n    \"Cholesterol\": [\"cholesterol\",\"total cholesterol\"],\n    \"LDL\": [\"ldl\",\"ldl-c\",\"ldl cholesterol\"],\n    \"HDL\": [\"hdl\",\"hdl-c\",\"hdl cholesterol\"],\n    \"Triglycerides\": [\"triglyceride\",\"triglycerides\",\"tg\"],\n    \"eGFR\": [\"egfr\",\"estimated gfr\",\"e-gfr\"],\n    \"Creatinine\": [\"creatinine\",\"serum creatinine\",\"creat\"],\n    \"Insulin_units_per_kg\": [\"insulin units/kg\",\"insulin/kg\",\"insulin per kg\",\"insulinunitskg\"],\n    \"SmartGuard_percent\": [\"smartguard %\",\"smartguard%\",\"smartguard percent\",\"smartguardpct\",\"smartguard\"]\n}\n\n# Build normalized lookup from actual columns\nnorm_to_original = {}\nfor c in all_df.columns:\n    norm_to_original.setdefault(_norm(c), c)\n\ndef resolve_one(canonical: str) -> str | None:\n    # Try exact\n    hit = norm_to_original.get(_norm(canonical))\n    if hit: return hit\n    # Try aliases\n    for alias in ALIASES.get(canonical, []):\n        hit = norm_to_original.get(_norm(alias))\n        if hit: return hit\n    return None\n\nresolved_visit = {canon: resolve_one(canon) for canon in VISIT_TARGETS}\nresolved_static = {canon: resolve_one(canon) for canon in STATIC_TARGETS}\n\nmissing_visit = [k for k, v in resolved_visit.items() if v is None]\nmissing_static = [k for k, v in resolved_static.items() if v is None]\n\nprint(\"— Column resolution —\")\nprint(\"Visit vars:\")\nfor k, v in resolved_visit.items():\n    print(f\"  {k:>22}  ←  {v if v else '[MISSING]'}\")\nprint(\"Static vars:\")\nfor k, v in resolved_static.items():\n    print(f\"  {k:>22}  ←  {v if v else '[MISSING]'}\")\n\n# ========= BUILD STATIC (one row per patient) =========\nsorted_df = all_df.sort_values(\"__visit_order__\")\nstatic_src_cols = [v for v in resolved_static.values() if v is not None]\nstatic_block = (\n    sorted_df[[pid_col] + static_src_cols]\n    .groupby(pid_col, as_index=False)\n    .apply(lambda g: g.ffill().bfill().iloc[0])\n    .reset_index(drop=True)\n)\n# Rename found columns to canonical\nstatic_block = static_block.rename(columns={v: k for k, v in resolved_static.items() if v is not None})\n# Ensure all requested static columns exist (even if missing → NaN), then order them\nfor c in STATIC_TARGETS:\n    if c not in static_block.columns:\n        static_block[c] = pd.NA\nstatic_block = static_block[[pid_col] + STATIC_TARGETS]\n\n# ========= BUILD VISIT (patient × visit, only your visit targets) =========\nvisit_src_cols = [v for v in resolved_visit.values() if v is not None]\nvisit_block = all_df[[pid_col, \"Visit\"] + visit_src_cols].copy()\nvisit_block = visit_block.rename(columns={v: k for k, v in resolved_visit.items() if v is not None})\n# Ensure all requested visit columns exist, then order\nfor c in VISIT_TARGETS:\n    if c not in visit_block.columns:\n        visit_block[c] = pd.NA\nvisit_block = visit_block[[pid_col, \"Visit\"] + VISIT_TARGETS]\n\n# Long + variable×visit pivots\nvisit_long = (\n    visit_block\n      .melt(id_vars=[pid_col, \"Visit\"], var_name=\"Variable\", value_name=\"Value\")\n      .dropna(subset=[\"Value\"], how=\"all\")\n)\nvisit_wide_by_var = (\n    visit_long.pivot_table(index=[pid_col, \"Variable\"], columns=\"Visit\", values=\"Value\", aggfunc=\"first\")\n    .reset_index()\n)\n\n# ========= SAVE =========\nwith pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\") as w:\n    static_block.to_excel(w, index=False, sheet_name=\"Outcome_Static\")\n    visit_block.to_excel(w,  index=False, sheet_name=\"Outcome_By_Visit\")\n    visit_long.to_excel(w,   index=False, sheet_name=\"Outcome_By_Visit_Long\")\n    visit_wide_by_var.to_excel(w, index=False, sheet_name=\"Outcome_Var_x_Visit\")\n\nstatic_block.to_csv(OUT_STATIC_CSV, index=False)\nvisit_long.to_csv(OUT_VISIT_LONG_CSV, index=False)\nvisit_wide_by_var.to_csv(OUT_VISIT_WIDE_CSV, index=False)\n\nprint(\"\\n✅ Static targets requested:\", len(STATIC_TARGETS), \"| found:\", len([v for v in resolved_static.values() if v]))\nprint(\"   Missing static:\", missing_static)\nprint(\"✅ Visit targets  requested:\", len(VISIT_TARGETS),  \"| found:\", len([v for v in resolved_visit.values() if v]))\nprint(\"   Missing visit:\", missing_visit)\nprint(\"Saved Excel →\", OUT_XLSX)\nprint(\"Also CSVs   →\", OUT_STATIC_CSV, OUT_VISIT_LONG_CSV, OUT_VISIT_WIDE_CSV)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}