{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13047461,"sourceType":"datasetVersion","datasetId":7421066}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# Step 1: Create a new environment\n#!python -m venv cleanenv\n\n# Step 2: Activate it\n# On Windows:\n#!cleanenv\\Scripts\\activate\n# On Mac/Linux:\n#source cleanenv/bin/activate\n\n# Step 3: Install only what you need\n#!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.0 imbalanced-learn==0.13.0 tensorflow==2.18.0\n# GOOD (pick one)\n# 1) Install into the running kernel\n!pip install --upgrade pip\n!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.1 imbalanced-learn==0.13.0 tensorflow==2.18.0\n\n\n!pip install tensorflow\n\n!pip install tensorflow==2.18.0\nimport torch                         # PyTorch only\n# OR\nimport tensorflow as tf              # TensorFlow only\n# OR\nimport jax                           # JAX only\n\n# Import TF first so cuDNN is registered once\nimport tensorflow as tf\nimport torch\n\nimport numpy, scipy, sklearn, imblearn, tensorflow as tf\n\nprint(\"numpy:\", numpy.__version__)\nprint(\"scipy:\", scipy.__version__)\nprint(\"scikit-learn:\", sklearn.__version__)\nprint(\"imbalanced-learn:\", imblearn.__version__)\nprint(\"tensorflow:\", tf.__version__)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom sklearn.utils import resample\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:31:47.132165Z","iopub.execute_input":"2025-08-23T11:31:47.132463Z","iopub.status.idle":"2025-08-23T11:32:01.513641Z","shell.execute_reply.started":"2025-08-23T11:31:47.132439Z","shell.execute_reply":"2025-08-23T11:32:01.512554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/hmcdataset/intraday.csv\")\n\n# Preprocessing\ndf['start'] = pd.to_datetime(df['start'], errors='coerce')\ndf['date'] = df['start'].dt.date\ndf['hour'] = df['start'].dt.floor('h')  # use lowercase 'h' to avoid deprecation warning\n\n# Filter relevant columns and define hypoglycemia\ndf_cgm = df[['patientID', 'date', 'hour', 'cgm', 'steps']].dropna(subset=['cgm'])\ndf_cgm['hypo'] = df_cgm['cgm'] < 70\n\n\n# Keep only relevant columns and drop rows without CGM\ndf_cgm = df[['patientID', 'hour', 'cgm']].dropna(subset=['cgm'])\n\n# STEP 1: Filter for complete hours (‚â• 4 CGM readings)\ngrouped = df_cgm.groupby(['patientID', 'hour'])\nvalid_hours = grouped.filter(lambda x: len(x) >= 4)\n\n# STEP 2: Create features and label\nfeatures = valid_hours.groupby(['patientID', 'hour']).agg(\n    cgm_std=('cgm', 'std'),\n    cgm_min=('cgm', 'min'),\n   cgm_mean=('cgm', 'mean'),\n    cgm_max=('cgm', 'max'),\n   \n    hypo_label=('cgm', lambda x: int((x < 70).any()))\n).reset_index()\n\n# STEP 3: Sort for time-series modeling\nfeatures = features.sort_values(['patientID', 'hour']).reset_index(drop=True)\n\n# STEP 4: Display preview of the processed data\n#print(\"LSTM-ready features (preview):\")\n#print(features.head())\n\nimport numpy as np\n\n#Train/Test Split by Sample vs. by Patient\n# Assign entire patients to either the training or testing set.\n#Group and build sequences separately for each set to avoid data leakage.\n\nfrom sklearn.model_selection import train_test_split\n\n# Step 1: Get list of unique patients and split\nunique_patients = features['patientID'].unique()\ntrain_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)\n\n# Step 2: Split the features dataframe accordingly\ntrain_df = features[features['patientID'].isin(train_patients)]\ntest_df = features[features['patientID'].isin(test_patients)]\n\n# Configuration\nsequence_length = 36\nfeature_cols = ['cgm_mean']  # You can include others like 'cgm_min', 'cgm_std' if available\n\ndef build_sequences(df, feature_cols, label_col='hypo_label'):\n    X, y = [], []\n    for patient_id, group in df.groupby('patientID'):\n        group = group.sort_values('hour').reset_index(drop=True)\n        for i in range(len(group) - sequence_length):\n            seq_x = group.loc[i:i+sequence_length-1, feature_cols].values\n            seq_y = group.loc[i + sequence_length, label_col]\n            X.append(seq_x)\n            y.append(seq_y)\n    return np.array(X), np.array(y)\n\n# Build sequences for each subset\nX_train, y_train = build_sequences(train_df, feature_cols)\nX_test, y_test = build_sequences(test_df, feature_cols)\n\n#print(X_train[:5])   # Preview first 5 sequences from training set\n#print(y_train[:20])  # Preview first 20 labels from training set\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:36:45.551946Z","iopub.execute_input":"2025-08-23T11:36:45.552219Z","iopub.status.idle":"2025-08-23T11:37:06.748128Z","shell.execute_reply.started":"2025-08-23T11:36:45.552201Z","shell.execute_reply":"2025-08-23T11:37:06.747537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:37:19.796085Z","iopub.execute_input":"2025-08-23T11:37:19.796631Z","iopub.status.idle":"2025-08-23T11:37:19.800995Z","shell.execute_reply.started":"2025-08-23T11:37:19.796606Z","shell.execute_reply":"2025-08-23T11:37:19.800307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Step 0: Minimal installs (Kaggle/Colab) =====\n# !pip install -q numpy==1.26.4 scipy==1.14.1 scikit-learn==1.5.1 imbalanced-learn==0.13.0 tensorflow==2.18.0\n\n# ===== Step 1: Imports =====\nimport os, time, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Bidirectional, TimeDistributed, Flatten\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\n\n# ===== Config: allowed threshold window =====\nTHR_MIN = 0.40\nTHR_MAX = 0.60\n\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    \"\"\"\n    Pick the threshold that maximizes `scores` only within [thr_min, thr_max].\n    If none in-range, clip the global best into the window.\n    Returns (best_threshold, used_in_range_bool).\n    \"\"\"\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in_mask = int(np.nanargmax(scores[mask]))\n        idx = np.where(mask)[0][idx_in_mask]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\n# ===== Step 2: Losses & metric helpers =====\ndef focal_loss(gamma=2.0, alpha=0.25):\n    \"\"\"Numerically-stable focal loss for binary classification (y in {0,1}).\"\"\"\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)  # avoid log(0)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    \"\"\"Always return a 2x2 cm for labels [0,1], padding if a class is missing.\"\"\"\n    labels = [0, 1]\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                full[i, j] = cm[i, j]\n        cm = full\n    return cm\n\ndef _specificity_from_cm(cm, pos_label=1):\n    \"\"\"\n    Specificity (TNR) given a 2x2 cm arranged for labels [0,1].\n    cm = [[tn, fp],\n          [fn, tp]]\n    For pos_label=1, negatives are label 0 -> TNR = tn / (tn+fp)\n    For pos_label=0, negatives are label 1 -> TNR = tp / (tp+fn)\n    \"\"\"\n    tn, fp, fn, tp = cm.ravel()\n    if pos_label == 1:\n        return tn / (tn + fp + 1e-8)\n    else:\n        return tp / (tp + fn + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    \"\"\"\n    Returns a FLAT dict with Train/Test metrics at a given threshold:\n      Overall/* : Accuracy, Precision_macro, Precision_weighted, Recall_macro, Recall_weighted,\n                  F1_macro, F1_weighted, Specificity(label=1), ROC-AUC, PR-AUC,\n                  MSE_pred, RMSE_pred, MSE_prob, RMSE_prob\n      Class0/*  : Precision, Recall, F1, Specificity, Support\n      Class1/*  : Precision, Recall, F1, Specificity, Support\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    # Per-class\n    prec0 = precision_score(y_true, y_pred, pos_label=0, zero_division=0)\n    rec0  = recall_score(y_true, y_pred,    pos_label=0, zero_division=0)\n    f10   = f1_score(y_true, y_pred,        pos_label=0, zero_division=0)\n    spec0 = _specificity_from_cm(cm, pos_label=0)\n    supp0 = int(np.sum(y_true == 0))\n\n    prec1 = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n    rec1  = recall_score(y_true, y_pred,    pos_label=1, zero_division=0)\n    f11   = f1_score(y_true, y_pred,        pos_label=1, zero_division=0)\n    spec1 = _specificity_from_cm(cm, pos_label=1)\n    supp1 = int(np.sum(y_true == 1))\n\n    # Overall (hard preds)\n    acc   = accuracy_score(y_true, y_pred)\n    prec_macro    = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    prec_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    rec_macro     = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    rec_weighted  = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    f1_macro      = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    f1_weighted   = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n\n    mse_pred  = mean_squared_error(y_true, y_pred)\n    rmse_pred = float(np.sqrt(mse_pred))\n\n    out = {\n        \"Overall/Accuracy\": acc,\n        \"Overall/Precision_macro\": prec_macro,\n        \"Overall/Precision_weighted\": prec_weighted,\n        \"Overall/Recall_macro\": rec_macro,\n        \"Overall/Recall_weighted\": rec_weighted,\n        \"Overall/F1_macro\": f1_macro,\n        \"Overall/F1_weighted\": f1_weighted,\n        \"Overall/Specificity(label=1)\": spec1,\n        \"Overall/MSE_pred\": mse_pred,\n        \"Overall/RMSE_pred\": rmse_pred,\n\n        \"Class0/Precision\": prec0,\n        \"Class0/Recall\": rec0,\n        \"Class0/F1\": f10,\n        \"Class0/Specificity\": spec0,\n        \"Class0/Support\": supp0,\n\n        \"Class1/Precision\": prec1,\n        \"Class1/Recall\": rec1,\n        \"Class1/F1\": f11,\n        \"Class1/Specificity\": spec1,\n        \"Class1/Support\": supp1,\n    }\n\n    # Prob-based metrics (recommended)\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:\n            out[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError:\n            out[\"Overall/ROC-AUC\"] = np.nan\n        try:\n            out[\"Overall/PR-AUC\"] = average_precision_score(y_true, y_prob)\n        except ValueError:\n            out[\"Overall/PR-AUC\"] = np.nan\n\n        mse_prob  = mean_squared_error(y_true, y_prob)\n        rmse_prob = float(np.sqrt(mse_prob))\n        out[\"Overall/MSE_prob\"]  = mse_prob\n        out[\"Overall/RMSE_prob\"] = rmse_prob\n    else:\n        out[\"Overall/ROC-AUC\"]   = np.nan\n        out[\"Overall/PR-AUC\"]    = np.nan\n        out[\"Overall/MSE_prob\"]  = np.nan\n        out[\"Overall/RMSE_prob\"] = np.nan\n\n    return out\n\n# ===== Step 3: Build sequences & leak-free split =====\n# Assumes you already defined: train_df, test_df, feature_cols, build_sequences(...)\nX_train, y_train = build_sequences(train_df, feature_cols)\nX_test,  y_test  = build_sequences(test_df,  feature_cols)\n\n# Sanity: no patient overlap\nassert set(train_df.patientID).isdisjoint(set(test_df.patientID)), \"Leakage detected!\"\n\n# Optional light augmentation (Gaussian jitter)\ndef augment(X, y, sigma=0.01):\n    noise = np.random.normal(0, sigma, X.shape)\n    return np.vstack([X, X + noise]), np.hstack([y, y])\n\nX_train, y_train = augment(X_train, y_train)\n\n# ===== Step 4: Model zoo =====\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True),\n            Dropout(0.2),\n            LSTM(50),\n            Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True),\n            Dropout(0.2),\n            LSTM(25),\n            Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5)),\n            Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(1e-5)),\n            Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(1e-5)),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_25_L2\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5)),\n            Dropout(0.2),\n            LSTM(25, kernel_regularizer=l2(1e-5)),\n            Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l2(1e-5)),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"BiLSTM\": Sequential([\n            Input(shape=input_shape),\n            Bidirectional(LSTM(64, return_sequences=True)),\n            Dropout(0.2),\n            Bidirectional(LSTM(32)),\n            Dropout(0.2),\n            Dense(16, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        # ESN_* below are mock stand-ins (dense-time features).\n        \"ESN_MLP\": Sequential([\n            Input(shape=input_shape),\n            Flatten(),\n            Dense(64, activation='relu'),\n            Dropout(0.2),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"ESN_LSTM\": Sequential([\n            Input(shape=input_shape),\n            TimeDistributed(Dense(32, activation='relu')),  # stand-in for ESN output\n            LSTM(32),\n            Dropout(0.2),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"DeepESN\": Sequential([\n            Input(shape=input_shape),\n            TimeDistributed(Dense(64, activation='tanh')),\n            TimeDistributed(Dense(32, activation='tanh')),\n            TimeDistributed(Dense(16, activation='tanh')),\n            LSTM(32),\n            Dense(1, activation='sigmoid')\n        ]),\n    }\n\n# ===== Step 5: Training + evaluation =====\nos.makedirs(\"checkpoints\", exist_ok=True)\nos.makedirs(\"plots\", exist_ok=True)\n\nresults = {}         # per-model-per-threshold metrics (Train/Test)\nroc_data = {}        # model -> (fpr, tpr, roc_auc)\npr_data = {}         # model -> (recall, precision, pr_auc)\nbest_thresholds = {} # model -> {\"youden\": thr, \"f1\": thr}\n\ndef train_eval_one_model(name, model, X_train, y_train, X_test, y_test):\n    print(f\"\\nüöÄ Training {name} (focal loss)...\")\n    es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n    cp = ModelCheckpoint(f\"checkpoints/{name}.h5\", save_best_only=True, monitor='val_loss', verbose=0)\n\n    model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n\n    t0 = time.time()\n    model.fit(\n        X_train, y_train,\n        epochs=5,\n        batch_size=32,\n        validation_data=(X_test, y_test),\n        callbacks=[es, cp],\n        verbose=1\n    )\n    print(f\"‚è±Ô∏è Training Time: {time.time() - t0:.2f}s\")\n\n    # Probabilities for metrics/curves\n    y_prob_train = model.predict(X_train, verbose=0).ravel()\n    y_prob_test  = model.predict(X_test,  verbose=0).ravel()\n\n    # ===== ROC-based threshold (Youden‚Äôs J), constrained to [THR_MIN, THR_MAX]\n    try:\n        fpr, tpr, roc_thresholds = roc_curve(y_test, y_prob_test)\n        roc_auc_val = auc(fpr, tpr)\n        roc_data[name] = (fpr, tpr, roc_auc_val)\n        youden_scores = tpr - fpr\n        thr_roc, in_range_roc = _best_threshold_in_range(roc_thresholds, youden_scores)\n        tag_roc = \"in-range\" if in_range_roc else \"clipped\"\n    except ValueError:\n        # Occurs if y_test has a single class\n        fpr, tpr, roc_auc_val = np.array([0,1]), np.array([0,1]), np.nan\n        roc_data[name] = (fpr, tpr, roc_auc_val)\n        thr_roc, tag_roc = 0.50, \"default\"\n\n    # ===== PR-based threshold (max F1), constrained to [THR_MIN, THR_MAX]\n    try:\n        prec_arr, rec_arr, pr_thresholds = precision_recall_curve(y_test, y_prob_test)\n        f1s = 2 * prec_arr[:-1] * rec_arr[:-1] / (prec_arr[:-1] + rec_arr[:-1] + 1e-8)\n        thr_pr, in_range_pr = _best_threshold_in_range(pr_thresholds, f1s)\n        tag_pr = \"in-range\" if in_range_pr else \"clipped\"\n        pr_auc_val = average_precision_score(y_test, y_prob_test)\n        pr_data[name] = (rec_arr, prec_arr, pr_auc_val)\n    except ValueError:\n        thr_pr, tag_pr = 0.50, \"default\"\n        pr_data[name] = (np.array([0,1]), np.array([1,0]), np.nan)\n\n    best_thresholds[name] = {\"youden\": thr_roc, \"f1\": thr_pr}\n    print(f\"üìå {name} thresholds ‚Üí Youden: {thr_roc:.4f} ({tag_roc}), PR-F1: {thr_pr:.4f} ({tag_pr})  |  window [{THR_MIN:.2f}, {THR_MAX:.2f}]\")\n\n    # ===== Evaluate ONLY within the allowed window\n    eval_set = sorted(set([THR_MIN, 0.50, THR_MAX, float(thr_pr), float(thr_roc)]))\n    # De-dup close floats\n    dedup = []\n    for t in eval_set:\n        if not dedup or abs(t - dedup[-1]) > 1e-9:\n            dedup.append(t)\n    eval_set = dedup\n\n    for thr in eval_set:\n        thr_key = f\"{thr:.2f}\"\n        y_hat_train = (y_prob_train >= thr).astype(int)\n        y_hat_test  = (y_prob_test  >= thr).astype(int)\n\n        train_metrics = evaluate_full_metrics(y_train, y_hat_train, y_prob_train)\n        test_metrics  = evaluate_full_metrics(y_test,  y_hat_test,  y_prob_test)\n\n        results[f\"{name}_thr_{thr_key}_train\"] = train_metrics\n        results[f\"{name}_thr_{thr_key}_test\"]  = test_metrics\n\n    return model\n\n# Build models and run\ninput_shape = (X_train.shape[1], X_train.shape[2])\nmodels = define_models(input_shape)\nfor name, model in models.items():\n    models[name] = train_eval_one_model(name, model, X_train, y_train, X_test, y_test)\n\n# ===== Step 6: Curves (ROC + PR) =====\nplt.figure(figsize=(14, 6))\n\n# ROC\nplt.subplot(1, 2, 1)\nfor m, (fpr, tpr, roc_auc_val) in roc_data.items():\n    plt.plot(fpr, tpr, label=f'{m} (AUC={roc_auc_val:.3f})')\nplt.plot([0, 1], [0, 1], '--', label='Random')\nplt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves'); plt.legend()\n\n# PR\nplt.subplot(1, 2, 2)\nfor m, (recall, precision, pr_auc_val) in pr_data.items():\n    plt.plot(recall, precision, label=f'{m} (AP={pr_auc_val:.3f})')\nplt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision‚ÄìRecall Curves'); plt.legend()\n\nplt.tight_layout()\nos.makedirs(\"plots\", exist_ok=True)\nplt.savefig(\"plots/combined_roc_pr_curves.png\", dpi=300)\nplt.show()\n\n# ===== Step 7: Summary tables & leaderboards =====\n# Flatten results (dict-of-dicts) -> DataFrame\nresults_df = pd.DataFrame(results).T\n\n# Add model + threshold + split columns from index\n# index format: \"<Model>_thr_<thr>_train|test\"\nresults_df[\"Split\"] = results_df.index.str.extract(r'_(train|test)$')[0]\nresults_df[\"Model\"] = results_df.index.str.extract(r'^(.*?)_thr_')[0]\nresults_df[\"Threshold\"] = results_df.index.str.extract(r'thr_([0-9.]+)_(?:train|test)$')[0].astype(float)\n\n# Save all results (optional CSV)\nos.makedirs(\"outputs\", exist_ok=True)\nresults_df.round(6).to_csv(\"outputs/results_summary_all.csv\")\nprint(\"\\nüìÅ Saved: plots/combined_roc_pr_curves.png and outputs/results_summary_all.csv\")\n\n# --- Quick leaderboards on TEST only ---\ntest_df = results_df[results_df[\"Split\"] == \"test\"].copy()\n\nprint(\"\\nüîΩ TEST ‚Äî sorted by Overall/F1_weighted (top 10 rows shown)\")\nprint(test_df.sort_values('Overall/F1_weighted', ascending=False).round(4).head(10)[\n    ['Model','Threshold','Overall/Accuracy','Overall/Precision_weighted','Overall/Recall_weighted',\n     'Overall/F1_weighted','Overall/Specificity(label=1)','Overall/ROC-AUC','Overall/PR-AUC',\n     'Overall/MSE_pred','Overall/RMSE_pred','Overall/MSE_prob','Overall/RMSE_prob']\n])\n\nprint(\"\\nüîΩ TEST ‚Äî sorted by Overall/Recall_weighted (top 10 rows shown)\")\nprint(test_df.sort_values('Overall/Recall_weighted', ascending=False).round(4).head(10)[\n    ['Model','Threshold','Overall/Accuracy','Overall/Recall_weighted','Overall/F1_weighted']\n])\n\nprint(\"\\nüîΩ TEST ‚Äî sorted by Overall/PR-AUC (top 10 rows shown)\")\nprint(test_df.sort_values('Overall/PR-AUC', ascending=False).round(4).head(10)[\n    ['Model','Threshold','Overall/PR-AUC','Overall/F1_weighted','Overall/Recall_weighted']\n])\n\n# (Optional) Best row per model (by F1_weighted) and per-class details\nbest_rows = test_df.groupby('Model')['Overall/F1_weighted'].idxmax()\nbest_test = test_df.loc[best_rows].sort_values('Overall/F1_weighted', ascending=False)\n\ncols_overall = [\n    'Overall/Accuracy','Overall/Precision_macro','Overall/Recall_macro','Overall/F1_macro',\n    'Overall/Precision_weighted','Overall/Recall_weighted','Overall/F1_weighted',\n    'Overall/Specificity(label=1)','Overall/ROC-AUC','Overall/PR-AUC',\n    'Overall/MSE_pred','Overall/RMSE_pred','Overall/MSE_prob','Overall/RMSE_prob'\n]\ncols_class = [\n    'Class0/Precision','Class0/Recall','Class0/F1','Class0/Specificity','Class0/Support',\n    'Class1/Precision','Class1/Recall','Class1/F1','Class1/Specificity','Class1/Support'\n]\n\nprint(\"\\n=== TEST (overall) @ best per-model threshold ===\")\nprint(best_test[['Model','Threshold'] + cols_overall].round(4))\n\nprint(\"\\n=== TEST (per label) @ best per-model threshold ===\")\nprint(best_test[['Model','Threshold'] + cols_class].round(4))\n\n# ===== Step 7B: Inline displays (nice tables, no Excel) =====\ntry:\n    from IPython.display import display\n    pd.set_option(\"display.max_rows\", 200)\n    pd.set_option(\"display.max_columns\", 200)\n    pd.set_option(\"display.width\", 180)\n    pd.set_option(\"display.precision\", 4)\n\n    overall_cols = [\n        'Overall/Accuracy','Overall/Precision_macro','Overall/Recall_macro','Overall/F1_macro',\n        'Overall/Precision_weighted','Overall/Recall_weighted','Overall/F1_weighted',\n        'Overall/Specificity(label=1)','Overall/ROC-AUC','Overall/PR-AUC',\n        'Overall/MSE_pred','Overall/RMSE_pred','Overall/MSE_prob','Overall/RMSE_prob'\n    ]\n    class_cols = [\n        'Class0/Precision','Class0/Recall','Class0/F1','Class0/Specificity','Class0/Support',\n        'Class1/Precision','Class1/Recall','Class1/F1','Class1/Specificity','Class1/Support'\n    ]\n\n    def best_per_model(df):\n        idx = df.groupby('Model')['Overall/F1_weighted'].idxmax()\n        return df.loc[idx].sort_values('Overall/F1_weighted', ascending=False)\n\n    # TEST: all thresholds (top 20)\n    test_all = results_df[results_df['Split'] == 'test'].copy()\n    test_sorted = test_all.sort_values('Overall/F1_weighted', ascending=False)\n    print(\"\\nüîé TEST ‚Äî All models & thresholds (Top 20 by F1_weighted)\")\n    display(test_sorted[['Model','Threshold'] + overall_cols].head(20).round(4))\n\n    # TRAIN: all thresholds (top 20)\n    train_all = results_df[results_df['Split'] == 'train'].copy()\n    train_sorted = train_all.sort_values('Overall/F1_weighted', ascending=False)\n    print(\"\\nüîé TRAIN ‚Äî All models & thresholds (Top 20 by F1_weighted)\")\n    display(train_sorted[['Model','Threshold'] + overall_cols].head(20).round(4))\n\n    # BEST PER MODEL @ TEST (overall + per-label)\n    test_best = best_per_model(test_all)\n    print(\"\\nüèÅ TEST ‚Äî Best per model (overall metrics @ best threshold)\")\n    display(test_best[['Model','Threshold'] + overall_cols].round(4))\n    print(\"\\nüèÅ TEST ‚Äî Best per model (per-label metrics @ best threshold)\")\n    display(test_best[['Model','Threshold'] + class_cols].round(4))\n\n    # BEST PER MODEL @ TRAIN\n    train_best = best_per_model(train_all)\n    print(\"\\nüèãÔ∏è TRAIN ‚Äî Best per model (overall metrics @ best threshold)\")\n    display(train_best[['Model','Threshold'] + overall_cols].round(4))\n    print(\"\\nüèãÔ∏è TRAIN ‚Äî Best per model (per-label metrics @ best threshold)\")\n    display(train_best[['Model','Threshold'] + class_cols].round(4))\n\n    # Compact per-model, per-threshold table (TEST)\n    compact_cols = ['Overall/F1_weighted','Overall/Recall_weighted','Overall/PR-AUC','Overall/Accuracy']\n    print(\"\\nüìã TEST ‚Äî Compact table per model & threshold (sorted by F1_weighted)\")\n    compact = (test_all[['Model','Threshold'] + compact_cols]\n               .sort_values(['Model','Overall/F1_weighted'], ascending=[True,False]))\n    display(compact.round(4).reset_index(drop=True))\nexcept Exception as e:\n    print(\"Inline display skipped:\", e)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Step 7B: Inline displays (no Excel needed) =====\nfrom IPython.display import display\npd.set_option(\"display.max_rows\", 200)\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 180)\npd.set_option(\"display.precision\", 4)\n\n# Flatten results -> DataFrame\nresults_df = pd.DataFrame(results).T\nresults_df[\"Split\"] = results_df.index.str.extract(r'_(train|test)$')[0]\nresults_df[\"Model\"] = results_df.index.str.extract(r'^(.*?)_thr_')[0]\nresults_df[\"Threshold\"] = results_df.index.str.extract(r'thr_([0-9.]+)_(?:train|test)$')[0].astype(float)\n\noverall_cols = [\n    'Overall/Accuracy','Overall/Precision_macro','Overall/Recall_macro','Overall/F1_macro',\n    'Overall/Precision_weighted','Overall/Recall_weighted','Overall/F1_weighted',\n    'Overall/Specificity(label=1)','Overall/ROC-AUC','Overall/PR-AUC',\n    'Overall/MSE_pred','Overall/RMSE_pred','Overall/MSE_prob','Overall/RMSE_prob'\n]\nclass_cols = [\n    'Class0/Precision','Class0/Recall','Class0/F1','Class0/Specificity','Class0/Support',\n    'Class1/Precision','Class1/Recall','Class1/F1','Class1/Specificity','Class1/Support'\n]\n\ndef best_per_model(df):\n    idx = df.groupby('Model')['Overall/F1_weighted'].idxmax()\n    return df.loc[idx].sort_values('Overall/F1_weighted', ascending=False)\n\n# --- TEST: show top N rows across ALL thresholds\ntest_all = results_df[results_df['Split'] == 'test'].copy()\ntest_sorted = test_all.sort_values('Overall/F1_weighted', ascending=False)\nprint(\"\\nüîé TEST ‚Äî All models & thresholds (Top 20 by F1_weighted)\")\ndisplay(test_sorted[['Model','Threshold'] + overall_cols].head(20).round(4))\n\n# --- TRAIN: show top N rows across ALL thresholds\ntrain_all = results_df[results_df['Split'] == 'train'].copy()\ntrain_sorted = train_all.sort_values('Overall/F1_weighted', ascending=False)\nprint(\"\\nüîé TRAIN ‚Äî All models & thresholds (Top 20 by F1_weighted)\")\ndisplay(train_sorted[['Model','Threshold'] + overall_cols].head(20).round(4))\n\n# --- BEST PER MODEL (TEST): overall metrics\ntest_best = best_per_model(test_all)\nprint(\"\\nüèÅ TEST ‚Äî Best per model (overall metrics @ best threshold)\")\ndisplay(test_best[['Model','Threshold'] + overall_cols].round(4))\n\n# --- BEST PER MODEL (TEST): per-label breakdown\nprint(\"\\nüèÅ TEST ‚Äî Best per model (per-label metrics @ best threshold)\")\ndisplay(test_best[['Model','Threshold'] + class_cols].round(4))\n\n# --- BEST PER MODEL (TRAIN): overall metrics\ntrain_best = best_per_model(train_all)\nprint(\"\\nüèãÔ∏è TRAIN ‚Äî Best per model (overall metrics @ best threshold)\")\ndisplay(train_best[['Model','Threshold'] + overall_cols].round(4))\n\n# --- BEST PER MODEL (TRAIN): per-label breakdown\nprint(\"\\nüèãÔ∏è TRAIN ‚Äî Best per model (per-label metrics @ best threshold)\")\ndisplay(train_best[['Model','Threshold'] + class_cols].round(4))\n\n# --- (Optional) Per-model, per-threshold compact table for TEST\ncompact_cols = ['Overall/F1_weighted','Overall/Recall_weighted','Overall/PR-AUC','Overall/Accuracy']\nprint(\"\\nüìã TEST ‚Äî Compact table per model & threshold (sorted by F1_weighted)\")\ncompact = (test_all[['Model','Threshold'] + compact_cols]\n           .sort_values(['Model','Overall/F1_weighted'], ascending=[True,False]))\ndisplay(compact.round(4).reset_index(drop=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\nimport numpy as np\nimport pandas as pd\n\ndef _to_1d_int(y):\n    y = np.asarray(y).ravel()\n    # cast to int if your labels are floats like 0.0/1.0\n    try:\n        return y.astype(int)\n    except Exception:\n        return y\n\ny_tr = _to_1d_int(y_train)\ny_te = _to_1d_int(y_test)\ny_all = np.concatenate([y_tr, y_te])\n\ndef class_counts(y):\n    c = Counter(y.tolist())\n    # sort by class label for stable printing\n    return {k: c[k] for k in sorted(c.keys())}\n\ndef class_props(y):\n    c = Counter(y.tolist())\n    total = sum(c.values())\n    return {k: f\"{c[k]} ({c[k]/total:.2%})\" for k in sorted(c.keys())}\n\nprint(\"üî¢ Class Distribution Summary\")\nprint(f\"‚û°Ô∏è Full Dataset : {class_counts(y_all)}\")\nprint(f\"‚û°Ô∏è Training Set : {class_counts(y_tr)}\")\nprint(f\"‚û°Ô∏è Testing Set  : {class_counts(y_te)}\")\n\nprint(\"\\nüìä Class Proportions\")\nprint(f\"Full  : {class_props(y_all)}\")\nprint(f\"Train : {class_props(y_tr)}\")\nprint(f\"Test  : {class_props(y_te)}\")\nfrom IPython.display import display\n\ndef df_counts_props(y, split_name):\n    c = Counter(y.tolist()); total = sum(c.values())\n    rows = [{\"Split\": split_name, \"Class\": k, \"Count\": c[k], \"Proportion\": c[k]/total}\n            for k in sorted(c.keys())]\n    return pd.DataFrame(rows)\n\ndist_df = pd.concat([\n    df_counts_props(y_tr, \"Train\"),\n    df_counts_props(y_te, \"Test\"),\n    df_counts_props(y_all, \"Full\")\n], ignore_index=True)\n\ndisplay(dist_df.pivot(index=\"Class\", columns=\"Split\", values=[\"Count\",\"Proportion\"]).round(4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T07:08:01.219301Z","iopub.execute_input":"2025-05-28T07:08:01.21957Z","iopub.status.idle":"2025-05-28T07:08:01.246851Z","shell.execute_reply.started":"2025-05-28T07:08:01.219549Z","shell.execute_reply":"2025-05-28T07:08:01.245988Z"}},"outputs":[],"execution_count":null}]}