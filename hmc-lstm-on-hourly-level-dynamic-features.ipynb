{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10739756,"sourceType":"datasetVersion","datasetId":6659625},{"sourceId":13049616,"sourceType":"datasetVersion","datasetId":8213963}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. ID mapping \n\nyour final_master_sheet_clean.xlsx has a unified patient ID column that lines up with your intraday data.\nwhat we’ll do\n\n##### 1. load final_master_sheet_clean.xlsx.\n##### 2. build a mapping Code → patientID (from the table you provided).\n##### 3. for every visit sheet (Ramadan, Visit 1 … Visit 7), replace/add a column PatientID (Huawei Data).\n##### 4. keep the original Code if you like, or drop it once you confirm the mapping is correct.\n##### 5. save a new Excel (e.g., final_master_sheet_clean_with_huawei.xlsx).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\n# input/output\nMASTER_PATH = Path(\"/kaggle/input/static-variables/final_master_sheet_clean.xlsx\")\nOUT_PATH = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\n\n# your mapping dictionary\nPATIENT_ID_MAP = {\n    \"R01\": 45, \"R02\": 46, \"R04\": 47, \"R05\": 48, \"R06\": 49, \"R07\": 53,\n    \"R10\": 54, \"R11\": 55, \"R12\": 57, \"R15\": 59, \"R16\": 60, \"R17\": 61,\n    \"R20\": 63, \"R21\": 64, \"R22\": 66, \"R23\": 67, \"R24\": 68, \"R25\": 69,\n    \"R26\": 70, \"R27\": 71, \"R28\": 72, \"R29\": 73, \"R30\": 74, \"R31\": 75,\n    \"R32\": 76, \"R33\": 77, \"R34\": 78, \"R35\": 79, \"R36\": 80, \"R37\": 81,\n    \"R39\": 82, \"R40\": 83, \"R41\": 84, \"R42\": 85, \"R43\": 86,\n}\nid_map_df = pd.DataFrame(list(PATIENT_ID_MAP.items()), columns=[\"Code\", \"PatientID (Huawei Data)\"])\n\n# load workbook\nxls = pd.ExcelFile(MASTER_PATH)\nsheets = {s: pd.read_excel(MASTER_PATH, sheet_name=s) for s in xls.sheet_names}\n\n# update each sheet that has \"Code\"\nupdated_sheets = {}\nfor name, df in sheets.items():\n    if \"Code\" in df.columns:\n        df = df.merge(id_map_df, on=\"Code\", how=\"left\")\n        # optional: drop old Code col and just keep Huawei ID\n        # df = df.drop(columns=[\"Code\"])\n        updated_sheets[name] = df\n    else:\n        updated_sheets[name] = df\n\n# save to new Excel\nwith pd.ExcelWriter(OUT_PATH, engine=\"openpyxl\") as writer:\n    for name, df in updated_sheets.items():\n        df.to_excel(writer, index=False, sheet_name=name)\n\nprint(f\"saved → {OUT_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T13:52:43.639321Z","iopub.execute_input":"2025-09-06T13:52:43.639676Z","iopub.status.idle":"2025-09-06T13:52:45.164197Z","shell.execute_reply.started":"2025-09-06T13:52:43.639648Z","shell.execute_reply":"2025-09-06T13:52:45.163327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. subperiods + Ramadan/Shawwal periods\n\nSets main periods (Ramadan/Shawwal) with your exact dates.\n\nDefines visit subperiods as inclusive date ranges:\n\nV1 = 2023-03-13 → 2023-03-26\nV2 = 2023-03-27 → 2023-04-02\nV3 = 2023-04-03 → 2023-04-09\nV4 = 2023-04-10 → 2023-04-19 (ends at Ramadan end)\nV5 = 2023-04-20 → 2023-04-26\nV6 = 2023-04-27 → 2023-05-08\nV7 = 2023-05-09 → 2023-05-19 (ends at Shawwal end)\n\n\nAdd explicit Visit subperiods + Ramadan/Shawwal periods:\n- Annotates intraday rows with `period_main` and `visit_assigned`\n- Writes spec sheets into master workbook\n- Saves:\n  /kaggle/working/intraday_with_visits.csv\n  /kaggle/working/final_master_sheet_clean_with_visits.xlsx\n\n","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nAnnotate intraday with Visit subperiods + Ramadan/Shawwal periods\n- Adds `visit_assigned` (inclusive visit windows) and `period_main` (Ramadan/Shawwal/outside)\n- Saves annotated intraday to CSV and **single-sheet Excel (Intraday_All)**\n- Injects spec sheets into the master workbook and adds Visit_Anchor_Date to visit sheets\n- Also writes a dynamic missingness-by-visit CSV (optional QA artifact)\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ---------------- CONFIG ----------------\nINTRADAY_CSV_PATH = Path(\"/kaggle/input/intraday/intraday.csv\")\nMASTER_XLSX_IN    = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\nMASTER_XLSX_OUT   = Path(\"/kaggle/working/final_master_sheet_clean_with_visits.xlsx\")\n\nINTRADAY_OUT_CSV  = Path(\"/kaggle/working/intraday_with_visits.csv\")\nINTRADAY_OUT_XLSX = Path(\"/kaggle/working/intraday_with_visits.xlsx\")\n\nDYN_VISIT_OUT     = Path(\"/kaggle/working/missingness_dynamic_by_visit.csv\")  # optional QA\n\n# Main periods (inclusive)\nI_RAMADAN_START = pd.Timestamp(\"2023-03-22\")\nI_RAMADAN_END   = pd.Timestamp(\"2023-04-19\")\nI_SHAWWAL_START = pd.Timestamp(\"2023-04-20\")\nI_SHAWWAL_END   = pd.Timestamp(\"2023-05-19\")\n\n# Visit subperiods (ALL INCLUSIVE — exactly as specified)\nVISIT_SUBPERIODS = [\n    {\"Visit\": \"Visit 1\", \"start\": \"2023-03-13\", \"end\": \"2023-03-21\"},\n    {\"Visit\": \"Visit 2\", \"start\": \"2023-03-22\", \"end\": \"2023-03-30\"},\n    {\"Visit\": \"Visit 3\", \"start\": \"2023-03-31\", \"end\": \"2023-04-06\"},\n    {\"Visit\": \"Visit 4\", \"start\": \"2023-04-08\", \"end\": \"2023-04-16\"},\n    {\"Visit\": \"Visit 5\", \"start\": \"2023-04-17\", \"end\": \"2023-04-26\"},\n    {\"Visit\": \"Visit 6\", \"start\": \"2023-04-27\", \"end\": \"2023-05-08\"},\n    {\"Visit\": \"Visit 7\", \"start\": \"2023-05-09\", \"end\": \"2023-05-19\"},\n]\n\n# Sheets in master we do not modify\nSKIP_SHEETS = {\"HMC_map_patientID\"}\n\n\n# ---------------- HELPERS ----------------\ndef _normalize_date_col(s: pd.Series) -> pd.Series:\n    return pd.to_datetime(s, errors=\"coerce\").dt.normalize()\n\ndef build_visit_spec_df(rows: list[dict]) -> pd.DataFrame:\n    \"\"\"\n    Build inclusive visit windows as a tidy DataFrame, warn (don't crash) on overlaps or bad spans.\n    \"\"\"\n    df = pd.DataFrame(rows)\n    df[\"start\"] = _normalize_date_col(df[\"start\"])\n    df[\"end\"]   = _normalize_date_col(df[\"end\"])\n    df = df.sort_values([\"start\", \"end\"]).reset_index(drop=True)\n\n    bad_span = df[df[\"end\"] < df[\"start\"]]\n    if not bad_span.empty:\n        print(\"⚠️ Visit with end < start:\\n\", bad_span)\n\n    df[\"prev_end\"] = df[\"end\"].shift(1)\n    overlap = df[(df.index > 0) & (df[\"start\"] <= df[\"prev_end\"])]\n    if not overlap.empty:\n        print(\"⚠️ Overlapping visits detected (inclusive ranges). Ensure previous 'end' < next 'start'.\")\n        print(overlap[[\"Visit\", \"start\", \"end\", \"prev_end\"]])\n\n    return df.drop(columns=[\"prev_end\"], errors=\"ignore\")\n\ndef tag_main_period(ts: pd.Timestamp) -> str | float:\n    if pd.isna(ts):\n        return np.nan\n    if I_RAMADAN_START <= ts <= I_RAMADAN_END:\n        return \"Ramadan (Mar 22–Apr 19, 2023)\"\n    if I_SHAWWAL_START <= ts <= I_SHAWWAL_END:\n        return \"Shawwal (Apr 20–May 19, 2023)\"\n    return \"Outside Ramadan/Shawwal\"\n\ndef assign_visit_inclusive(dates: pd.Series, visit_spec: pd.DataFrame) -> pd.Series:\n    \"\"\"\n    Inclusive mapping: a date belongs to Visit i if start_i <= date <= end_i.\n    Dates outside all windows remain NaN.\n    \"\"\"\n    visits_cat = pd.CategoricalDtype(categories=list(visit_spec[\"Visit\"]), ordered=True)\n    out = pd.Series(pd.Categorical([None] * len(dates), dtype=visits_cat), index=dates.index)\n    for _, r in visit_spec.iterrows():\n        mask = (dates >= r[\"start\"]) & (dates <= r[\"end\"])\n        out.loc[mask] = r[\"Visit\"]\n    return out\n\ndef _order_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Put key columns first for readability if they exist.\"\"\"\n    cols = list(df.columns)\n    front = [c for c in [\"date\", \"visit_assigned\", \"period_main\", \"patientID\", \"huaweiID\"] if c in cols]\n    rest  = [c for c in cols if c not in front]\n    return df[front + rest]\n\n\n# ---------------- 1) LOAD & ANNOTATE INTRADAY ----------------\nprint(\"Annotating intraday with visit subperiods & main periods…\")\nintraday = pd.read_csv(INTRADAY_CSV_PATH)\nif \"date\" not in intraday.columns:\n    raise ValueError(\"intraday.csv must have a 'date' column\")\n\nintraday[\"date\"] = _normalize_date_col(intraday[\"date\"])\n\nvisit_spec_df = build_visit_spec_df(VISIT_SUBPERIODS)\nintraday[\"period_main\"]    = intraday[\"date\"].apply(tag_main_period)\nintraday[\"visit_assigned\"] = assign_visit_inclusive(intraday[\"date\"], visit_spec_df)\n\n# Save CSV once\nintraday.to_csv(INTRADAY_OUT_CSV, index=False)\nprint(f\"✅ Saved annotated intraday CSV → {INTRADAY_OUT_CSV}\")\n\n# ---------------- 2) UPDATE MASTER WORKBOOK ----------------\nprint(\"Injecting Visit_Subperiods_Spec & Period_Bounds into master…\")\nxls = pd.ExcelFile(MASTER_XLSX_IN)\nsheets = {s: pd.read_excel(MASTER_XLSX_IN, sheet_name=s) for s in xls.sheet_names}\n\n# optional: add a Visit_Anchor_Date to each Visit sheet (start of its inclusive subperiod)\nANCHORS = {row[\"Visit\"]: row[\"start\"] for _, row in visit_spec_df.iterrows()}\n\nupdated = {}\nfor name, df in sheets.items():\n    if name in SKIP_SHEETS:\n        updated[name] = df\n        continue\n    if name in ANCHORS and \"Visit_Anchor_Date\" not in df.columns:\n        df = df.copy()\n        df[\"Visit_Anchor_Date\"] = ANCHORS[name].date()\n    updated[name] = df\n\nperiod_bounds = pd.DataFrame({\n    \"Period\":    [\"Ramadan\", \"Shawwal\"],\n    \"Start\":     [I_RAMADAN_START.date(), I_SHAWWAL_START.date()],\n    \"End\":       [I_RAMADAN_END.date(),   I_SHAWWAL_END.date()],\n    \"Inclusive\": [True, True],\n})\n\nwith pd.ExcelWriter(MASTER_XLSX_OUT, engine=\"openpyxl\") as writer:\n    for name, df in updated.items():\n        df.to_excel(writer, index=False, sheet_name=name[:31])  # safe truncate\n    visit_spec_df.to_excel(writer, index=False, sheet_name=\"Visit_Subperiods_Spec\")\n    period_bounds.to_excel(writer, index=False, sheet_name=\"Period_Bounds\")\n\nprint(f\"✅ Saved master with specs → {MASTER_XLSX_OUT}\")\n\n# ---------------- 3) OPTIONAL QA: MISSINGNESS BY VISIT (CSV) ----------------\nEXCLUDE = {\"Unnamed: 0\", \"huaweiID\", \"date\", \"start\", \"period_main\", \"visit_assigned\"}\ndfv = intraday.copy()\nfeature_cols = [c for c in dfv.columns if c not in EXCLUDE and c != \"patientID\"]\n\nif feature_cols:\n    out = (\n        dfv.groupby(\"visit_assigned\")[feature_cols]\n           .apply(lambda g: g.isna().mean().mul(100).round(2))\n           .reset_index()\n           .rename(columns={\"visit_assigned\": \"Visit\"})\n    )\n    out.melt(id_vars=[\"Visit\"], var_name=\"Feature\", value_name=\"% Missing (Visit Window)\") \\\n       .to_csv(DYN_VISIT_OUT, index=False)\n    print(f\"✅ Saved dynamic-by-visit missingness CSV → {DYN_VISIT_OUT}\")\nelse:\n    print(\"ℹ️ Skipped missingness export (no feature columns found).\")\n\n# ---------------- 4) SINGLE-SHEET EXCEL EXPORT ----------------\nprint(\"Writing single-sheet annotated intraday Excel…\")\nintraday_ordered = _order_columns(intraday)\n\nwith pd.ExcelWriter(INTRADAY_OUT_XLSX, engine=\"openpyxl\") as writer:\n    intraday_ordered.to_excel(writer, index=False, sheet_name=\"Intraday_All\")\n\nprint(f\"✅ Saved single-sheet intraday Excel → {INTRADAY_OUT_XLSX}\")\n\n# ---------------- 5) QUICK CHECKS (stdout) ----------------\nprint(\"\\n— Quick checks —\")\ntry:\n    print(\"Date coverage:\", intraday[\"date\"].min(), \"→\", intraday[\"date\"].max(), \"| rows:\", len(intraday))\n    print(\"\\nVisit assignment counts:\")\n    print(intraday[\"visit_assigned\"].value_counts(dropna=False))\n    print(\"\\nMain period counts:\")\n    print(intraday[\"period_main\"].value_counts(dropna=False))\nexcept Exception as e:\n    print(\"Sanity checks skipped:\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T17:29:33.610930Z","iopub.execute_input":"2025-09-06T17:29:33.611207Z","iopub.status.idle":"2025-09-06T17:29:44.286791Z","shell.execute_reply.started":"2025-09-06T17:29:33.611183Z","shell.execute_reply":"2025-09-06T17:29:44.285599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Overview about the Static and Dynamic feature \n\nUnified Missingness Pipeline (Qatar edition, percent-aware)\n- Normalized name alignment (cleaned + aliases + percent-aware)\n- Separate Static vs Dynamic outputs\n- Ramadan/Shawwal rollups using visit groups\n- QC conditional formatting (>30 warn, >50 alert)\n- Patient-level missingness per visit\n- Robust dictionary merge (canonical-first, strict percent-safe fallback)\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport json\nimport re\nimport unicodedata\n\n# =============== CONFIG ===============\n# Inputs\nMASTER_PATH = Path(\"/kaggle/working/final_master_sheet_clean_with_huawei.xlsx\")\nDICT_PATH   = Path(\"/kaggle/input/static-variables/Categorized_Data_Dictionary.xlsx\")  # xlsx or csv\nINTRADAY_CSV_PATH = Path(\"/kaggle/input/intraday/intraday.csv\")  # optional dynamic\n\n# Patient ID column in the master visit sheets (after your mapping step)\nPATIENT_ID_COL = \"PatientID (Huawei Data)\"\n\n# Which sheets to treat as visits (the script will read all except SKIP)\nSKIP_SHEETS = {\"HMC_map_patientID\"}\n\n# Visit groups for rollups (edit as needed)\nRAMADAN_VISITS = {\"Ramadan\", \"Visit 1\", \"Visit 2\", \"Visit 3\", \"Visit 4 (whole Ramadan)\"}\nSHAWWAL_VISITS = {\"Visit 6 (Shawal)\"}  # add Visit 5 here if it belongs to Shawwal\n\n# Intraday options\nINCLUDE_INTRADAY = True\nRAMADAN_START = pd.to_datetime(\"2023-03-23\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-21\")\nSHAWWAL_START = pd.to_datetime(\"2023-04-22\")\nSHAWWAL_END   = pd.to_datetime(\"2023-05-21\")\n# keep patientID to align; exclude only non-feature fields\nINTRADAY_EXCLUDE_COLS = {\"Unnamed: 0\", \"huaweiID\", \"date\", \"start\"}\n\n# Optional stage-2 CSV that (if present) we add as a sheet\nDYN_VISIT_OUT = Path(\"/kaggle/working/missingness_dynamic_by_visit.csv\")\n\n# Outputs\nOUT_XLSX = Path(\"/kaggle/working/missingness_report_full.xlsx\")\nOUT_CSV_STATIC  = Path(\"/kaggle/working/missingness_static.csv\")\nOUT_CSV_DYNAMIC = Path(\"/kaggle/working/missingness_dynamic.csv\")\nOUT_CSV_PATIENT = Path(\"/kaggle/working/missingness_patient_level.csv\")\n\n# QC thresholds (%)\nWARN_THRES = 30.0\nALERT_THRES = 50.0\n# =====================================\n\n\n# ===== Percent-aware normalization =====\ndef _strip_accents(s: str) -> str:\n    s = unicodedata.normalize(\"NFKD\", s)\n    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n\ndef clean_name_base(s: str) -> str:\n    \"\"\"Base alnum key (lower, accents removed, spaces collapsed, punctuation removed).\"\"\"\n    if s is None or (isinstance(s, float) and pd.isna(s)):\n        return \"\"\n    s = str(s).lower().strip()\n    s = _strip_accents(s)\n    s = re.sub(r\"\\s+\", \" \", s)\n    s = re.sub(r\"[^a-z0-9]\", \"\", s)\n    return s\n\ndef has_percent(s: str) -> bool:\n    \"\"\"Detect percent variants like 'HbA1c (%)', 'hba1c%', 'hba1c%%'.\"\"\"\n    if s is None or (isinstance(s, float) and pd.isna(s)):\n        return False\n    s0 = str(s).lower()\n    if \"%\" in s0:\n        return True\n    if \"(%)\" in s0:\n        return True\n    # If you want the word 'percent' to count:\n    # if re.search(r\"\\bpercent\\b\", s0): return True\n    return False\n\ndef canonical_key(raw_name: str) -> str:\n    \"\"\"Percent-safe canonical key: base or base+'_pct'.\"\"\"\n    base = clean_name_base(raw_name)\n    return f\"{base}_pct\" if has_percent(raw_name) else base\n\n# Canonical targets (extend as needed)\nCANON = {\n    \"meals_per_day\": \"meals_per_day\",\n    \"carbs_per_day\": \"carbs_per_day\",\n    \"active_insulin_time_hours\": \"active_insulin_time_hours\",\n    \"average_sg_mgdl\": \"average_sg_mgdl\",\n    \"icr_1\": \"icr_1\",\n    \"icr_2\": \"icr_2\",\n    \"sg_sd_mgdl\": \"sg_sd_mgdl\",\n    \"total_daily_dose_units\": \"total_daily_dose_units\",\n    # example duals for percent/non-percent:\n    \"hba1c\": \"hba1c\",\n    \"hba1c_pct\": \"hba1c_pct\",\n}\n\n# Aliases on canonical_key(raw) -> canonical name\nALIASES = {\n    # meals\n    canonical_key(\"meals\"): \"meals_per_day\",\n    canonical_key(\"meal\"): \"meals_per_day\",\n    canonical_key(\"meals/day\"): \"meals_per_day\",\n    canonical_key(\"meals per day\"): \"meals_per_day\",\n    canonical_key(\"number of meals\"): \"meals_per_day\",\n\n    # carbs\n    canonical_key(\"carb\"): \"carbs_per_day\",\n    canonical_key(\"carbs\"): \"carbs_per_day\",\n    canonical_key(\"carb/day\"): \"carbs_per_day\",\n    canonical_key(\"carbs/day\"): \"carbs_per_day\",\n    canonical_key(\"carbohydrates per day\"): \"carbs_per_day\",\n\n    # specifics\n    canonical_key(\"Active insulin time (hours)\"): \"active_insulin_time_hours\",\n    canonical_key(\"Active insulin time hours\"): \"active_insulin_time_hours\",\n    canonical_key(\"AIT (hours)\"): \"active_insulin_time_hours\",\n\n    canonical_key(\"Average SG mg/dL\"): \"average_sg_mgdl\",\n    canonical_key(\"Avg SG mg/dL\"): \"average_sg_mgdl\",\n    canonical_key(\"Average sensor glucose (mg/dL)\"): \"average_sg_mgdl\",\n\n    canonical_key(\"ICR-1\"): \"icr_1\",\n    canonical_key(\"ICR 1\"): \"icr_1\",\n    canonical_key(\"insulin carb ratio 1\"): \"icr_1\",\n\n    canonical_key(\"ICR-2\"): \"icr_2\",\n    canonical_key(\"ICR 2\"): \"icr_2\",\n    canonical_key(\"insulin carb ratio 2\"): \"icr_2\",\n\n    canonical_key(\"SG SD mg/dL\"): \"sg_sd_mgdl\",\n    canonical_key(\"sensor glucose sd (mg/dL)\"): \"sg_sd_mgdl\",\n\n    canonical_key(\"Total daily dose (Unit)\"): \"total_daily_dose_units\",\n    canonical_key(\"Total daily dose (Units)\"): \"total_daily_dose_units\",\n    canonical_key(\"TDD (units)\"): \"total_daily_dose_units\",\n    canonical_key(\"TDD\"): \"total_daily_dose_units\",\n\n    # HbA1c (non-percent vs percent — DO NOT MERGE)\n    canonical_key(\"HbA1c\"): \"hba1c\",\n    canonical_key(\"HBA1C\"): \"hba1c\",\n    canonical_key(\"HbA1c (%)\"): \"hba1c_pct\",\n    canonical_key(\"hba1c%\"): \"hba1c_pct\",\n    canonical_key(\"hba1c%%\"): \"hba1c_pct\",\n}\n\ndef normalize_to_canonical(raw_name: str) -> str:\n    \"\"\"Map raw -> canonical (percent-safe).\"\"\"\n    key = canonical_key(raw_name)\n    return ALIASES.get(key, key)\n\n\n# =========================\n# ====== HELPERS ==========\n# =========================\ndef _clean_df_strings(df: pd.DataFrame) -> pd.DataFrame:\n    df2 = df.copy()\n    for col in df2.columns:\n        if df2[col].dtype == object:\n            s = df2[col].astype(str).str.strip()\n            s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"NA\": np.nan, \"None\": np.nan})\n            df2[col] = s\n    return df2\n\ndef percent_missing_by_column(df: pd.DataFrame, visit_name: str) -> pd.DataFrame:\n    df2 = _clean_df_strings(df)\n    miss = df2.isna().mean().mul(100).round(2)\n    out = miss.reset_index()\n    out.columns = [\"Feature\", f\"% Missing {visit_name}\"]\n    return out\n\ndef missing_counts(df: pd.DataFrame, visit_name: str) -> pd.DataFrame:\n    df2 = _clean_df_strings(df)\n    total = len(df2)\n    miss = df2.isna().sum()\n    return pd.DataFrame({\n        \"Feature\": miss.index,\n        f\"missing_{visit_name}\": miss.values,\n        f\"total_{visit_name}\": total\n    })\n\ndef availability_for_features(features, visit_dfs):\n    avail = []\n    for feat in features:\n        present_in = [vname for vname, vdf in visit_dfs.items() if feat in vdf.columns]\n        avail.append(present_in)\n    return pd.Series(avail, name=\"Availability (Visits)\")\n\ndef _pick_feature_col(cols):\n    for c in [\"Feature\", \"Feature Name\", \"Variable\", \"Field\", \"Name\"]:\n        if c in cols:\n            return c\n    return None\n\ndef load_dictionary(dict_path: Path) -> pd.DataFrame | None:\n    if not dict_path.exists():\n        print(f\"[INFO] Dictionary not found at {dict_path}. Skipping metadata merge.\")\n        return None\n\n    try:\n        if dict_path.suffix.lower() == \".csv\":\n            raw = pd.read_csv(dict_path)\n        else:\n            xls = pd.ExcelFile(dict_path)\n            selected = None\n            for s in xls.sheet_names:\n                tmp = pd.read_excel(dict_path, sheet_name=s)\n                fcol = _pick_feature_col(tmp.columns)\n                if fcol and \"Category\" in tmp.columns:\n                    selected = tmp.copy()\n                    break\n            if selected is None:\n                selected = pd.read_excel(dict_path)\n            raw = selected\n    except Exception as e:\n        print(f\"[WARN] Failed to read dictionary: {e}. Skipping metadata merge.\")\n        return None\n\n    fcol = _pick_feature_col(raw.columns)\n    if fcol is None:\n        print(\"[WARN] No recognizable feature-name column in dictionary. Skipping metadata merge.\")\n        return None\n\n    df = raw.rename(columns={fcol: \"Feature\"})\n    if \"Definition\" in df.columns and \"Definition & Unit\" not in df.columns:\n        df = df.rename(columns={\"Definition\": \"Definition & Unit\"})\n    keep_cols = [c for c in [\n        \"Feature\", \"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"\n    ] if c in df.columns]\n    if \"Feature\" not in keep_cols or \"Category\" not in keep_cols:\n        print(\"[WARN] Dictionary missing 'Feature' or 'Category' after normalization. Skipping metadata merge.\")\n        return None\n\n    df = df[keep_cols].drop_duplicates()\n    # percent-aware merge keys for the dictionary side\n    df[\"Feature_base\"]      = df[\"Feature\"].map(clean_name_base)\n    df[\"Feature_is_pct\"]    = df[\"Feature\"].map(has_percent)\n    df[\"Feature_canonical\"] = df[\"Feature\"].map(normalize_to_canonical)\n    return df\n\ndef weighted_overall_pct(merged_counts: pd.DataFrame) -> pd.Series:\n    missing_cols = [c for c in merged_counts.columns if c.startswith(\"missing_\")]\n    total_cols   = [c for c in merged_counts.columns if c.startswith(\"total_\")]\n    missing_total = merged_counts[missing_cols].sum(axis=1)\n    total_total   = merged_counts[total_cols].sum(axis=1)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        pct = np.where(total_total > 0, (missing_total / total_total) * 100, np.nan)\n    return np.round(pct, 2)\n\ndef qc_format_excel(writer, sheet_name, warn=WARN_THRES, alert=ALERT_THRES):\n    \"\"\"Add conditional formatting for % columns (>30 yellow, >50 red).\"\"\"\n    try:\n        from openpyxl.styles import PatternFill\n        from openpyxl.formatting.rule import CellIsRule\n        wb = writer.book\n        ws = wb[sheet_name]\n        header = [cell.value for cell in ws[1]]\n        pct_cols = [i for i, h in enumerate(header, start=1) if isinstance(h, str) and \"Missing\" in h]\n        yellow = PatternFill(start_color=\"FFF3CD\", end_color=\"FFF3CD\", fill_type=\"solid\")  # warn\n        red    = PatternFill(start_color=\"F8D7DA\", end_color=\"F8D7DA\", fill_type=\"solid\")  # alert\n        max_row = ws.max_row\n        for col in pct_cols:\n            col_letter = ws.cell(row=1, column=col).column_letter\n            rng = f\"{col_letter}2:{col_letter}{max_row}\"\n            ws.conditional_formatting.add(rng, CellIsRule(operator=\"greaterThan\", formula=[str(warn)], fill=yellow))\n            ws.conditional_formatting.add(rng, CellIsRule(operator=\"greaterThan\", formula=[str(alert)], fill=red))\n    except Exception as e:\n        print(f\"[INFO] QC formatting skipped on {sheet_name}: {e}\")\n\n\n# ------------- Main -------------\ndef main():\n    # Ensure output dir exists\n    OUT_XLSX.parent.mkdir(parents=True, exist_ok=True)\n\n    # Load visit sheets\n    xls = pd.ExcelFile(MASTER_PATH)\n    visit_sheet_names = [s for s in xls.sheet_names if s not in SKIP_SHEETS]\n    visits = {name: pd.read_excel(MASTER_PATH, sheet_name=name) for name in visit_sheet_names}\n\n    # ---- Static: per-visit missingness ----\n    miss_tables = [percent_missing_by_column(df, name) for name, df in visits.items()]\n    from functools import reduce\n    merged_miss = reduce(lambda L, R: pd.merge(L, R, on=\"Feature\", how=\"outer\"), miss_tables) if miss_tables else pd.DataFrame(columns=[\"Feature\"])\n\n    count_tables = [missing_counts(df, name) for name, df in visits.items()]\n    merged_counts = reduce(lambda L, R: pd.merge(L, R, on=\"Feature\", how=\"outer\"), count_tables) if count_tables else pd.DataFrame(columns=[\"Feature\"])\n    for col in merged_counts.columns:\n        if col.startswith(\"missing_\") or col.startswith(\"total_\"):\n            merged_counts[col] = merged_counts[col].fillna(0)\n\n    final_static = merged_miss.copy()\n    if not merged_counts.empty:\n        final_static[\"Overall % Missing\"] = weighted_overall_pct(merged_counts)\n    else:\n        final_static[\"Overall % Missing\"] = np.nan\n\n    final_static[\"Availability (Visits)\"] = availability_for_features(final_static[\"Feature\"], visits).apply(lambda x: json.dumps(x))\n\n    # ---- Merge dictionary (percent-aware) ----\n    dict_core = load_dictionary(DICT_PATH)\n    final_static[\"Feature_base\"]      = final_static[\"Feature\"].map(clean_name_base)\n    final_static[\"Feature_is_pct\"]    = final_static[\"Feature\"].map(has_percent)\n    final_static[\"Feature_canonical\"] = final_static[\"Feature\"].map(normalize_to_canonical)\n\n    if dict_core is not None:\n        # Pass 1: canonical join\n        merged = final_static.merge(\n            dict_core.add_suffix(\"_dict\"),\n            left_on=\"Feature_canonical\",\n            right_on=\"Feature_canonical_dict\",\n            how=\"left\"\n        )\n        # Pass 2 (strict): fallback by (base, is_pct) for unmatched rows\n        need_fb = merged[\"Category_dict\"].isna() if \"Category_dict\" in merged.columns else pd.Series(False, index=merged.index)\n        if need_fb.any():\n            fb_left = final_static.loc[need_fb, [\"Feature\", \"Feature_base\", \"Feature_is_pct\"]].copy()\n            fb_right = dict_core.add_suffix(\"_dict\")[\n                [\"Feature_dict\", \"Feature_base_dict\", \"Feature_is_pct_dict\",\n                 \"Category_dict\", \"Definition & Unit_dict\", \"Unit/Type_dict\", \"Feature Type_dict\", \"Subtype_dict\"]\n            ]\n            fb_joined = fb_left.merge(\n                fb_right,\n                left_on=[\"Feature_base\", \"Feature_is_pct\"],\n                right_on=[\"Feature_base_dict\", \"Feature_is_pct_dict\"],\n                how=\"left\"\n            )\n            idx = merged.index[need_fb]\n            for col in fb_joined.columns:\n                if col.endswith(\"_dict\") and col in merged.columns:\n                    merged.loc[idx, col] = fb_joined[col].values\n\n        # Prefer dictionary metadata where available\n        for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"]:\n            lc, rc = c, f\"{c}_dict\"\n            if lc not in merged.columns:\n                merged[lc] = np.nan\n            if rc in merged.columns:\n                merged[lc] = merged[lc].combine_first(merged[rc])\n\n        merged[\"Matched Dictionary Feature\"] = merged.get(\"Feature_dict\", np.nan)\n        # Cleanup helper cols\n        drop_helpers = [c for c in merged.columns if c.endswith(\"_dict\")] + \\\n                       [\"Feature_base\", \"Feature_base_dict\", \"Feature_is_pct\", \"Feature_is_pct_dict\",\n                        \"Feature_canonical\", \"Feature_canonical_dict\"]\n        final_static = merged.drop(columns=[c for c in drop_helpers if c in merged.columns])\n    else:\n        for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\", \"Matched Dictionary Feature\"]:\n            if c not in final_static.columns:\n                final_static[c] = np.nan\n\n    # Order columns\n    visit_cols = [c for c in final_static.columns if c.startswith(\"% Missing \")]\n    final_static = final_static[\n        [\"Category\", \"Feature\", \"Matched Dictionary Feature\", \"Definition & Unit\",\n         \"Unit/Type\", \"Feature Type\", \"Subtype\"]\n        + visit_cols\n        + [\"Overall % Missing\", \"Availability (Visits)\"]\n    ]\n\n    # ---- Ramadan/Shawwal rollups (static) ----\n    counts_map = {name: missing_counts(df, name).set_index(\"Feature\") for name, df in visits.items()}\n\n    def pooled_group_pct(features, group_visits):\n        if len(features) == 0:\n            return np.array([])\n        miss_total = pd.Series(0, index=features, dtype=float)\n        tot_total  = pd.Series(0, index=features, dtype=float)\n        for v in group_visits:\n            if v not in counts_map:\n                continue\n            mc = counts_map[v]\n            miss_total = miss_total.add(mc.get(f\"missing_{v}\", pd.Series(0, index=features)).reindex(features).fillna(0), fill_value=0)\n            tot_total  = tot_total.add(mc.get(f\"total_{v}\", pd.Series(0, index=features)).reindex(features).fillna(0),   fill_value=0)\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            pct = np.where(tot_total > 0, (miss_total / tot_total) * 100, np.nan)\n        return np.round(pct, 2)\n\n    features_list = final_static[\"Feature\"]\n    final_static[\"% Missing Ramadan (rollup)\"] = pooled_group_pct(features_list, RAMADAN_VISITS)\n    final_static[\"% Missing Shawwal (rollup)\"] = pooled_group_pct(features_list, SHAWWAL_VISITS)\n\n    # ---- Patient-level missingness per visit (row-wise) ----\n    plm_list = []\n    for vname, vdf in visits.items():\n        if PATIENT_ID_COL not in vdf.columns:\n            continue\n        df2 = _clean_df_strings(vdf)\n        cols_eval = [c for c in df2.columns if c != PATIENT_ID_COL]\n        if not cols_eval:\n            continue\n        row_pct = df2[cols_eval].isna().mean(axis=1).mul(100).round(2)\n        tmp = pd.DataFrame({\n            \"Visit\": vname,\n            PATIENT_ID_COL: df2[PATIENT_ID_COL],\n            \"Row % Missing (all fields)\": row_pct,\n            \"Missing Cells\": df2[cols_eval].isna().sum(axis=1),\n            \"Total Cells\": len(cols_eval)\n        })\n        plm_list.append(tmp)\n    patient_level_missing = pd.concat(plm_list, ignore_index=True) if plm_list else pd.DataFrame()\n\n    # ---- Dynamic (intraday) Ramadan/Shawwal ----\n    final_dynamic = pd.DataFrame()\n    if INCLUDE_INTRADAY and INTRADAY_CSV_PATH.exists():\n        try:\n            intraday = pd.read_csv(INTRADAY_CSV_PATH)\n            if \"date\" in intraday.columns:\n                intraday[\"date\"] = pd.to_datetime(intraday[\"date\"], errors=\"coerce\")\n                ram = intraday[(intraday[\"date\"] >= RAMADAN_START) & (intraday[\"date\"] <= RAMADAN_END)]\n                shw = intraday[(intraday[\"date\"] >= SHAWWAL_START) & (intraday[\"date\"] <= SHAWWAL_END)]\n\n                dyn_cols = [c for c in intraday.columns if c not in INTRADAY_EXCLUDE_COLS]\n\n                rows = []\n                for feat in dyn_cols:\n                    r_miss = ram[feat].isna().mean() * 100 if len(ram) else np.nan\n                    s_miss = shw[feat].isna().mean() * 100 if len(shw) else np.nan\n                    both = pd.concat([ram[feat], shw[feat]]) if len(ram) or len(shw) else pd.Series(dtype=float)\n                    overall = both.isna().mean() * 100 if len(both) else np.nan\n                    rows.append({\n                        \"Feature\": feat,\n                        \"% Missing Ramadan\": round(r_miss, 2) if pd.notna(r_miss) else np.nan,\n                        \"% Missing Shawwal\": round(s_miss, 2) if pd.notna(s_miss) else np.nan,\n                        \"Overall % Missing\": round(overall, 2) if pd.notna(overall) else np.nan,\n                        \"Availability (Visits)\": json.dumps([\"Ramadan (intraday)\", \"Shawwal (intraday)\"])\n                    })\n                final_dynamic = pd.DataFrame(rows)\n\n                # Dictionary enrichment (percent-aware)\n                dict_core_dyn = dict_core  # reuse if available\n                if dict_core_dyn is not None and not final_dynamic.empty:\n                    final_dynamic[\"Feature_base\"]      = final_dynamic[\"Feature\"].map(clean_name_base)\n                    final_dynamic[\"Feature_is_pct\"]    = final_dynamic[\"Feature\"].map(has_percent)\n                    final_dynamic[\"Feature_canonical\"] = final_dynamic[\"Feature\"].map(normalize_to_canonical)\n\n                    merged_dyn = final_dynamic.merge(\n                        dict_core_dyn.add_suffix(\"_dict\"),\n                        left_on=\"Feature_canonical\",\n                        right_on=\"Feature_canonical_dict\",\n                        how=\"left\"\n                    )\n                    # strict fallback on (base, is_pct)\n                    need_fb = merged_dyn[\"Category_dict\"].isna() if \"Category_dict\" in merged_dyn.columns else pd.Series(False, index=merged_dyn.index)\n                    if need_fb.any():\n                        fb_left = final_dynamic.loc[need_fb, [\"Feature\", \"Feature_base\", \"Feature_is_pct\"]].copy()\n                        fb_right = dict_core_dyn.add_suffix(\"_dict\")[\n                            [\"Feature_dict\", \"Feature_base_dict\", \"Feature_is_pct_dict\",\n                             \"Category_dict\", \"Definition & Unit_dict\", \"Unit/Type_dict\", \"Feature Type_dict\", \"Subtype_dict\"]\n                        ]\n                        fb_joined = fb_left.merge(\n                            fb_right,\n                            left_on=[\"Feature_base\", \"Feature_is_pct\"],\n                            right_on=[\"Feature_base_dict\", \"Feature_is_pct_dict\"],\n                            how=\"left\"\n                        )\n                        idx = merged_dyn.index[need_fb]\n                        for col in fb_joined.columns:\n                            if col.endswith(\"_dict\") and col in merged_dyn.columns:\n                                merged_dyn.loc[idx, col] = fb_joined[col].values\n\n                    for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\"]:\n                        lc, rc = c, f\"{c}_dict\"\n                        if lc not in merged_dyn.columns:\n                            merged_dyn[lc] = np.nan\n                        if rc in merged_dyn.columns:\n                            merged_dyn[lc] = merged_dyn[lc].combine_first(merged_dyn[rc])\n\n                    merged_dyn[\"Matched Dictionary Feature\"] = merged_dyn.get(\"Feature_dict\", np.nan)\n                    drop_helpers = [c for c in merged_dyn.columns if c.endswith(\"_dict\")] + \\\n                                   [\"Feature_base\", \"Feature_base_dict\", \"Feature_is_pct\", \"Feature_is_pct_dict\",\n                                    \"Feature_canonical\", \"Feature_canonical_dict\"]\n                    final_dynamic = merged_dyn.drop(columns=[c for c in drop_helpers if c in merged_dyn.columns])\n                else:\n                    for c in [\"Category\", \"Definition & Unit\", \"Unit/Type\", \"Feature Type\", \"Subtype\", \"Matched Dictionary Feature\"]:\n                        if c not in final_dynamic.columns:\n                            final_dynamic[c] = np.nan\n            else:\n                print(\"[INFO] intraday has no 'date' column; dynamic missingness skipped.\")\n        except Exception as e:\n            print(f\"[WARN] Intraday integration skipped: {e}\")\n\n    # ---- Save everything to Excel with QC formatting ----\n    with pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\") as writer:\n        # Static (always create at least one sheet to avoid openpyxl 'no visible sheet' error)\n        final_static.to_excel(writer, index=False, sheet_name=\"Missingness_Static\")\n        qc_format_excel(writer, \"Missingness_Static\")\n\n        # Dynamic (Ramadan/Shawwal)\n        if not final_dynamic.empty:\n            dyn_cols_order = [\"Category\", \"Feature\", \"Matched Dictionary Feature\", \"Definition & Unit\",\n                              \"Unit/Type\", \"Feature Type\", \"Subtype\",\n                              \"% Missing Ramadan\", \"% Missing Shawwal\", \"Overall % Missing\", \"Availability (Visits)\"]\n            dyn_cols_order = [c for c in dyn_cols_order if c in final_dynamic.columns] + \\\n                             [c for c in final_dynamic.columns if c not in dyn_cols_order]\n            final_dynamic[dyn_cols_order].to_excel(writer, index=False, sheet_name=\"Missingness_Dynamic\")\n            qc_format_excel(writer, \"Missingness_Dynamic\")\n\n        # Dynamic-by-Visit (optional stage 2 CSV)\n        if DYN_VISIT_OUT.exists():\n            try:\n                dyn_visit_df = pd.read_csv(DYN_VISIT_OUT)\n                dyn_visit_df.to_excel(writer, index=False, sheet_name=\"Missingness_Dynamic_By_Visit\")\n                qc_format_excel(writer, \"Missingness_Dynamic_By_Visit\")\n                print(\"✅ Added sheet: Missingness_Dynamic_By_Visit\")\n            except Exception as e:\n                print(f\"[INFO] Skipped Missingness_Dynamic_By_Visit: {e}\")\n\n        # Patient-level\n        if not patient_level_missing.empty:\n            patient_level_missing.to_excel(writer, index=False, sheet_name=\"Patient_Level_Missingness\")\n\n        # Unmapped audits\n        if \"Category\" in final_static.columns:\n            unmapped_static = (\n                final_static[final_static[\"Category\"].isna()][[\"Feature\"]]\n                .drop_duplicates().sort_values(\"Feature\")\n            )\n            unmapped_static.to_excel(writer, index=False, sheet_name=\"Unmapped_Static\")\n\n        if not final_dynamic.empty and \"Category\" in final_dynamic.columns:\n            unmapped_dyn = (\n                final_dynamic[final_dynamic[\"Category\"].isna()][[\"Feature\"]]\n                .drop_duplicates().sort_values(\"Feature\")\n            )\n            unmapped_dyn.to_excel(writer, index=False, sheet_name=\"Unmapped_Dynamic\")\n\n        # Name mapping audits\n        if \"Matched Dictionary Feature\" in final_static.columns:\n            mapping_static = (\n                final_static[[\"Feature\", \"Matched Dictionary Feature\"]]\n                .drop_duplicates().sort_values([\"Feature\", \"Matched Dictionary Feature\"])\n            )\n            mapping_static.to_excel(writer, index=False, sheet_name=\"Name_Mapping_Static\")\n\n        if not final_dynamic.empty and \"Matched Dictionary Feature\" in final_dynamic.columns:\n            mapping_dyn = (\n                final_dynamic[[\"Feature\", \"Matched Dictionary Feature\"]]\n                .drop_duplicates().sort_values([\"Feature\", \"Matched Dictionary Feature\"])\n            )\n            mapping_dyn.to_excel(writer, index=False, sheet_name=\"Name_Mapping_Dynamic\")\n\n    # CSVs\n    final_static.to_csv(OUT_CSV_STATIC, index=False)\n    if not final_dynamic.empty:\n        final_dynamic.to_csv(OUT_CSV_DYNAMIC, index=False)\n    if not patient_level_missing.empty:\n        patient_level_missing.to_csv(OUT_CSV_PATIENT, index=False)\n\n    print(f\"Saved Excel report → {OUT_XLSX}\")\n    print(f\"Saved static CSV   → {OUT_CSV_STATIC}\")\n    if not final_dynamic.empty:\n        print(f\"Saved dynamic CSV  → {OUT_CSV_DYNAMIC}\")\n    if not patient_level_missing.empty:\n        print(f\"Saved patient CSV  → {OUT_CSV_PATIENT}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T11:45:57.930451Z","iopub.execute_input":"2025-09-06T11:45:57.930746Z","iopub.status.idle":"2025-09-06T11:46:00.106022Z","shell.execute_reply.started":"2025-09-06T11:45:57.930727Z","shell.execute_reply":"2025-09-06T11:46:00.105096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. CGM with Dynamic Features \n\n1. Correlation heatmap → checks pairwise correlations of features vs CGM.\n2. VIF (Variance Inflation Factor) → detects collinearity (e.g., steps ↔ distance ↔ calories).\n3. Random Forest regression → finds the most important predictors of CGM.\n4. Partial Dependence + ICE plots with smoothing → visualizes how the top predictors (heart_rate, calories, steps, distance, spo2 in your example) influence CGM, both on average and at the individual sample level.\n      Blue = ICE curves (individual sample effects).\n      Orange = average PDP.\n      Red = smoothed PDP (to reduce jaggedness).\n6. PCA (95% variance) → reduces 10 correlated features to the minimum number of orthogonal components that still explain ≥95% of the total variance.\n\n## 1. VIF (Multicollinearity check)\n\nBoth periods show moderate collinearity between steps, calories, and distance (VIF ~3–5). That’s expected — people who walk more burn more calories and cover more distance.\n\nOther features (heart rate, sleep stages, SpO₂, awake, nap) are clean (VIF ~1–1.3).\n\n👉 Interpretation: nothing is screaming \"drop me immediately\", but steps/calories/distance are partially redundant. If you want a leaner model, you could collapse them (e.g., PCA or just pick one).\n\n## 2. Random Forest Feature Importance\n\nHeart rate + calories + steps are the big three drivers for CGM in both Ramadan & Shawwal.\n\nDistance still matters, but less once steps/calories are in.\n\nSpO₂ shows some relevance (~7–8%). Sleep features (deep, light, REM, nap, awake) barely move the needle (<3%).\n\n👉 Interpretation: CGM is being explained more by physical activity + cardio strain than sleep metrics.\n\n## 4. Partial Dependence + ICE plots with smoothing \n\n→ visualizes how the top predictors (heart_rate, calories, steps, distance, spo2 in your example) influence CGM, both on average and at the individual sample level.\n\nBlue = ICE curves (individual sample effects). \nOrange = average PDP. \nRed = smoothed PDP (to reduce jaggedness).\n\n## 5. PCA (Dimensionality reduction)\n\nRamadan: 10 features → 3 principal components capture 98.5% variance.\nShawwal: 10 features → 3 components capture 97.9% variance.\n\n👉 Interpretation: data is very compressible. Three latent “axes” explain basically everything. Likely:\n   Activity/energy (steps + calories + distance).\n   Physiological (heart rate + SpO₂).\n   ![](http://)Sleep/rest cycle (deep, REM, light, awake, nap).\n\n## 6. Correlation differences\n\nYou mentioned a file \"Live_CGM_Correlation_Ramadan_vs_Shawwal.xlsx\" where you want to sort properly. Probably you want to line up features side-by-side and order them consistently (e.g., by Ramadan strength or absolute correlation).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# ---------------- CONFIG ----------------\nPATH = \"intraday_with_visits.xlsx\"  # ← Update path if needed\n\nFEATURES = [\n    \"calories\", \"deep\", \"distance\", \"heart_rate\",\n    \"light\", \"nap\", \"rem\", \"spo2\", \"steps\", \"awake\"\n]\n\nTARGET = \"cgm\"\nPERIOD_COL_CANDIDATES = [\"period_main\", \"period\", \"ramadan_shawwal\", \"season\", \"phase\"]\nTOP_K = 5\n\n# ---------------- HELPERS ----------------\ndef find_period_col(df, candidates):\n    for c in candidates:\n        if c in df.columns:\n            return c\n    return None\n\ndef normalize_period(s):\n    if pd.isna(s): return np.nan\n    x = str(s).strip().lower()\n    if \"ramad\" in x: return \"Ramadan\"\n    if \"shaw\" in x: return \"Shawwal\"\n    return np.nan\n\ndef to_numeric(df, cols):\n    return df[cols].apply(pd.to_numeric, errors='coerce')\n\ndef safe_heatmap(mat, title):\n    mask = mat.isna()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(mat, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\ndef compute_vif(X_df):\n    vif = []\n    for i in range(X_df.shape[1]):\n        try:\n            v = variance_inflation_factor(X_df.values, i)\n        except Exception:\n            v = np.nan\n        vif.append(v)\n    return pd.DataFrame({'Feature': X_df.columns, 'VIF': vif}).sort_values('VIF', ascending=False)\n\ndef ensure_min_samples(df, nmin=10):\n    if len(df) < nmin:\n        print(f\"⚠️ Not enough samples: {len(df)} < {nmin}\")\n        return False\n    return True\n\n# ---------------- LOAD DATA ----------------\ndf = pd.read_excel(PATH)\ndf.columns = df.columns.str.lower().str.replace(\" \", \"_\")\nperiod_col = find_period_col(df, PERIOD_COL_CANDIDATES)\n\nif not period_col:\n    raise ValueError(\"No valid period column found.\")\n\ndf[period_col] = df[period_col].apply(normalize_period)\n\ndf_r = to_numeric(df[df[period_col] == \"Ramadan\"].copy(), FEATURES + [TARGET]).dropna(subset=[TARGET])\ndf_s = to_numeric(df[df[period_col] == \"Shawwal\"].copy(), FEATURES + [TARGET]).dropna(subset=[TARGET])\n\n# ---------------- ANALYSIS FUNCTION ----------------\ndef analyze_period(df_period, label):\n    print(f\"\\n=== {label} Analysis ===\")\n    if not ensure_min_samples(df_period): \n        return None\n\n    # --- Correlation ---\n    corr = df_period[FEATURES + [TARGET]].corr()\n    safe_heatmap(corr, f\"{label} – Feature Correlation\")\n\n    # --- VIF ---\n    X = pd.DataFrame(\n        SimpleImputer(strategy=\"median\").fit_transform(df_period[FEATURES]),\n        columns=FEATURES\n    )\n    vif_df = compute_vif(X)\n    print(\"\\nVIF:\")\n    print(vif_df)\n\n    # --- Random Forest ---\n    y = df_period[TARGET].values\n    rf = RandomForestRegressor(n_estimators=300, random_state=42)\n    rf.fit(X, y)\n    importances = pd.DataFrame({\n        \"Feature\": FEATURES,\n        \"Importance (%)\": 100 * rf.feature_importances_ / rf.feature_importances_.sum()\n    }).sort_values(\"Importance (%)\", ascending=False)\n    print(\"\\nRandom Forest Importances:\")\n    print(importances)\n\n    # --- PDP + ICE with smoothing ---\n    from sklearn.inspection import partial_dependence\n    top_feats = importances[\"Feature\"].head(TOP_K).tolist()\n    valid_feats = [f for f in top_feats if X[f].nunique() > 2]\n\n    if valid_feats:\n        try:\n            fig, ax = plt.subplots(1, len(valid_feats), figsize=(5 * len(valid_feats), 4))\n            if len(valid_feats) == 1:\n                ax = [ax]\n\n            for i, feat in enumerate(valid_feats):\n                disp = PartialDependenceDisplay.from_estimator(\n                    rf, X, [feat],\n                    kind=\"both\", subsample=200,\n                    random_state=42, ax=ax[i],\n                    percentiles=(0.01, 0.99)\n                )\n\n                # Smoothing\n                pd_results = partial_dependence(\n                    rf, X, [feat], kind=\"average\", percentiles=(0.01, 0.99)\n                )\n                xx = pd_results['values'][0]\n                yy = pd_results['average'][0]\n                smooth_y = pd.Series(yy).rolling(window=10, min_periods=1, center=True).mean()\n\n                ax[i].plot(xx, smooth_y, color=\"red\", linewidth=2, label=\"Smoothed PDP\")\n                ax[i].legend()\n\n            plt.suptitle(f\"{label} – PDP + ICE (Smoothed)\")\n            plt.tight_layout()\n            plt.show()\n\n        except Exception as e:\n            print(f\"PDP failed even after filtering: {e}\")\n    else:\n        print(f\"⚠️ No valid features for PDP in {label}\")\n\n    # --- PCA ---\n    pca = PCA(n_components=0.95, svd_solver='full')\n    pca.fit(X)\n\n\n    print(f\"\\nPCA – 95% Variance (Raw Features): {X.shape[1]} → {pca.n_components_}\")\n    print(\"Cumulative Variance:\", np.round(np.cumsum(pca.explained_variance_ratio_), 4))\n\n    # PCA loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,\n        index=FEATURES,\n        columns=[f\"PC{i+1}\" for i in range(pca.n_components_)]\n    )\n    print(\"\\nPCA Loadings (feature contributions per component):\")\n    print(loadings.round(3))\n\n \n    # --- Plot PCA loadings as heatmap ---\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(loadings, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n    plt.title(f\"{label} – PCA Loadings Heatmap\")\n    plt.xlabel(\"Principal Components\")\n    plt.ylabel(\"Features\")\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        \"corr\": corr,\n        \"vif\": vif_df,\n        \"rf_importance\": importances,\n        \"pca\": pca,\n        \"pca_loadings\": loadings\n    }\n\n\n\n# ---------------- RUN ANALYSIS ----------------\nres_r = analyze_period(df_r, \"Ramadan\")\nres_s = analyze_period(df_s, \"Shawwal\")\n\n\n\n# ---------------- OPTIONAL: EXPORT OR PLOT DIFFERENCE ----------------\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Extract correlation of each feature with CGM from the results\ncorr_r = res_r[\"corr\"][\"cgm\"].drop(\"cgm\")   # Ramadan correlations\ncorr_s = res_s[\"corr\"][\"cgm\"].drop(\"cgm\")   # Shawwal correlations\n\n# Build dataframe\ndf_corr = pd.DataFrame({\n    \"Ramadan\": corr_r,\n    \"Shawwal\": corr_s\n})\n\n# Use absolute values so all bars are positive\ndf_corr = df_corr.abs()\n\n# --- Plot Ramadan only ---\ndf_corr[\"Ramadan\"].sort_values().plot(\n    kind=\"barh\", color=\"firebrick\", figsize=(8,6)\n)\nplt.title(\"Correlation with CGM – Ramadan\")\nplt.xlabel(\"Absolute Pearson Correlation\")\nplt.tight_layout()\nplt.show()\n\n# --- Plot Shawwal only ---\ndf_corr[\"Shawwal\"].sort_values().plot(\n    kind=\"barh\", color=\"royalblue\", figsize=(8,6)\n)\nplt.title(\"Correlation with CGM – Shawwal\")\nplt.xlabel(\"Absolute Pearson Correlation\")\nplt.tight_layout()\nplt.show()\n\n# --- Combined Comparison ---\ndf_corr.sort_values(\"Ramadan\", ascending=True)[[\"Ramadan\", \"Shawwal\"]].plot(\n    kind=\"barh\",\n    color=[\"firebrick\", \"royalblue\"],\n    figsize=(10,6)\n)\nplt.title(\"Correlation with CGM – Ramadan vs. Shawwal\")\nplt.xlabel(\"Absolute Pearson Correlation\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T21:05:58.591388Z","iopub.execute_input":"2025-10-21T21:05:58.591610Z","iopub.status.idle":"2025-10-21T21:06:03.971898Z","shell.execute_reply.started":"2025-10-21T21:05:58.591587Z","shell.execute_reply":"2025-10-21T21:06:03.970276Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1079797553.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# ---------------- LOAD DATA ----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mperiod_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_period_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERIOD_COL_CANDIDATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'intraday_with_visits.xlsx'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'intraday_with_visits.xlsx'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# A: Correlation heatmap (already saved as .png)\nimg1 = mpimg.imread(\"Correlation with CGM – Ramadanl.png\")\naxes[0].imshow(img1)\naxes[0].axis(\"off\")\naxes[0].set_title(\"(A) Feature Correlation\")\n\n# B: VIF bar chart\nvif = {\"steps\":4.99,\"distance\":2.99,\"calories\":2.70,\"heart_rate\":1.26,\"spo2\":1.17,\"light\":1.07,\n       \"rem\":1.04,\"deep\":1.04,\"nap\":1.01,\"awake\":1.01}\naxes[1].barh(list(vif.keys()), list(vif.values()), color=\"skyblue\")\naxes[1].axvline(5, color=\"r\", linestyle=\"--\", label=\"threshold\")\naxes[1].invert_yaxis()\naxes[1].set_title(\"(B) VIF < 5 – Acceptable collinearity\")\naxes[1].set_xlabel(\"VIF\")\n\n# C: Random Forest importance\nrf_imp = {\"heart_rate\":29.35,\"calories\":26.78,\"steps\":17.13,\"distance\":11.03,\"spo2\":7.68,\n          \"light\":2.61,\"rem\":2.10,\"deep\":1.26,\"awake\":1.22,\"nap\":0.84}\naxes[2].barh(list(rf_imp.keys()), list(rf_imp.values()), color=\"orange\")\naxes[2].invert_yaxis()\naxes[2].set_title(\"(C) Random Forest Importance\")\naxes[2].set_xlabel(\"Importance (%)\")\n\nplt.tight_layout()\nplt.savefig(\"Feature_Stability_Importance_Figure_Sx.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PCA Components Conceptual Diagram ","metadata":{}},{"cell_type":"code","source":"# ================= PCA Components Conceptual Diagram ==================\nimport matplotlib.pyplot as plt\n\n# Define conceptual components\npoints = {\n    \"PC1: Activity / Energy\\n(steps, distance, calories)\": (0, 1),\n    \"PC2: Physiology\\n(heart_rate, SpO₂)\": (-0.87, -0.5),\n    \"PC3: Sleep / Rest Pattern\\n(deep, light, REM, nap, awake)\": (0.87, -0.5)\n}\n\n# Create plot\nfig, ax = plt.subplots(figsize=(6,6))\n\n# Plot points and labels\nfor (label, (x,y)) in points.items():\n    ax.scatter(x, y, s=500, marker=\"o\", color=\"skyblue\", edgecolors=\"k\", zorder=3)\n    ax.text(x, y+0.1, label, ha=\"center\", va=\"bottom\", fontsize=10, weight=\"bold\")\n\n# Connect points to form a triangle\ncoords = list(points.values())\nfor i in range(len(coords)):\n    x1, y1 = coords[i]\n    x2, y2 = coords[(i+1) % len(coords)]\n    ax.plot([x1, x2], [y1, y2], \"k-\", lw=1.5)\n\n# Formatting\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1, 1.2)\nax.axis(\"off\")\nax.set_title(\"PCA Components from CGM Features\", fontsize=14, weight=\"bold\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:42:10.724459Z","iopub.execute_input":"2025-09-13T13:42:10.724824Z","iopub.status.idle":"2025-09-13T13:42:10.995193Z","shell.execute_reply.started":"2025-09-13T13:42:10.724796Z","shell.execute_reply":"2025-09-13T13:42:10.994201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Great—steps 1–4 are done and you now have:\n\nfinal_master_sheet_clean_with_visits.xlsx\n\nintraday_with_visits.csv\n\nThe next milestone is to materialize PATH_HOURLY (the hourly ML-ready dataset) and also produce the two helper tables PATH_VISIT (per‑visit features) and PATH_BASE (static baseline).\n\nKaggle note: you cannot write to /kaggle/input. Write to /kaggle/working and, if needed, create a dataset from those files afterward.\n\nBelow are drop‑in cells you can paste into your Kaggle notebook. They will:\n\nBuild hourly CGM features (+ context) from intraday_with_visits.csv.\n\nExtract per‑visit features (carb, meals, TDD, fasting%) from the master workbook.\n\nExtract static baseline variables (Age, Gender, BMI, HbA1C, Cholesterol, LDL, HDL, Triglycerides, eGFR, Creatinine, Insulin_units_per_kg, SmartGuard_percent).\n\nMerge them (patientID & visit‑aware) into a single hourly ML table.\n\nSave the three CSVs under /kaggle/working/… and expose feature lists you’ll pass to the XGB/BiLSTM cells.\n\nLeakage‑safe PCA: we do not compute pca_cgm1–3 or lifestyle PCs here. You’ll compute those inside the modeling pipeline on the training fold only, as we set up earlier. The hourly CSV will carry the raw columns (CGM dynamics + activity/sleep/HR) that PCA will be derived from.","metadata":{}},{"cell_type":"markdown","source":"# Cell A — Paths & setup","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\n\n# ----- Inputs produced in your earlier steps -----\nINTRADAY_ANN = Path(\"/kaggle/working/intraday_with_visits.csv\")\nMASTER_XLSX  = Path(\"/kaggle/working/final_master_sheet_clean_with_visits.xlsx\")\n\n# ----- Outputs (write to /kaggle/working) -----\nPATH_HOURLY = Path(\"/kaggle/working/hourly_features_ml.csv\")\nPATH_VISIT  = Path(\"/kaggle/working/per_visit_features.csv\")\nPATH_BASE   = Path(\"/kaggle/working/static_baseline.csv\")\n\nprint(\"Reading:\", INTRADAY_ANN.exists(), MASTER_XLSX.exists())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:16:00.241063Z","iopub.execute_input":"2025-10-17T16:16:00.241256Z","iopub.status.idle":"2025-10-17T16:16:01.831537Z","shell.execute_reply.started":"2025-10-17T16:16:00.241236Z","shell.execute_reply":"2025-10-17T16:16:01.830847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell B — Build hourly CGM features (+ context)","metadata":{}},{"cell_type":"code","source":"# Robust loader\nintraday = pd.read_csv(INTRADAY_ANN)\ncols = {c.lower(): c for c in intraday.columns}\n\n# unify ID\npid_col = None\nfor k in [\"patientid\", \"patient_id\", \"patientid_(huawei_data)\", \"patientid_(huawei data)\", \"huaweiid\"]:\n    if k in cols:\n        pid_col = cols[k]; break\nif pid_col is None:\n    raise ValueError(\"No patient ID column found in intraday_with_visits.csv\")\n\n# unify time → 'start' and 'hour'\ntime_col = None\nfor k in [\"start\", \"datetime\", \"timestamp\", \"time\"]:\n    if k in cols:\n        time_col = cols[k]; break\nif time_col is None and \"date\" in cols:\n    # if only 'date' present, treat it as midnight timestamps and warn\n    intraday[\"start\"] = pd.to_datetime(intraday[cols[\"date\"]], errors=\"coerce\")\nelse:\n    intraday[\"start\"] = pd.to_datetime(intraday[time_col], errors=\"coerce\")\n\nintraday[\"hour\"] = intraday[\"start\"].dt.floor(\"h\")\n\n# unify CGM column (sensor glucose)\ncgm_col = None\nfor k in [\"cgm\", \"sg\", \"sensor_glucose\", \"glucose\"]:\n    if k in cols:\n        cgm_col = cols[k]; break\nif cgm_col is None:\n    raise ValueError(\"No CGM column found in intraday_with_visits.csv (expected one of: cgm, sg, sensor_glucose, glucose).\")\n\n# Optional lifestyle columns at intraday grain (aggregate later if present)\nlifestyle_candidates = [\n    \"steps\",\"calories\",\"distance\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\npresent_dyn = [cols[k] for k in lifestyle_candidates if k in cols]\n\n# Keep period/visit context if present\nvisit_col  = cols.get(\"visit_assigned\", None)\nperiod_col = cols.get(\"period_main\", None)\n\nkeep_cols = [pid_col, \"hour\", cgm_col] + [c for c in [visit_col, period_col] if c] + present_dyn\ndf = intraday[keep_cols].copy()\n\n# --- hourly CGM stats per patient-hour\ng = df.groupby([pid_col, \"hour\"], as_index=False)\n\nhourly_cgm = g.agg(\n    cgm_mean=(cgm_col, \"mean\"),\n    cgm_min =(cgm_col, \"min\"),\n    cgm_max =(cgm_col, \"max\"),\n    cgm_std =(cgm_col, \"std\")\n)\n\nhourly_cgm[\"cgm_mean_plus_std\"]  = hourly_cgm[\"cgm_mean\"] + hourly_cgm[\"cgm_std\"].fillna(0)\nhourly_cgm[\"cgm_mean_minus_std\"] = hourly_cgm[\"cgm_mean\"] - hourly_cgm[\"cgm_std\"].fillna(0)\n\n# Label: hypoglycemia within this hour (detection). (Forecast label is created later in modeling if you choose.)\nhypo_hour = (df[cgm_col] < 70).groupby([df[pid_col], df[\"hour\"]]).any().reset_index()\nhypo_hour.columns = [pid_col, \"hour\", \"hypo_label\"]\nhourly_cgm = hourly_cgm.merge(hypo_hour, on=[pid_col, \"hour\"], how=\"left\")\nhourly_cgm[\"hypo_label\"] = hourly_cgm[\"hypo_label\"].fillna(False).astype(int)\n\n# Re-attach visit/period context (take the most frequent within the hour if needed)\ndef mode_or_first(s):\n    try:\n        return s.mode(dropna=True).iloc[0]\n    except Exception:\n        return s.dropna().iloc[0] if s.notna().any() else np.nan\n\nctx = df.groupby([pid_col, \"hour\"]).agg(\n    visit_assigned=(visit_col, mode_or_first) if visit_col else (cgm_col, \"size\"),\n    period_main=(period_col, mode_or_first) if period_col else (cgm_col, \"size\")\n).reset_index()\n\nif visit_col is None:\n    ctx = ctx.drop(columns=[\"visit_assigned\"])\nif period_col is None:\n    ctx = ctx.drop(columns=[\"period_main\"])\n\nhourly = hourly_cgm.merge(ctx, on=[pid_col, \"hour\"], how=\"left\")\n\n# hour-of-day (0–23)\nhourly[\"hour_of_day\"] = pd.to_datetime(hourly[\"hour\"]).dt.hour\n\n# rename patientID column consistently\nhourly = hourly.rename(columns={pid_col: \"patientID\"})\n\nprint(\"Hourly CGM rows:\", len(hourly), \"| patients:\", hourly[\"patientID\"].nunique())\nhourly.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell C — Aggregate lifestyle to hourly (if present)","metadata":{}},{"cell_type":"code","source":"# For activity-like streams, use sums per hour; for vitals, use means.\nSUM_COLS  = [c for c in [\"steps\",\"calories\",\"distance\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\"] if c in cols]\nMEAN_COLS = [c for c in [\"heart_rate\",\"spo2\"] if c in cols]\n\nagg_parts = []\nif SUM_COLS:\n    agg_sum = df.groupby([pid_col,\"hour\"])[[cols[c] for c in SUM_COLS]].sum().reset_index()\n    # restore canonical names\n    agg_sum = agg_sum.rename(columns={cols[c]: c for c in SUM_COLS})\n    agg_parts.append(agg_sum)\n\nif MEAN_COLS:\n    agg_mean = df.groupby([pid_col,\"hour\"])[[cols[c] for c in MEAN_COLS]].mean().reset_index()\n    agg_mean = agg_mean.rename(columns={cols[c]: c for c in MEAN_COLS})\n    agg_parts.append(agg_mean)\n\nif agg_parts:\n    agg_all = agg_parts[0]\n    for k in agg_parts[1:]:\n        agg_all = agg_all.merge(k, on=[pid_col, \"hour\"], how=\"outer\")\n    agg_all = agg_all.rename(columns={pid_col: \"patientID\"})\n    hourly = hourly.merge(agg_all, on=[\"patientID\",\"hour\"], how=\"left\")\n\nprint(\"Added lifestyle columns:\", [c for c in [\"steps\",\"calories\",\"distance\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\",\"heart_rate\",\"spo2\"] if c in hourly.columns])\nhourly.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell D — Build per‑visit table (PATH_VISIT)","metadata":{}},{"cell_type":"code","source":"import re\nfrom pandas import ExcelFile\n\ndef normalize_visit_name(x: str) -> str:\n    if not isinstance(x, str): return x\n    t = x.strip()\n    m = re.search(r\"visit\\s*(\\d+)\", t, flags=re.I)\n    if m: return f\"Visit {int(m.group(1))}\"\n    if \"ramadan\" in t.lower(): return \"Ramadan\"\n    if \"shaw\" in t.lower():    return \"Shawwal\"\n    return t\n\n# Columns we want (robust fuzzy find)\nwant_keys = {\n    \"carb\": r\"carb\",                # carb(s)\n    \"meals\": r\"meal\",               # meals\n    \"tdd_u\": r\"(total\\s*daily\\s*dose|tdd)\",  # total daily dose units\n    \"fasting_pct_29\": r\"fasting.*(29|%)\"     # fasting % (out of 29 days)\n}\n\nvis_rows = []\nxls = ExcelFile(MASTER_XLSX)\nfor sheet in xls.sheet_names:\n    if sheet in {\"HMC_map_patientID\"}: \n        continue\n    dfv = pd.read_excel(MASTER_XLSX, sheet_name=sheet)\n    # locate patient id\n    pidc = None\n    for cand in [\"PatientID (Huawei Data)\",\"patientID\",\"PatientID\",\"huaweiID\"]:\n        if cand in dfv.columns: pidc = cand; break\n    if pidc is None: \n        continue\n    # pick columns if present\n    sel = { \"Visit\": normalize_visit_name(sheet), \"SourceSheet\": sheet }\n    ok = False\n    for key, pat in want_keys.items():\n        hit = [c for c in dfv.columns if re.search(pat, str(c), flags=re.I)]\n        if hit:\n            sel[key] = dfv[hit[0]]\n            ok = True\n        else:\n            sel[key] = np.nan\n    if ok:\n        tmp = pd.DataFrame({\n            \"patientID\": dfv[pidc].values,\n            \"visit\": sel[\"Visit\"],\n            \"carb\": sel[\"carb\"],\n            \"meals\": sel[\"meals\"],\n            \"total_daily_dose_u\": sel[\"tdd_u\"],\n            \"fasting_pct_29\": sel[\"fasting_pct_29\"]\n        })\n        vis_rows.append(tmp)\n\nvisit_tbl = pd.concat(vis_rows, ignore_index=True) if vis_rows else pd.DataFrame(\n    columns=[\"patientID\",\"visit\",\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_pct_29\"]\n)\n\n# Clean types\nfor c in [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_pct_29\"]:\n    if c in visit_tbl.columns:\n        visit_tbl[c] = pd.to_numeric(visit_tbl[c], errors=\"coerce\")\n\n# Save (and also keep to merge later)\nvisit_tbl.to_csv(PATH_VISIT, index=False)\nprint(f\"Saved per-visit features → {PATH_VISIT} | rows:\", len(visit_tbl), \"| visits:\", visit_tbl[\"visit\"].nunique())\nvisit_tbl.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell E — Build static baseline table (PATH_BASE)","metadata":{}},{"cell_type":"code","source":"# We’ll scan all sheets and pick the best-available columns per patient.\nSTATIC_WANT = {\n    \"Age\": [r\"^age$\"],\n    \"Gender\": [r\"^gender$\", r\"^sex$\"],\n    \"BMI\": [r\"^bmi$\"],\n    \"HbA1C\": [r\"^hba1c\\b(?!.*%)\", r\"^a1c$\"],\n    \"Cholesterol\": [r\"chol(?!\\w)\"],\n    \"LDL\": [r\"^ldl$\"],\n    \"HDL\": [r\"^hdl$\"],\n    \"Triglycerides\": [r\"^trig(lycerides)?$\",\"^tg$\"],\n    \"eGFR\": [r\"^egfr$\"],\n    \"Creatinine\": [r\"creat\"],\n    \"Insulin_units_per_kg\": [r\"units.*kg\", r\"tdd.*kg\", r\"^iu/kg$\"],\n    \"SmartGuard_percent\": [r\"smart.*guard\", r\"sg.*%\"]\n}\n\ndef first_match(df, pats):\n    for p in pats:\n        hit = [c for c in df.columns if re.search(p, str(c), flags=re.I)]\n        if hit: return hit[0]\n    return None\n\nstat_rows = []\nfor sheet in xls.sheet_names:\n    if sheet in {\"HMC_map_patientID\"}:\n        continue\n    d = pd.read_excel(MASTER_XLSX, sheet_name=sheet)\n    pidc = None\n    for cand in [\"PatientID (Huawei Data)\",\"patientID\",\"PatientID\",\"huaweiID\"]:\n        if cand in d.columns: pidc = cand; break\n    if pidc is None: \n        continue\n    rec = {\"patientID\": d[pidc]}\n    found_any = False\n    for out_name, patterns in STATIC_WANT.items():\n        col = first_match(d, patterns)\n        if col is not None:\n            rec[out_name] = pd.to_numeric(d[col], errors=\"coerce\") if d[col].dtype != object else d[col]\n            found_any = True\n    if found_any:\n        stat_rows.append(pd.DataFrame(rec))\n\nstatic_tbl = (pd.concat(stat_rows, ignore_index=True)\n              if stat_rows else pd.DataFrame({\"patientID\": []}))\n\n# collapse duplicates by patient (prefer last non-null)\nstatic_tbl = static_tbl.sort_values(\"patientID\").groupby(\"patientID\", as_index=False).agg(lambda s: s.dropna().iloc[-1] if s.notna().any() else np.nan)\n\n# Optional: derive Insulin_units_per_kg if missing and TDD/weight available (commented here; enable if you have columns)\n# if \"Insulin_units_per_kg\" not in static_tbl.columns and {\"total_daily_dose_u\",\"Weight_kg\"}.issubset(set(static_tbl.columns)):\n#     static_tbl[\"Insulin_units_per_kg\"] = static_tbl[\"total_daily_dose_u\"] / static_tbl[\"Weight_kg\"]\n\n# Save\nstatic_tbl.to_csv(PATH_BASE, index=False)\nprint(f\"Saved static baseline → {PATH_BASE} | patients:\", static_tbl[\"patientID\"].nunique())\nstatic_tbl.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell F — Merge everything → PATH_HOURLY","metadata":{}},{"cell_type":"code","source":"# Left join hourly ← per-visit by (patientID, visit_assigned)\nhourly_merged = hourly.copy()\nif \"visit_assigned\" in hourly_merged.columns:\n    # normalize visit name in visit_tbl\n    visit_tbl_norm = visit_tbl.copy()\n    visit_tbl_norm[\"visit\"] = visit_tbl_norm[\"visit\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n    hourly_merged[\"visit_assigned\"] = hourly_merged[\"visit_assigned\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n    hourly_merged = hourly_merged.merge(\n        visit_tbl_norm.rename(columns={\"visit\":\"visit_assigned\"}),\n        on=[\"patientID\",\"visit_assigned\"], how=\"left\"\n    )\nelse:\n    print(\"⚠️ No visit_assigned in hourly — skipping per-visit merge.\")\n\n# Left join hourly ← static baseline by patientID\nhourly_merged = hourly_merged.merge(static_tbl, on=\"patientID\", how=\"left\")\n\n# Final ordering\nfront = [\"patientID\",\"hour\",\"hour_of_day\"]\nctx   = [c for c in [\"visit_assigned\",\"period_main\"] if c in hourly_merged.columns]\ntarget= [\"hypo_label\"]\ncgm   = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\",\"cgm_mean_plus_std\",\"cgm_mean_minus_std\"]\nlifestyle = [c for c in [\"steps\",\"calories\",\"distance\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\",\"heart_rate\",\"spo2\"] if c in hourly_merged.columns]\npervisit  = [c for c in [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_pct_29\"] if c in hourly_merged.columns]\nstatic    = [c for c in [\"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"] if c in hourly_merged.columns]\n\ncol_order = front + ctx + target + cgm + lifestyle + pervisit + static\ncol_order += [c for c in hourly_merged.columns if c not in col_order]  # keep any extras at end\n\nhourly_merged = hourly_merged[col_order]\nhourly_merged.to_csv(PATH_HOURLY, index=False)\n\nprint(f\"✅ Saved hourly ML table → {PATH_HOURLY}\")\nprint(\"Rows:\", len(hourly_merged), \"patients:\", hourly_merged['patientID'].nunique())\nhourly_merged.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell G — Feature lists for modeling (to reuse exactly in the XGB/BiLSTM cells)","metadata":{}},{"cell_type":"code","source":"# These lists will feed directly into the modeling cells you have.\nTARGET = \"hypo_label\"\n\ndyn_cols = [c for c in [\n    # CGM dynamics (raw; PCA comes later in the pipeline, train-only)\n    \"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\",\n    \"cgm_mean_plus_std\",\"cgm_mean_minus_std\",\n    # Lifestyle raw streams aggregated hourly (PCs later, train-only)\n    \"steps\",\"calories\",\"distance\",\"deep\",\"light\",\"rem\",\"nap\",\"awake\",\"heart_rate\",\"spo2\"\n] if c in hourly_merged.columns]\n\nvisit_cols = [c for c in [\"carb\",\"meals\",\"total_daily_dose_u\",\"fasting_pct_29\"] if c in hourly_merged.columns]\nstatic_cols = [c for c in [\"Age\",\"Gender\",\"BMI\",\"HbA1C\",\"Cholesterol\",\"LDL\",\"HDL\",\"Triglycerides\",\"eGFR\",\"Creatinine\",\"Insulin_units_per_kg\",\"SmartGuard_percent\"] if c in hourly_merged.columns]\ncontext_cols = [c for c in [\"hour_of_day\",\"visit_assigned\",\"period_main\"] if c in hourly_merged.columns]\n\nprint(\"dyn_cols:\", dyn_cols)\nprint(\"visit_cols:\", visit_cols)\nprint(\"static_cols:\", static_cols)\nprint(\"context_cols:\", context_cols)\n\n# Load the final CSV as your canonical \"hourly\" frame for the next cells:\nhourly = pd.read_csv(PATH_HOURLY, parse_dates=[\"hour\"])\nprint(\"hourly shape:\", hourly.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}