{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13047461,"sourceType":"datasetVersion","datasetId":7421066}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# âœ… 1. Define the libraries and upload the dataset","metadata":{}},{"cell_type":"code","source":"# Step 1: Create a new environment\n!python -m venv cleanenv\n\n# Step 2: Activate it\n# On Windows:\n!cleanenv\\Scripts\\activate\n# On Mac/Linux:\n#source cleanenv/bin/activate\n\n# Step 3: Install only what you need\n!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.0 imbalanced-learn==0.13.0 tensorflow==2.18.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GOOD (pick one)\nimport torch                         # PyTorch only\n# OR\nimport tensorflow as tf              # TensorFlow only\n# OR\nimport jax                           # JAX only","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import TF first so cuDNN is registered once\nimport tensorflow as tf\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy, scipy, sklearn, imblearn, tensorflow as tf\n\nprint(\"numpy:\", numpy.__version__)\nprint(\"scipy:\", scipy.__version__)\nprint(\"scikit-learn:\", sklearn.__version__)\nprint(\"imbalanced-learn:\", imblearn.__version__)\nprint(\"tensorflow:\", tf.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom sklearn.utils import resample\nfrom collections import Counter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This script creates **hourly-level dynamic features** for each patient during **Ramadan** using continuous glucose monitoring (CGM) data and lifestyle metrics (activity, sleep, physiology) from wearable devices.\nItâ€™s part of a preprocessing pipeline for modeling glucose behavior and hypoglycemia risk.\n\nHereâ€™s a complete explanation ðŸ‘‡\n\n---\n\n## ðŸ§© **Overall Goal**\n\nTo transform raw timestamped CGM and wearable data into **hourly summarized features** that represent glucose dynamics, lifestyle behavior, and physiological activity during Ramadan â€” ready for statistical or machine-learning analysis.\n\n---\n\n## ðŸ§­ **1ï¸âƒ£ Load and Parse Data**\n\n* Loads the file:\n\n  ```\n  intraday_with_visits.csv\n  ```\n\n  which includes per-minute or per-sample CGM and Huawei sensor data.\n* Converts all timestamps (`start`, `date`) to datetime format.\n* Extracts:\n\n  * `hour` â†’ the nearest hour (e.g., 14:00, 15:00).\n  * `hour_of_day` â†’ the hour index (0â€“23).\n\nðŸ‘‰ *Purpose:* Prepare a unified hourly timeline for every patient.\n\n---\n\n## ðŸ“† **2ï¸âƒ£ Filter for Ramadan Period**\n\n* Keeps only data between **March 22 â€“ April 19, 2023**.\n* Ensures the dataset includes `cgm` readings (continuous glucose values).\n* Adds a **binary flag `hypo`** = `True` when CGM â‰¤ 70 mg/dL (hypoglycemia reading).\n\nðŸ‘‰ *Purpose:* Focus analysis strictly on the fasting month, removing other phases.\n\n---\n\n## â± **3ï¸âƒ£ Validate Hourly Windows**\n\n* Keeps only hours with **â‰¥4 CGM readings** to ensure data quality.\n* This removes incomplete or sparse hours.\n\nðŸ‘‰ *Purpose:* Guarantee each hourly feature represents stable glucose behavior.\n\n---\n\n## ðŸ“Š **4ï¸âƒ£ Compute Hourly CGM Statistics**\n\nFor each patient and hour:\n\n* `cgm_min` â†’ minimum glucose value\n* `cgm_max` â†’ maximum glucose value\n* `cgm_mean` â†’ mean glucose level\n* `cgm_std` â†’ standard deviation (glucose variability)\n\nAlso adds:\n\n* `hypo_label` â†’ `1` if any CGM reading in that hour was â‰¤70 mg/dL.\n\nðŸ‘‰ *Purpose:* Capture both variability and hypoglycemia presence within each hour.\n\n---\n\n## ðŸ§® **5ï¸âƒ£ Composite Glucose Features**\n\nCreates two derived indicators:\n\n* `cgm_mean_plus_std`  â†’ average + variability\n* `cgm_mean_minus_std` â†’ average â€“ variability\n\nðŸ‘‰ *Purpose:* Encode range boundaries for dynamic glucose variation.\n\n---\n\n## ðŸ§  **6ï¸âƒ£ PCA on CGM Variables**\n\n* Runs **Principal Component Analysis (PCA)** on `[cgm_min, cgm_max, cgm_mean, cgm_std]`.\n* Extracts **3 principal components** (`pca_cgm1`, `pca_cgm2`, `pca_cgm3`).\n* Reports explained variance (usually >95%).\n\nðŸ‘‰ *Purpose:* Compress CGM dynamics into orthogonal, interpretable axes â€” summarizing glucose pattern, amplitude, and variability.\n\n---\n\n## ðŸƒâ€â™€ï¸ **7ï¸âƒ£ PCA on Lifestyle / Activity / Sleep Features**\n\n* Selects available columns:\n\n  ```\n  steps, distance, calories, heart_rate, spo2, deep, light, rem, nap, awake\n  ```\n* Averages these per hour per patient.\n* Runs PCA â†’ extracts **3 lifestyle components**:\n\n  * `pc1_activity_energy` â†’ overall activity/energy output\n  * `pc2_physiology` â†’ physiological or heart-rateâ€“related factors\n  * `pc3_sleep_rest` â†’ rest and sleep quality indices\n* Reports explained variance ratio.\n\nðŸ‘‰ *Purpose:* Reduce multiple wearable signals into interpretable latent factors.\n\n---\n\n## ðŸ“‘ **8ï¸âƒ£ Finalize and Sort**\n\n* Orders the dataset by patient and hour.\n* Keeps only relevant feature columns:\n\n  ```\n  cgm_min, cgm_max, cgm_mean, cgm_std,\n  cgm_mean_plus_std, cgm_mean_minus_std,\n  pca_cgm1â€“3, pc1_activity_energy, pc2_physiology, pc3_sleep_rest, hypo_label\n  ```\n* Prints a preview of the final dataset.\n\n---\n\n## ðŸ’¾ **9ï¸âƒ£ Save Hourly Feature File**\n\nExports the final hourly-level dataset to:\n\n```\n/kaggle/working/dynamic_hourly_features_ramadan.csv\n```\n\nEach row now represents **one patient-hour** with fully engineered glucose and lifestyle features.\n\n---\n\n## âœ… **Summary in One Line**\n\n> This code aggregates intraday CGM and wearable sensor data into **hourly-level Ramadan features**, computing glucose statistics, detecting hypoglycemia, and summarizing glucose and lifestyle variability using **PCA-derived composite components** â€” producing a clean, feature-rich dataset for modeling hourly glucose dynamics during fasting.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# =========================\n# CONFIG\n# =========================\nCSV_PATH = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"  # âœ… update path if needed\nOUT_HOURLY_CSV = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\n\n# =========================\n# STEP 0: Load & prepare data\n# =========================\ndf = pd.read_csv(CSV_PATH)\n\n# Parse timestamps\ndf[\"start\"] = pd.to_datetime(df[\"start\"], errors=\"coerce\")\ndf[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\ndf[\"hour\"] = df[\"start\"].dt.floor(\"h\")\ndf[\"hour_of_day\"] = df[\"start\"].dt.hour\n\n# Numeric conversion\nfor col in df.columns:\n    if col not in [\"patientID\", \"huaweiID\", \"visit_assigned\", \"period_main\", \"start\", \"date\", \"hour\", \"hour_of_day\"]:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# =========================\n# STEP 0.1: Ramadan filter\n# =========================\ndf = df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\n# Ensure CGM exists\nif \"cgm\" not in df.columns:\n    raise ValueError(\"âŒ Dataset must include 'cgm' column.\")\ndf_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n# Hypo reading flag (<= 70 mg/dL)\ndf_cgm[\"hypo\"] = df_cgm[\"cgm\"] <= 70\n\n# =========================\n# STEP 1: Filter valid hours (â‰¥4 CGM readings)\n# =========================\nvalid_hours = (\n    df_cgm.groupby([\"patientID\", \"hour\"])\n    .filter(lambda g: g[\"cgm\"].notna().sum() >= 4)\n)\n\n# =========================\n# STEP 2: Compute hourly CGM statistics\n# =========================\nhourly_features = (\n    valid_hours\n    .groupby([\"patientID\", \"hour_of_day\", \"hour\"], as_index=False)\n    .agg(\n        cgm_min=(\"cgm\", \"min\"),\n        cgm_max=(\"cgm\", \"max\"),\n        cgm_mean=(\"cgm\", \"mean\"),\n        cgm_std=(\"cgm\", \"std\")\n    )\n)\n\n# Hypoglycemia label per hour\nhypo_per_hour = (\n    valid_hours.groupby([\"patientID\", \"hour\"])[\"cgm\"]\n    .apply(lambda x: (x < 70).any())\n    .reset_index(name=\"hypo_label\")\n)\nhourly_features = hourly_features.merge(hypo_per_hour, on=[\"patientID\", \"hour\"], how=\"left\")\n\n# =========================\n# STEP 3: Composite CGM features\n# =========================\nhourly_features[\"cgm_mean_plus_std\"] = hourly_features[\"cgm_mean\"] + hourly_features[\"cgm_std\"]\nhourly_features[\"cgm_mean_minus_std\"] = hourly_features[\"cgm_mean\"] - hourly_features[\"cgm_std\"]\n\n# =========================\n# STEP 4: PCA on CGM stats â†’ 3 components\n# =========================\npca_input_cgm = hourly_features[[\"cgm_min\", \"cgm_max\", \"cgm_mean\", \"cgm_std\"]].fillna(0)\npca_cgm = PCA(n_components=3, random_state=42)\ncgm_components = pca_cgm.fit_transform(pca_input_cgm)\n\nhourly_features[\"pca_cgm1\"] = cgm_components[:, 0]\nhourly_features[\"pca_cgm2\"] = cgm_components[:, 1]\nhourly_features[\"pca_cgm3\"] = cgm_components[:, 2]\n\nprint(\"CGM PCA explained variance:\", pca_cgm.explained_variance_ratio_.round(3))\n\n# =========================\n# STEP 5: PCA on lifestyle/activity/sleep features\n# =========================\nlifestyle_cols = [\"steps\", \"distance\", \"calories\", \"heart_rate\", \"spo2\",\n                  \"deep\", \"light\", \"rem\", \"nap\", \"awake\"]\nlifestyle_cols = [c for c in lifestyle_cols if c in df_cgm.columns]\n\nif lifestyle_cols:\n    lifestyle_hourly = (\n        df_cgm.groupby([\"patientID\", \"hour\"], as_index=False)[lifestyle_cols]\n        .mean()\n        .fillna(0)\n    )\n\n    # Merge lifestyle into hourly_features\n    hourly_features = hourly_features.merge(\n        lifestyle_hourly, on=[\"patientID\", \"hour\"], how=\"left\"\n    ).fillna(0)\n\n    # Run PCA\n    pca_life = PCA(n_components=3, random_state=42)\n    life_components = pca_life.fit_transform(hourly_features[lifestyle_cols])\n\n    hourly_features[\"pc1_activity_energy\"] = life_components[:, 0]\n    hourly_features[\"pc2_physiology\"] = life_components[:, 1]\n    hourly_features[\"pc3_sleep_rest\"] = life_components[:, 2]\n\n    print(\"Lifestyle PCA explained variance:\", pca_life.explained_variance_ratio_.round(3))\n\n# =========================\n# STEP 6: Finalize dataset\n# =========================\nhourly_features = hourly_features.sort_values([\"patientID\", \"hour\"]).reset_index(drop=True)\n\nDYNAMIC_FEATURES = [\n    \"cgm_min\", \"cgm_max\", \"cgm_mean\", \"cgm_std\",\n    \"cgm_mean_plus_std\", \"cgm_mean_minus_std\",\n    \"pca_cgm1\", \"pca_cgm2\", \"pca_cgm3\",\n    \"pc1_activity_energy\", \"pc2_physiology\", \"pc3_sleep_rest\"\n]\n\nprint(hourly_features[[\"patientID\", \"hour\"] + DYNAMIC_FEATURES + [\"hypo_label\"]].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================\n# CLEAN REWRITE â€” Ramadan features + LSTM inputs\n# ==============================================\nimport os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nCSV_INTRADAY_BASIC       = \"/kaggle/input/hmcdataset/intraday.csv\"\n\nOUT_HOURLY_CSV = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\nHYPO_CUTOFF   = 70.0          # mg/dL\nMIN_CGM_PER_H = 4\nSEQ_LEN       = 24            # hours per sequence window\n\n# Columns you *might* have (safeâ€“checked)\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\n# ---------------------------------------\n# Utilities\n# ---------------------------------------\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef standardize_and_pca(df, cols, n_components=3, random_state=42):\n    \"\"\"Return (components ndarray, explained_variance_ratio_) with standardization.\"\"\"\n    if not cols:\n        return None, None\n    X = df[cols].copy()\n    X = X.fillna(0.0).astype(float)\n    Xz = StandardScaler().fit_transform(X)\n    pca = PCA(n_components=n_components, random_state=random_state)\n    comps = pca.fit_transform(Xz)\n    return comps, pca.explained_variance_ratio_\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef filter_ramadan(df):\n    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    return df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\ndef label_hypo_per_hour(g):\n    # label=1 if ANY CGM < HYPO_CUTOFF within the hour\n    return int((g[\"cgm\"] < HYPO_CUTOFF).any())\n\n# ------------------------------------------------------\n# Part A â€” Ramadan hourly features with PCA (save CSV)\n# ------------------------------------------------------\ndef build_ramadan_hourly_features(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H\n):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    df = pd.read_csv(in_csv)\n\n    # Timestamp handling\n    df[\"start\"] = to_dt(df.get(\"start\"))\n    df[\"date\"] = pd.to_datetime(df.get(\"date\"), errors=\"coerce\")\n    df[\"hour\"] = df[\"start\"].dt.floor(\"h\")\n    df[\"hour_of_day\"] = df[\"hour\"].dt.hour\n\n    # Make numerics consistent\n    df = ensure_numeric(df)\n\n    # Ramadan window\n    df = filter_ramadan(df)\n\n    # Require CGM\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"âŒ Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # Keep only hours with enough CGM samples\n    valid_hours = (\n        df_cgm\n        .groupby([\"patientID\",\"hour\"])\n        .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # Base CGM stats per (patient, hour)\n    hourly = (\n        valid_hours\n        .groupby([\"patientID\",\"hour\"], as_index=False)\n        .agg(\n            cgm_min=(\"cgm\",\"min\"),\n            cgm_max=(\"cgm\",\"max\"),\n            cgm_mean=(\"cgm\",\"mean\"),\n            cgm_std=(\"cgm\",\"std\")\n        )\n    )\n    # Add hour_of_day\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    # Hypo label (any CGM < cutoff during that hour)\n    lab = (\n        valid_hours\n        .groupby([\"patientID\",\"hour\"])[\"cgm\"]\n        .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n        .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # Composite CGM features\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # PCA on CGM stats\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    cgm_comps, cgm_var = standardize_and_pca(hourly, cgm_cols, n_components=3)\n    if cgm_comps is not None:\n        hourly[\"pca_cgm1\"] = cgm_comps[:,0]\n        hourly[\"pca_cgm2\"] = cgm_comps[:,1]\n        hourly[\"pca_cgm3\"] = cgm_comps[:,2]\n        print(\"CGM PCA explained variance:\", np.round(cgm_var, 3))\n\n    # PCA on lifestyle/activity/sleep (if present)\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean()\n                  .fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\").fillna(0.0)\n\n        life_comps, life_var = standardize_and_pca(hourly, lifestyle_cols, n_components=3)\n        if life_comps is not None:\n            hourly[\"pc1_activity_energy\"] = life_comps[:,0]\n            hourly[\"pc2_physiology\"]      = life_comps[:,1]\n            hourly[\"pc3_sleep_rest\"]      = life_comps[:,2]\n            print(\"Lifestyle PCA explained variance:\", np.round(life_var, 3))\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    # Sort + save\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly.to_csv(out_csv, index=False)\n    print(f\"âœ… Saved Ramadan hourly dynamic features to: {out_csv}\")\n\n    return hourly\n\n# ------------------------------------------------------\n# Part B1 â€” LSTM sequences from raw intraday (CGM-only)\n# ------------------------------------------------------\ndef build_lstm_sequences_from_intraday(\n    in_csv=CSV_INTRADAY_BASIC,\n    seq_len=SEQ_LEN,\n    label_cutoff=HYPO_CUTOFF,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    feature_cols=(\"cgm_mean\",),  # CGM-only features expected\n    test_size=0.2,\n    random_state=42\n):\n    \"\"\"CGM-only version (does not have PC features).\"\"\"\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    df = pd.read_csv(in_csv)\n    df[\"start\"] = to_dt(df.get(\"start\"))\n    df[\"hour\"]  = df[\"start\"].dt.floor(\"h\")\n\n    base = df[[\"patientID\",\"hour\",\"cgm\"]].dropna(subset=[\"cgm\"])\n\n    valid = base.groupby([\"patientID\",\"hour\"]).filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n\n    feats = (\n        valid.groupby([\"patientID\",\"hour\"])\n             .agg(\n                 cgm_std=(\"cgm\",\"std\"),\n                 cgm_min=(\"cgm\",\"min\"),\n                 cgm_mean=(\"cgm\",\"mean\"),\n                 cgm_max=(\"cgm\",\"max\"),\n                 hypo_label=(\"cgm\", lambda x: int((x < label_cutoff).any()))\n             )\n             .reset_index()\n             .sort_values([\"patientID\",\"hour\"])\n             .reset_index(drop=True)\n    )\n\n    # leakage-proof split\n    unique_patients = feats[\"patientID\"].unique()\n    train_pat, test_pat = train_test_split(unique_patients, test_size=test_size, random_state=random_state)\n    train_df = feats[feats[\"patientID\"].isin(train_pat)].copy()\n    test_df  = feats[feats[\"patientID\"].isin(test_pat)].copy()\n\n    # build windows\n    feature_cols = list(feature_cols)\n    for col in feature_cols + [\"hypo_label\"]:\n        if col not in feats.columns:\n            raise KeyError(f\"Column '{col}' not found in CGM-only features. \"\n                           f\"Use build_lstm_sequences_from_hourly() if you need '{col}'.\")\n\n    def build_sequences(df_in, feature_cols, label_col=\"hypo_label\", seq_len=seq_len):\n        X, y = [], []\n        for pid, grp in df_in.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            for i in range(len(grp) - seq_len):\n                X.append(grp.loc[i:i+seq_len-1, feature_cols].values)\n                y.append(int(grp.loc[i+seq_len, label_col]))\n        return np.array(X), np.array(y)\n\n    X_train, y_train = build_sequences(train_df, feature_cols)\n    X_test,  y_test  = build_sequences(test_df,  feature_cols)\n\n    print(f\"âœ… Sequences (CGM-only) â€” train: {X_train.shape}, test: {X_test.shape} | features={feature_cols}\")\n    return (X_train, y_train, X_test, y_test), (train_df, test_df)\n\n# ------------------------------------------------------\n# Part B2 â€” LSTM sequences from HOURLY CSV (supports PCs)\n# ------------------------------------------------------\ndef build_lstm_sequences_from_hourly(\n    hourly_df_or_path=OUT_HOURLY_CSV,\n    seq_len=SEQ_LEN,\n    feature_cols=(\"cgm_mean\",\"cgm_std\",\"pc1_activity_energy\"),  # includes PC feature(s)\n    test_size=0.2,\n    random_state=42\n):\n    \"\"\"\n    Build sequences directly from the Ramadan hourly features CSV/DataFrame.\n    Use this when you want columns like 'pc1_activity_energy', 'pca_cgm1', etc.\n    \"\"\"\n    if isinstance(hourly_df_or_path, (str, Path)):\n        if not os.path.exists(hourly_df_or_path):\n            raise FileNotFoundError(f\"Hourly features not found: {hourly_df_or_path}\")\n        hourly = pd.read_csv(hourly_df_or_path)\n    else:\n        hourly = hourly_df_or_path.copy()\n\n    required = {\"patientID\",\"hour\",\"hypo_label\"}\n    missing = required - set(hourly.columns)\n    if missing:\n        raise KeyError(f\"Hourly features missing required columns: {missing}\")\n\n    # Ensure hour is datetime\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n\n    # Keep only required + wanted features\n    feature_cols = list(feature_cols)\n    missing_feats = [c for c in feature_cols if c not in hourly.columns]\n    if missing_feats:\n        raise KeyError(f\"Requested feature(s) not found in hourly CSV: {missing_feats}\")\n\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n\n    # leakage-proof split\n    unique_patients = hourly[\"patientID\"].unique()\n    train_pat, test_pat = train_test_split(unique_patients, test_size=test_size, random_state=random_state)\n    train_df = hourly[hourly[\"patientID\"].isin(train_pat)].copy()\n    test_df  = hourly[hourly[\"patientID\"].isin(test_pat)].copy()\n\n    def build_sequences(df_in, feature_cols, label_col=\"hypo_label\", seq_len=seq_len):\n        X, y = [], []\n        for pid, grp in df_in.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            for i in range(len(grp) - seq_len):\n                X.append(grp.loc[i:i+seq_len-1, feature_cols].values)\n                y.append(int(grp.loc[i+seq_len, label_col]))\n        return np.array(X), np.array(y)\n\n    X_train, y_train = build_sequences(train_df, feature_cols)\n    X_test,  y_test  = build_sequences(test_df,  feature_cols)\n\n    print(f\"âœ… Sequences (Hourly/PCs) â€” train: {X_train.shape}, test: {X_test.shape} | features={feature_cols}\")\n    return (X_train, y_train, X_test, y_test), (train_df, test_df)\n\n# --------------------\n# Run both parts\n# --------------------\nif __name__ == \"__main__\":\n    # Part A: Ramadan hourly features with PCA\n    try:\n        hourly_features = build_ramadan_hourly_features(\n            in_csv=CSV_INTRADAY_WITH_VISITS,\n            out_csv=OUT_HOURLY_CSV,\n            min_cgm_per_hour=MIN_CGM_PER_H\n        )\n    except Exception as e:\n        print(f\"[WARN] Skipping Ramadan feature build: {e}\")\n        hourly_features = None\n\n    # OPTION 1 â€” CGM-only sequences from raw intraday.csv\n    # (Use this when you're NOT using pc1_activity_energy / PCA lifestyle features)\n    # (X_train, y_train, X_test, y_test), _ = build_lstm_sequences_from_intraday(\n    #     in_csv=CSV_INTRADAY_BASIC,\n    #     seq_len=SEQ_LEN,\n    #     label_cutoff=HYPO_CUTOFF,\n    #     min_cgm_per_hour=MIN_CGM_PER_H,\n    #     feature_cols=(\"cgm_mean\",\"cgm_std\",\"cgm_min\",\"cgm_max\")\n    # )\n\n    # OPTION 2 â€” Sequences from HOURLY CSV (so you can use PC features)\n    try:\n        (X_train, y_train, X_test, y_test), _ = build_lstm_sequences_from_hourly(\n            hourly_df_or_path=OUT_HOURLY_CSV,\n            seq_len=SEQ_LEN,\n            feature_cols=(\"cgm_mean\",\"cgm_std\",\"pc1_activity_energy\")  # âœ… now valid\n        )\n    except Exception as e:\n        print(f\"[WARN] Skipping hourly-based sequence build: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T23:19:43.176832Z","iopub.execute_input":"2025-10-17T23:19:43.177345Z","iopub.status.idle":"2025-10-17T23:20:03.760734Z","shell.execute_reply.started":"2025-10-17T23:19:43.177321Z","shell.execute_reply":"2025-10-17T23:20:03.759911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport joblib\n\ndef build_ramadan_hourly_features_leakfree(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.2,\n    random_state=42,\n    save_artifacts=True\n):\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    df = pd.read_csv(in_csv)\n\n    # --- timestamps / numerics\n    df[\"start\"] = to_dt(df.get(\"start\"))\n    # If \"date\" column is unreliable, derive it from start:\n    if \"date\" not in df.columns or df[\"date\"].isna().all():\n        df[\"date\"] = df[\"start\"].dt.tz_localize(None).dt.date\n        df[\"date\"] = pd.to_datetime(df[\"date\"])\n\n    df[\"hour\"] = df[\"start\"].dt.floor(\"h\")\n    df[\"hour_of_day\"] = df[\"hour\"].dt.hour\n    df = ensure_numeric(df)\n\n    # --- Ramadan filter\n    df = filter_ramadan(df)\n\n    # --- require CGM\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"âŒ Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # --- only hours with enough CGM samples\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # --- base hourly CGM stats\n    hourly = (\n        valid_hours\n        .groupby([\"patientID\",\"hour\"], as_index=False)\n        .agg(\n            cgm_min=(\"cgm\",\"min\"),\n            cgm_max=(\"cgm\",\"max\"),\n            cgm_mean=(\"cgm\",\"mean\"),\n            cgm_std=(\"cgm\",\"std\")\n        )\n        .sort_values([\"patientID\",\"hour\"])\n        .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    # --- hypo label\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n        .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n        .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # --- composite CGM features\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # --- optional lifestyle block\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean()\n                  .fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\").fillna(0.0)\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    # --- split patients BEFORE PCA to avoid leakage\n    unique_patients = hourly[\"patientID\"].dropna().unique()\n    train_pat, test_pat = train_test_split(unique_patients, test_size=test_size, random_state=random_state)\n    train_df = hourly[hourly[\"patientID\"].isin(train_pat)].copy()\n    test_df  = hourly[hourly[\"patientID\"].isin(test_pat)].copy()\n\n    # --- fit CGM PCA on train only, transform both\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    scal_cgm = StandardScaler().fit(train_df[cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(scal_cgm.transform(train_df[cgm_cols].fillna(0.0)))\n\n    def _apply_cgm(df):\n        X = scal_cgm.transform(df[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n\n    train_df = _apply_cgm(train_df)\n    test_df  = _apply_cgm(test_df)\n\n    # --- lifestyle PCA (if present), also fit on train only\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(train_df[lifestyle_cols])\n        pca_life  = PCA(n_components=3, random_state=random_state).fit(scal_life.transform(train_df[lifestyle_cols]))\n\n        def _apply_life(df):\n            X = scal_life.transform(df[lifestyle_cols])\n            Z = pca_life.transform(X)\n            out = df.copy()\n            out[\"pc1_activity_energy\"] = Z[:,0]\n            out[\"pc2_physiology\"]      = Z[:,1]\n            out[\"pc3_sleep_rest\"]      = Z[:,2]\n            return out\n\n        train_df = _apply_life(train_df)\n        test_df  = _apply_life(test_df)\n\n    # --- save & return (combined for convenience)\n    hourly_out = pd.concat([train_df, test_df], axis=0).sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly_out.to_csv(out_csv, index=False)\n    print(f\"âœ… Saved leakâ€‘free Ramadan hourly features to: {out_csv}\")\n\n    # Optionally save artifacts for reproducibility\n    if save_artifacts:\n        os.makedirs(\"/kaggle/working/artifacts\", exist_ok=True)\n        joblib.dump({\"scal_cgm\":scal_cgm, \"pca_cgm\":pca_cgm}, \"/kaggle/working/artifacts/cgm_pca.joblib\")\n        if lifestyle_cols:\n            joblib.dump({\"scal_life\":scal_life, \"pca_life\":pca_life, \"cols\":lifestyle_cols},\n                        \"/kaggle/working/artifacts/life_pca.joblib\")\n\n    return hourly_out, (train_pat, test_pat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T23:43:53.325277Z","iopub.execute_input":"2025-10-17T23:43:53.325547Z","iopub.status.idle":"2025-10-17T23:43:53.340651Z","shell.execute_reply.started":"2025-10-17T23:43:53.325528Z","shell.execute_reply":"2025-10-17T23:43:53.339949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_lstm_sequences_from_hourly(\n    hourly_df_or_path=OUT_HOURLY_CSV,\n    seq_len=SEQ_LEN,\n    feature_cols=(\"cgm_mean\",\"cgm_std\",\"pc1_activity_energy\"),\n    split_patients=None  # <-- (train_pat, test_pat)\n):\n    if isinstance(hourly_df_or_path, (str, Path)):\n        if not os.path.exists(hourly_df_or_path):\n            raise FileNotFoundError(f\"Hourly features not found: {hourly_df_or_path}\")\n        hourly = pd.read_csv(hourly_df_or_path)\n    else:\n        hourly = hourly_df_or_path.copy()\n\n    req = {\"patientID\",\"hour\",\"hypo_label\"}\n    missing = req - set(hourly.columns)\n    if missing:\n        raise KeyError(f\"Hourly features missing required columns: {missing}\")\n\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n    feature_cols = list(feature_cols)\n    miss = [c for c in feature_cols if c not in hourly.columns]\n    if miss:\n        raise KeyError(f\"Requested feature(s) not found in hourly CSV: {miss}\")\n\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n\n    if split_patients is None:\n        raise ValueError(\"Pass split_patients=(train_pat, test_pat) from the leakâ€‘free PCA step.\")\n    train_pat, test_pat = split_patients\n    train_df = hourly[hourly[\"patientID\"].isin(train_pat)].copy()\n    test_df  = hourly[hourly[\"patientID\"].isin(test_pat)].copy()\n\n    def build_sequences(df_in, feature_cols, label_col=\"hypo_label\", seq_len=SEQ_LEN):\n        X, y = [], []\n        for pid, grp in df_in.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            for i in range(len(grp) - seq_len):\n                X.append(grp.loc[i:i+seq_len-1, feature_cols].values)\n                y.append(int(grp.loc[i+seq_len, label_col]))\n        return np.array(X), np.array(y)\n\n    X_train, y_train = build_sequences(train_df, feature_cols)\n    X_test,  y_test  = build_sequences(test_df,  feature_cols)\n\n    print(f\"âœ… Sequences (Hourly/PCs, leakâ€‘free) â€” train: {X_train.shape}, test: {X_test.shape} | features={feature_cols}\")\n    return (X_train, y_train, X_test, y_test), (train_df, test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T00:14:47.311052Z","iopub.execute_input":"2025-10-18T00:14:47.311829Z","iopub.status.idle":"2025-10-18T00:14:47.321207Z","shell.execute_reply.started":"2025-10-18T00:14:47.31178Z","shell.execute_reply":"2025-10-18T00:14:47.320451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T00:14:52.948272Z","iopub.execute_input":"2025-10-18T00:14:52.948568Z","iopub.status.idle":"2025-10-18T00:15:06.23163Z","shell.execute_reply.started":"2025-10-18T00:14:52.948549Z","shell.execute_reply":"2025-10-18T00:15:06.230766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================\n# Leak-free Ramadan features + Balanced LSTM\n# ==============================================\n\n# (Optional) pin versions in Kaggle/Colab:\n!pip install --upgrade pip\n!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.1 imbalanced-learn==0.13.0 tensorflow==2.18.0 joblib==1.4.2\n\nimport os\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport random\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport joblib\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nCSV_INTRADAY_BASIC       = \"/kaggle/input/hmcdataset/intraday.csv\"\n\nOUT_HOURLY_CSV = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\nHYPO_CUTOFF   = 70.0          # mg/dL\nMIN_CGM_PER_H = 4\nSEQ_LEN       = 36            # hours per sequence window\n\n# Columns you *might* have (safeâ€“checked)\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\n# Training config\nRANDOM_STATE     = 42\nTHR_MIN, THR_MAX = 0.40, 0.60\nAUGMENT_SIGMA    = 0.01  # small Gaussian jitter on train (optional); set None to disable\nRESAMPLE_METHODS = [\n    \"none\",           # baseline (class_weight + focal)\n    \"oversample_seq\", # duplicate minority sequences\n    \"undersample_seq\",# downsample majority sequences\n    \"smote\",          # SMOTE on flattened sequences\n    \"smoteenn\",       # SMOTE+ENN on flattened sequences\n    \"smotetomek\"      # SMOTE+Tomek on flattened sequences\n]\n\n# --------------------\n# General Utilities\n# --------------------\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef filter_ramadan(df):\n    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    return df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\ndef set_global_seeds(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nset_global_seeds(RANDOM_STATE)\n\n# ------------------------------------------------------\n# Part A â€” Ramadan hourly features with leak-free PCA\n# ------------------------------------------------------\ndef build_ramadan_hourly_features_leakfree(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n    save_artifacts=True\n):\n    \"\"\"\n    Build Ramadan hourly features, splitting patients BEFORE any learned transform (scaling/PCA)\n    to avoid leakage. Fits scalers/PCA on train patients only, then transforms test patients.\n\n    Returns:\n      hourly_out: combined DataFrame with engineered features (train+test concatenated)\n      (train_pat, test_pat): tuple of patient ID arrays used for the split\n    \"\"\"\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    df = pd.read_csv(in_csv)\n\n    # --- timestamps / numerics\n    df[\"start\"] = to_dt(df.get(\"start\"))\n    # Derive date if missing/unreliable\n    if \"date\" not in df.columns or df[\"date\"].isna().all():\n        # Safe naive date regardless of tz\n        df[\"date\"] = pd.to_datetime(df[\"start\"].dt.date)\n\n    df[\"hour\"] = df[\"start\"].dt.floor(\"h\")\n    df[\"hour_of_day\"] = df[\"hour\"].dt.hour\n    df = ensure_numeric(df)\n\n    # --- Ramadan filter\n    df = filter_ramadan(df)\n\n    # --- require CGM\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"âŒ Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # --- only hours with enough CGM samples\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # --- base hourly CGM stats\n    hourly = (\n        valid_hours\n        .groupby([\"patientID\",\"hour\"], as_index=False)\n        .agg(\n            cgm_min=(\"cgm\",\"min\"),\n            cgm_max=(\"cgm\",\"max\"),\n            cgm_mean=(\"cgm\",\"mean\"),\n            cgm_std=(\"cgm\",\"std\")\n        )\n        .sort_values([\"patientID\",\"hour\"])\n        .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    # --- hypo label (any CGM < cutoff during the hour)\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n        .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n        .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # --- composite CGM features\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # --- optional lifestyle block\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean()\n                  .fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\").fillna(0.0)\n    else:\n        hourly[\"pc1_activity_energy\"] = 0.0\n        hourly[\"pc2_physiology\"]      = 0.0\n        hourly[\"pc3_sleep_rest\"]      = 0.0\n\n    # --- split patients BEFORE PCA to avoid leakage\n    unique_patients = hourly[\"patientID\"].dropna().unique()\n    train_pat, test_pat = train_test_split(unique_patients, test_size=test_size, random_state=random_state)\n    train_df = hourly[hourly[\"patientID\"].isin(train_pat)].copy()\n    test_df  = hourly[hourly[\"patientID\"].isin(test_pat)].copy()\n\n    # --- fit CGM PCA on train only, transform both\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    scal_cgm = StandardScaler().fit(train_df[cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(scal_cgm.transform(train_df[cgm_cols].fillna(0.0)))\n\n    def _apply_cgm(df_in):\n        X = scal_cgm.transform(df_in[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df_in.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n\n    train_df = _apply_cgm(train_df)\n    test_df  = _apply_cgm(test_df)\n\n    # --- lifestyle PCA (if present), also fit on train only\n    if lifestyle_cols:\n        scal_life = StandardScaler().fit(train_df[lifestyle_cols])\n        pca_life  = PCA(n_components=3, random_state=random_state).fit(scal_life.transform(train_df[lifestyle_cols]))\n\n        def _apply_life(df_in):\n            X = scal_life.transform(df_in[lifestyle_cols])\n            Z = pca_life.transform(X)\n            out = df_in.copy()\n            out[\"pc1_activity_energy\"] = Z[:,0]\n            out[\"pc2_physiology\"]      = Z[:,1]\n            out[\"pc3_sleep_rest\"]      = Z[:,2]\n            return out\n\n        train_df = _apply_life(train_df)\n        test_df  = _apply_life(test_df)\n\n    # --- save & return (combined for convenience)\n    hourly_out = pd.concat([train_df, test_df], axis=0).sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly_out.to_csv(out_csv, index=False)\n    print(f\"âœ… Saved leakâ€‘free Ramadan hourly features to: {out_csv}\")\n\n    # Optionally save artifacts for reproducibility\n    if save_artifacts:\n        os.makedirs(\"/kaggle/working/artifacts\", exist_ok=True)\n        joblib.dump({\"scal_cgm\":scal_cgm, \"pca_cgm\":pca_cgm, \"cols\":cgm_cols},\n                    \"/kaggle/working/artifacts/cgm_pca.joblib\")\n        if lifestyle_cols:\n            joblib.dump({\"scal_life\":scal_life, \"pca_life\":pca_life, \"cols\":lifestyle_cols},\n                        \"/kaggle/working/artifacts/life_pca.joblib\")\n\n    return hourly_out, (train_pat, test_pat)\n\n# ------------------------------------------------------\n# Part B â€” LSTM sequences from HOURLY CSV (supports PCs)\n# ------------------------------------------------------\ndef build_lstm_sequences_from_hourly(\n    hourly_df_or_path=OUT_HOURLY_CSV,\n    seq_len=SEQ_LEN,\n    feature_cols=(\"cgm_mean\",\"cgm_std\",\"pc1_activity_energy\"),\n    split_patients=None  # <-- (train_pat, test_pat)\n):\n    \"\"\"\n    Build sequences directly from the Ramadan hourly features CSV/DataFrame.\n    Use this when you want columns like 'pc1_activity_energy', 'pca_cgm1', etc.\n    Pass split_patients from the leak-free PCA step to avoid leakage.\n    \"\"\"\n    if isinstance(hourly_df_or_path, (str, Path)):\n        if not os.path.exists(hourly_df_or_path):\n            raise FileNotFoundError(f\"Hourly features not found: {hourly_df_or_path}\")\n        hourly = pd.read_csv(hourly_df_or_path)\n    else:\n        hourly = hourly_df_or_path.copy()\n\n    req = {\"patientID\",\"hour\",\"hypo_label\"}\n    missing = req - set(hourly.columns)\n    if missing:\n        raise KeyError(f\"Hourly features missing required columns: {missing}\")\n\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n    feature_cols = list(feature_cols)\n    miss = [c for c in feature_cols if c not in hourly.columns]\n    if miss:\n        raise KeyError(f\"Requested feature(s) not found in hourly CSV: {miss}\")\n\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n\n    if split_patients is None:\n        raise ValueError(\"Pass split_patients=(train_pat, test_pat) from the leakâ€‘free PCA step.\")\n    train_pat, test_pat = split_patients\n    train_df = hourly[hourly[\"patientID\"].isin(train_pat)].copy()\n    test_df  = hourly[hourly[\"patientID\"].isin(test_pat)].copy()\n\n    def build_sequences(df_in, feature_cols, label_col=\"hypo_label\", seq_len=SEQ_LEN):\n        X, y = [], []\n        for pid, grp in df_in.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            for i in range(len(grp) - seq_len):\n                X.append(grp.loc[i:i+seq_len-1, feature_cols].values)\n                y.append(int(grp.loc[i+seq_len, label_col]))\n        return np.array(X), np.array(y)\n\n    X_train, y_train = build_sequences(train_df, feature_cols, seq_len=seq_len)\n    X_test,  y_test  = build_sequences(test_df,  feature_cols, seq_len=seq_len)\n\n    print(f\"âœ… Sequences (Hourly/PCs, leakâ€‘free) â€” train: {X_train.shape}, test: {X_test.shape} | features={feature_cols}\")\n    return (X_train, y_train, X_test, y_test), (train_df, test_df)\n\n# ------------------------------------------------------\n# Balanced LSTM pipeline utilities (metrics, resampling)\n# ------------------------------------------------------\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask]))\n        idx    = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_overall(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef _specificity_per_class(y_true, y_pred, positive_label):\n    # TNR for the \"not positive_label\" class, computed by binarizing on that label\n    y_true_bin = (np.asarray(y_true).ravel() == positive_label).astype(int)\n    y_pred_bin = (np.asarray(y_pred).ravel() == positive_label).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_per_class(y_true, y_pred, positive_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    # overall (hard preds)\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    metrics[\"Overall/Specificity\"]          = _specificity_overall(y_true, y_pred)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/MSE_pred\"]             = mse_pred\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    # prob-based\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/MSE_prob\"]       = mse_prob\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/MSE_prob\"] = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n\n    return metrics\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\ndef augment(X, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0: \n        return X, y\n    noise = np.random.normal(0, sigma, X.shape)\n    return np.vstack([X, X+noise]), np.hstack([y, y])\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE):\n    \"\"\"\n    Sequence-level resampling.\n    method âˆˆ {\n      \"none\",\n      \"oversample_seq\", \"undersample_seq\",        # window-level (no interpolation)\n      \"smote\", \"smoteenn\", \"smotetomek\"           # flattened window resampling\n    }\n    \"\"\"\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n\n    if method == \"none\":\n        return X, y\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        rng = np.random.default_rng(random_state)\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0:\n            return X, y\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = rng.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = rng.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:\n            if n0 > n1:\n                keep0 = rng.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = rng.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        rng.shuffle(keep)\n        return X[keep], y[keep]\n\n    # SMOTE family on flattened sequences with guard\n    Xf = X.reshape(n, -1)\n    minority_n = int((y==1).sum())\n    majority_n = int((y==0).sum())\n    if minority_n < 2 or majority_n < 2:\n        print(\"âš ï¸ Not enough samples for SMOTE/SMOTEENN/SMOTETomek; skipping resampling.\")\n        return X, y\n\n    if method == \"smote\":\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n    return Xr.reshape(-1, T, F), yr\n\ndef make_balanced_test(X_test, y_test, random_state=RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0: return X_test, y_test\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    return X_test[keep], y_test[keep]\n\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True), Dropout(0.2),\n            LSTM(50), Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True), Dropout(0.2),\n            LSTM(25), Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5)), Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(1e-5)), Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(1e-5)),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_25_L2\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5)), Dropout(0.2),\n            LSTM(25, kernel_regularizer=l2(1e-5)), Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l2(1e-5)),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"BiLSTM\": Sequential([\n            Input(shape=input_shape),\n            Bidirectional(LSTM(64, return_sequences=True)), Dropout(0.2),\n            Bidirectional(LSTM(32)), Dropout(0.2),\n            Dense(16, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ])\n    }\n\ndef run_balanced_lstm_pipeline(X_train, y_train, X_test, y_test,\n                               resample_methods=RESAMPLE_METHODS,\n                               thr_min=THR_MIN, thr_max=THR_MAX,\n                               random_state=RANDOM_STATE):\n    \"\"\"\n    Trains all models across resampling methods, writes plots + CSV summary,\n    and returns the results DataFrame.\n    \"\"\"\n    os.makedirs(\"checkpoints\", exist_ok=True)\n    os.makedirs(\"plots\", exist_ok=True)\n    os.makedirs(\"outputs\", exist_ok=True)\n\n    # tiny augmentation (optional)\n    X_train_aug, y_train_aug = augment(X_train, y_train, sigma=AUGMENT_SIGMA)\n\n    # balanced test copy (for fair diagnostic read)\n    X_test_bal, y_test_bal = make_balanced_test(X_test, y_test)\n\n    results = {}       # key -> metrics dict\n    roc_data = {}      # (method, model) -> (fpr, tpr, auc)\n    pr_data  = {}      # (method, model) -> (recall, precision, ap)\n    best_thresholds = {}  # (method, model) -> {\"youden\": t, \"f1\": t}\n\n    def train_eval_one(method_name, model_name, model, Xtr, ytr, Xte, yte, XteB, yteB):\n        tag = f\"{method_name}__{model_name}\"\n        print(f\"\\nðŸš€ Training [{tag}] with class-weighted focal loss\")\n        es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n        cp = ModelCheckpoint(f\"checkpoints/{tag}.h5\", save_best_only=True, monitor='val_loss', verbose=0)\n        model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n\n        class_weight = make_class_weight(ytr)\n        t0 = time.time()\n        model.fit(Xtr, ytr, epochs=5, batch_size=32,\n                  validation_data=(Xte, yte),\n                  callbacks=[es, cp], verbose=1\n                  class_weight=class_weight)\n        print(f\"â±ï¸ Training Time: {time.time()-t0:.2f}s\")\n\n        # probabilities\n        p_tr  = model.predict(Xtr,  verbose=0).ravel()\n        p_te  = model.predict(Xte,  verbose=0).ravel()\n        p_teB = model.predict(XteB, verbose=0).ravel()\n\n        # thresholds (on ORIGINAL test), constrained to [thr_min, thr_max]\n        try:\n            fpr, tpr, thr_roc = roc_curve(yte, p_te); auc_roc = auc(fpr, tpr)\n        except ValueError:\n            fpr, tpr, thr_roc, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n        youden = tpr - fpr\n        t_roc, _ = _best_threshold_in_range(thr_roc, youden, thr_min, thr_max)\n\n        prec, rec, thr_pr = precision_recall_curve(yte, p_te)\n        f1s = 2*prec[:-1]*rec[:-1] / (prec[:-1]+rec[:-1]+1e-8)\n        t_pr, _ = _best_threshold_in_range(thr_pr, f1s, thr_min, thr_max)\n        ap_val  = average_precision_score(yte, p_te)\n\n        roc_data[(method_name, model_name)] = (fpr, tpr, auc_roc)\n        pr_data[(method_name, model_name)]  = (rec, prec, ap_val)\n        best_thresholds[(method_name, model_name)] = {\"youden\": t_roc, \"f1\": t_pr}\n        print(f\"ðŸ“Œ [{tag}] thresholds â†’ Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{thr_min},{thr_max}])\")\n\n        eval_ts = sorted(set([thr_min, 0.50, thr_max, float(t_roc), float(t_pr)]))\n        # evaluate at all thresholds on train / test / testBalanced\n        for t in eval_ts:\n            yhat_tr  = (p_tr  >= t).astype(int)\n            yhat_te  = (p_te  >= t).astype(int)\n            yhat_teB = (p_teB >= t).astype(int)\n\n            results[f\"{tag}__thr_{t:.2f}__train\"]         = evaluate_full_metrics(ytr,  yhat_tr,  p_tr)\n            results[f\"{tag}__thr_{t:.2f}__test\"]          = evaluate_full_metrics(yte,  yhat_te,  p_te)\n            results[f\"{tag}__thr_{t:.2f}__testBalanced\"]  = evaluate_full_metrics(yteB, yhat_teB, p_teB)\n\n    # run all methods x models\n    input_shape = (X_train_aug.shape[1], X_train_aug.shape[2])\n    for METHOD in resample_methods:\n        Xtr_rs, ytr_rs = seq_resample(X_train_aug, y_train_aug, method=METHOD, random_state=random_state)\n        print(f\"\\nðŸ” Resampling: {METHOD} â†’ X={Xtr_rs.shape}, y={Counter(ytr_rs)}\")\n        for mname, model in define_models(input_shape).items():\n            train_eval_one(METHOD, mname, model, Xtr_rs, ytr_rs, X_test, y_test, X_test_bal, y_test_bal)\n\n    # --------------------------\n    # Curves (optional plots)\n    # --------------------------\n    plt.figure(figsize=(14,6))\n    # ROC\n    plt.subplot(1,2,1)\n    for (meth, mname), (fpr, tpr, auc_roc) in roc_data.items():\n        plt.plot(fpr, tpr, label=f'{meth}/{mname} (AUC={auc_roc:.3f})')\n    plt.plot([0,1],[0,1],'--',label='Random')\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC'); plt.legend(fontsize=8)\n    # PR\n    plt.subplot(1,2,2)\n    for (meth, mname), (rec, prec, ap) in pr_data.items():\n        plt.plot(rec, prec, label=f'{meth}/{mname} (AP={ap:.3f})')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR'); plt.legend(fontsize=8)\n    plt.tight_layout(); plt.savefig(\"plots/combined_roc_pr_curves.png\", dpi=300); plt.show()\n\n    # --------------------------\n    # Summaries\n    # --------------------------\n    results_df = pd.DataFrame(results).T\n    results_df = results_df.reset_index().rename(columns={\"index\":\"Key\"})\n\n    # Split extraction\n    k = results_df[\"Key\"].str.strip()\n    split = np.where(k.str.endswith(\"__train\"), \"train\",\n             np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n             np.where(k.str.endswith(\"__test\"), \"test\", np.nan)))\n    results_df[\"Split\"] = split\n\n    # Method, Model, Threshold extraction\n    parts = k.str.split(\"__\")\n    results_df[\"Method\"] = parts.str[0]\n    results_df[\"Model\"]  = parts.str[1]\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    with np.errstate(all='ignore'):\n        results_df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n\n    # Save\n    results_df.round(6).to_csv(\"outputs/results_summary_all2.csv\", index=False)\n    print(\"\\nðŸ“ Saved files:\")\n    print(\" - plots/combined_roc_pr_curves.png\")\n    print(\" - outputs/results_summary_all2.csv\")\n\n    # Quick sanity\n    print(\"\\nSplit counts:\")\n    print(results_df[\"Split\"].value_counts(dropna=False))\n\n    # Leaderboards\n    eval_test_df  = results_df[results_df[\"Split\"]==\"test\"].copy()\n    eval_tbal_df  = results_df[results_df[\"Split\"]==\"testBalanced\"].copy()\n\n    def top_k(df, by_col, k=10, cols=None):\n        if df.empty:\n            return pd.DataFrame(columns=(cols or []))\n        if cols is None:\n            cols = ['Method','Model','Threshold','Overall/F1_weighted','Overall/Recall_weighted',\n                    'Overall/Precision_weighted','Overall/ROC-AUC','Overall/PR-AUC','Overall/Accuracy']\n        present = [c for c in cols if c in df.columns]\n        return df.sort_values(by_col, ascending=False)[present].head(k).round(4)\n\n    print(\"\\nðŸ”½ ORIGINAL TEST â€” top by Overall/F1_weighted\")\n    print(top_k(eval_test_df, 'Overall/F1_weighted'))\n\n    print(\"\\nðŸ”½ BALANCED TEST â€” top by Overall/F1_weighted\")\n    print(top_k(eval_tbal_df, 'Overall/F1_weighted'))\n\n    def best_per_model(df):\n        if df.empty:\n            return df\n        idx = df.groupby(['Method','Model'])['Overall/F1_weighted'].idxmax()\n        return df.loc[idx].sort_values(['Overall/F1_weighted'], ascending=False)\n\n    overall_cols = [\n        'Overall/Accuracy','Overall/Precision_macro','Overall/Recall_macro','Overall/F1_macro',\n        'Overall/Precision_weighted','Overall/Recall_weighted','Overall/F1_weighted',\n        'Overall/Specificity','Overall/ROC-AUC','Overall/PR-AUC',\n        'Overall/MSE_pred','Overall/RMSE_pred','Overall/MSE_prob','Overall/RMSE_prob'\n    ]\n    class_cols = [\n        'Class0/Precision','Class0/Recall','Class0/F1','Class0/Specificity','Class0/Support',\n        'Class1/Precision','Class1/Recall','Class1/F1','Class1/Specificity','Class1/Support'\n    ]\n\n    print(\"\\n=== ORIGINAL TEST â€” best per (Method,Model) ===\")\n    best_test = best_per_model(eval_test_df)\n    if best_test.empty:\n        print(\"âš ï¸ No TEST rows found â€” check Split counts above.\")\n    else:\n        print(best_test[['Method','Model','Threshold']+[c for c in overall_cols if c in best_test.columns]].round(4))\n        print(\"\\n--- Per-class breakdown:\")\n        print(best_test[['Method','Model','Threshold']+[c for c in class_cols if c in best_test.columns]].round(4))\n\n    print(\"\\n=== BALANCED TEST â€” best per (Method,Model) ===\")\n    best_tbal = best_per_model(eval_tbal_df)\n    if best_tbal.empty:\n        print(\"âš ï¸ No BALANCED TEST rows found â€” check Split counts above.\")\n    else:\n        print(best_tbal[['Method','Model','Threshold']+[c for c in overall_cols if c in best_tbal.columns]].round(4))\n        print(\"\\n--- Per-class breakdown:\")\n        print(best_tbal[['Method','Model','Threshold']+[c for c in class_cols if c in best_tbal.columns]].round(4))\n\n    return results_df\n\n# ------------------------------------------------------\n# CGM-only (raw intraday) sequence builder\n# ------------------------------------------------------\ndef build_cgm_only_sequences_from_intraday(\n    in_csv=CSV_INTRADAY_BASIC,\n    seq_len=SEQ_LEN,\n    label_cutoff=HYPO_CUTOFF,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    feature_cols=(\"cgm_mean\",\"cgm_std\",\"cgm_min\",\"cgm_max\"),  # customize: (\"cgm_mean\",\"cgm_std\",\"cgm_min\",\"cgm_max\")\n    test_size=0.3,\n    random_state=RANDOM_STATE\n):\n    \"\"\"\n    Build CGM-only hourly features from intraday and convert to sequences.\n    Splits by patient to avoid leakage.\n    \"\"\"\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    df = pd.read_csv(in_csv)\n    df[\"start\"] = to_dt(df.get(\"start\"))\n    df[\"hour\"]  = df[\"start\"].dt.floor(\"h\")\n\n    base = df[[\"patientID\",\"hour\",\"cgm\"]].dropna(subset=[\"cgm\"])\n    valid = base.groupby([\"patientID\",\"hour\"]).filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n\n    feats = (\n        valid.groupby([\"patientID\",\"hour\"])\n             .agg(\n                 cgm_std=(\"cgm\",\"std\"),\n                 cgm_min=(\"cgm\",\"min\"),\n                 cgm_mean=(\"cgm\",\"mean\"),\n                 cgm_max=(\"cgm\",\"max\"),\n                 hypo_label=(\"cgm\", lambda x: int((x < label_cutoff).any()))\n             )\n             .reset_index()\n             .sort_values([\"patientID\",\"hour\"])\n             .reset_index(drop=True)\n    )\n\n    # leakage-proof split\n    unique_patients = feats[\"patientID\"].unique()\n    train_pat, test_pat = train_test_split(unique_patients, test_size=test_size, random_state=random_state)\n    train_df = feats[feats[\"patientID\"].isin(train_pat)].copy()\n    test_df  = feats[feats[\"patientID\"].isin(test_pat)].copy()\n\n    feature_cols = list(feature_cols)\n    for col in feature_cols + [\"hypo_label\"]:\n        if col not in feats.columns:\n            raise KeyError(f\"Column '{col}' not found. Available: {feats.columns.tolist()}\")\n\n    def build_sequences(df_in, feature_cols, label_col=\"hypo_label\", seq_len=seq_len):\n        X, y = [], []\n        for pid, grp in df_in.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            for i in range(len(grp) - seq_len):\n                X.append(grp.loc[i:i+seq_len-1, feature_cols].values)\n                y.append(int(grp.loc[i+seq_len, label_col]))\n        return np.array(X), np.array(y)\n\n    X_train, y_train = build_sequences(train_df, feature_cols)\n    X_test,  y_test  = build_sequences(test_df,  feature_cols)\n\n    print(f\"âœ… Sequences (CGM-only) â€” train: {X_train.shape}, test: {X_test.shape} | features={feature_cols}\")\n    return (X_train, y_train, X_test, y_test), (train_df, test_df)\n\n# ------------------------------------------------------\n# Main control flags\n# ------------------------------------------------------\nRUN_FEATURE_BUILD          = False  # Build Ramadan hourly features + leak-free PCA\nRUN_TRAINING_FROM_HOURLY   = True  # Train models using sequences built from hourly features\nRUN_TRAINING_FROM_CGM_ONLY = False  # Train models using CGM-only sequences from raw intraday\n\n# ------------------------------------------------------\n# Orchestrate\n# ------------------------------------------------------\nif __name__ == \"__main__\":\n    hourly_features = None\n    split_pat       = None\n\n    # Part A: Ramadan hourly features with leak-free PCA\n    if RUN_FEATURE_BUILD or RUN_TRAINING_FROM_HOURLY:\n        try:\n            hourly_features, split_pat = build_ramadan_hourly_features_leakfree(\n                in_csv=CSV_INTRADAY_WITH_VISITS,\n                out_csv=OUT_HOURLY_CSV,\n                min_cgm_per_hour=MIN_CGM_PER_H,\n                test_size=0.4,\n                random_state=RANDOM_STATE\n            )\n        except Exception as e:\n            print(f\"[WARN] Skipping Ramadan feature build: {e}\")\n\n    # Part B option 1 â€” Train from HOURLY (PC) sequences\n    if RUN_TRAINING_FROM_HOURLY:\n        try:\n            if split_pat is None:\n                raise RuntimeError(\"No patient split found. Ensure feature build step ran successfully.\")\n            (X_train, y_train, X_test, y_test), _ = build_lstm_sequences_from_hourly(\n                hourly_df_or_path=OUT_HOURLY_CSV,\n                seq_len=SEQ_LEN,\n                feature_cols=(\"cgm_mean\",\"cgm_std\",\"pc1_activity_energy\"),\n                split_patients=split_pat\n            )\n            _ = run_balanced_lstm_pipeline(X_train, y_train, X_test, y_test)\n        except Exception as e:\n            print(f\"[WARN] Skipping hourly-based training: {e}\")\n\n    # Part B option 2 â€” Train from CGM-only (raw intraday) sequences\n    if RUN_TRAINING_FROM_CGM_ONLY:\n        try:\n            (X_train_cgm, y_train_cgm, X_test_cgm, y_test_cgm), _ = build_cgm_only_sequences_from_intraday(\n                in_csv=CSV_INTRADAY_BASIC,\n                seq_len=SEQ_LEN,\n                label_cutoff=HYPO_CUTOFF,\n                min_cgm_per_hour=MIN_CGM_PER_H,\n                feature_cols=(\"cgm_mean\",)  # or: (\"cgm_mean\",\"cgm_std\",\"cgm_min\",\"cgm_max\")\n            )\n            _ = run_balanced_lstm_pipeline(X_train_cgm, y_train_cgm, X_test_cgm, y_test_cgm)\n        except Exception as e:\n            print(f\"[WARN] Skipping CGM-only training: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T00:15:19.140207Z","iopub.execute_input":"2025-10-18T00:15:19.1412Z","iopub.status.idle":"2025-10-18T00:15:19.413764Z","shell.execute_reply.started":"2025-10-18T00:15:19.141173Z","shell.execute_reply":"2025-10-18T00:15:19.412722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================\n# Leak-free Ramadan features + Balanced LSTM\n# (NO visit/daily features; lifestyle PCs kept)\n# ==============================================\n\n# (Optional) pin versions in Kaggle/Colab:\n!pip install --upgrade pip\n!pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.5.1 imbalanced-learn==0.13.0 tensorflow==2.18.0 joblib==1.4.2\n\nimport os\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport random\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n    average_precision_score, auc, mean_squared_error\n)\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport joblib\n\n# --------------------\n# GLOBAL CONFIG\n# --------------------\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\nCSV_INTRADAY_BASIC       = \"/kaggle/input/hmcdataset/intraday.csv\"\n\nOUT_HOURLY_CSV = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\n\nRAMADAN_START = pd.to_datetime(\"2023-03-22\")\nRAMADAN_END   = pd.to_datetime(\"2023-04-19\")\nHYPO_CUTOFF   = 70.0          # mg/dL\nMIN_CGM_PER_H = 4\nSEQ_LEN       = 36            # hours per sequence window\n\n# Wearable columns you *might* have (safe-checked & hour-averaged)\nLIFESTYLE_COLS_CANDIDATES = [\n    \"steps\",\"distance\",\"calories\",\"heart_rate\",\"spo2\",\n    \"deep\",\"light\",\"rem\",\"nap\",\"awake\"\n]\n\n# ==> FINAL feature set (no visit/daily broadcasts)\nALL_FEATURES = (\n    \"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\",\n    \"cgm_mean_plus_std\",\"cgm_mean_minus_std\",\n    \"pca_cgm1\",\"pca_cgm2\",\"pca_cgm3\",\n    \"pc1_activity_energy\",\"pc2_physiology\",\"pc3_sleep_rest\"\n)\n\n# Training config\nRANDOM_STATE     = 42\nTHR_MIN, THR_MAX = 0.40, 0.60\nAUGMENT_SIGMA    = 0.01  # small Gaussian jitter on train (optional); set None to disable\nRESAMPLE_METHODS = [\n    \"none\",           # baseline (class_weight + focal)\n    \"oversample_seq\", # duplicate minority sequences\n    \"undersample_seq\",# downsample majority sequences\n    \"smote\",          # SMOTE on flattened sequences\n    \"smoteenn\",       # SMOTE+ENN on flattened sequences\n    \"smotetomek\"      # SMOTE+Tomek on flattened sequences\n]\n\n# --------------------\n# General Utilities\n# --------------------\ndef to_dt(x, utc_ok=True):\n    return pd.to_datetime(x, errors=\"coerce\", utc=utc_ok)\n\ndef ensure_numeric(df, exclude=(\"patientID\",\"huaweiID\",\"visit_assigned\",\"period_main\",\"start\",\"date\",\"hour\",\"hour_of_day\")):\n    ex = set(exclude)\n    for c in df.columns:\n        if c not in ex:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef filter_ramadan(df):\n    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n    return df[(df[\"date\"] >= RAMADAN_START) & (df[\"date\"] <= RAMADAN_END)].copy()\n\ndef set_global_seeds(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nset_global_seeds(RANDOM_STATE)\n\n# ------------------------------------------------------\n# Part A â€” Ramadan hourly features with leak-free PCA\n# ------------------------------------------------------\ndef build_ramadan_hourly_features_leakfree(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    test_size=0.3,\n    random_state=RANDOM_STATE,\n    save_artifacts=True\n):\n    \"\"\"\n    Build Ramadan hourly features from *dynamic* signals only:\n      - CGM stats/composites\n      - CGM PCA (fit on train patients only)\n      - Lifestyle PCA (fit on train patients only; optional if wearable cols exist)\n    NO visit/daily broadcasts are used; NO static columns.\n\n    Returns:\n      hourly_out: combined DataFrame with engineered features (train+test concatenated)\n      (train_pat, test_pat): patient ID arrays used for the split\n    \"\"\"\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    df = pd.read_csv(in_csv)\n\n    # --- timestamps / numerics\n    df[\"start\"] = to_dt(df.get(\"start\"))\n    if \"date\" not in df.columns or df[\"date\"].isna().all():\n        df[\"date\"] = pd.to_datetime(df[\"start\"].dt.date)\n\n    df[\"hour\"] = df[\"start\"].dt.floor(\"h\")\n    df[\"hour_of_day\"] = df[\"hour\"].dt.hour\n    df = ensure_numeric(df)\n\n    # --- Ramadan filter\n    df = filter_ramadan(df)\n\n    # --- require CGM\n    if \"cgm\" not in df.columns:\n        raise ValueError(\"âŒ Dataset must include 'cgm' column.\")\n    df_cgm = df.dropna(subset=[\"cgm\"]).copy()\n\n    # --- only hours with enough CGM samples\n    valid_hours = (\n        df_cgm.groupby([\"patientID\",\"hour\"])\n              .filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n    )\n\n    # --- base hourly CGM stats\n    hourly = (\n        valid_hours\n        .groupby([\"patientID\",\"hour\"], as_index=False)\n        .agg(\n            cgm_min=(\"cgm\",\"min\"),\n            cgm_max=(\"cgm\",\"max\"),\n            cgm_mean=(\"cgm\",\"mean\"),\n            cgm_std=(\"cgm\",\"std\")\n        )\n        .sort_values([\"patientID\",\"hour\"])\n        .reset_index(drop=True)\n    )\n    hourly[\"hour_of_day\"] = hourly[\"hour\"].dt.hour\n\n    # --- hypo label (any CGM < cutoff during the hour)\n    lab = (\n        valid_hours.groupby([\"patientID\",\"hour\"])[\"cgm\"]\n        .apply(lambda x: int((x < HYPO_CUTOFF).any()))\n        .reset_index(name=\"hypo_label\")\n    )\n    hourly = hourly.merge(lab, on=[\"patientID\",\"hour\"], how=\"left\")\n\n    # --- composite CGM features\n    hourly[\"cgm_mean_plus_std\"]  = hourly[\"cgm_mean\"] + hourly[\"cgm_std\"]\n    hourly[\"cgm_mean_minus_std\"] = hourly[\"cgm_mean\"] - hourly[\"cgm_std\"]\n\n    # --- optional lifestyle block (dynamic wearable signals) to be PCA'd later\n    lifestyle_cols = [c for c in LIFESTYLE_COLS_CANDIDATES if c in df_cgm.columns]\n    if lifestyle_cols:\n        life_hourly = (\n            df_cgm.groupby([\"patientID\",\"hour\"], as_index=False)[lifestyle_cols]\n                  .mean()\n                  .fillna(0.0)\n        )\n        hourly = hourly.merge(life_hourly, on=[\"patientID\",\"hour\"], how=\"left\").fillna(0.0)\n\n    # Ensure PC columns exist (filled later / zeros if PCA not possible)\n    for col in [\"pc1_activity_energy\",\"pc2_physiology\",\"pc3_sleep_rest\"]:\n        if col not in hourly.columns:\n            hourly[col] = 0.0\n\n    # --- split patients BEFORE any PCA to avoid leakage\n    unique_patients = hourly[\"patientID\"].dropna().unique()\n    train_pat, test_pat = train_test_split(unique_patients, test_size=test_size, random_state=random_state)\n    train_df = hourly[hourly[\"patientID\"].isin(train_pat)].copy()\n    test_df  = hourly[hourly[\"patientID\"].isin(test_pat)].copy()\n\n    # --- CGM PCA on train only, transform both\n    cgm_cols = [\"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\"]\n    scal_cgm = StandardScaler().fit(train_df[cgm_cols].fillna(0.0))\n    pca_cgm  = PCA(n_components=3, random_state=random_state).fit(\n        scal_cgm.transform(train_df[cgm_cols].fillna(0.0))\n    )\n\n    def _apply_cgm(df_in):\n        X = scal_cgm.transform(df_in[cgm_cols].fillna(0.0))\n        Z = pca_cgm.transform(X)\n        out = df_in.copy()\n        out[\"pca_cgm1\"], out[\"pca_cgm2\"], out[\"pca_cgm3\"] = Z[:,0], Z[:,1], Z[:,2]\n        return out\n\n    train_df = _apply_cgm(train_df)\n    test_df  = _apply_cgm(test_df)\n\n    # --- lifestyle PCA (train-only fit), robust to <3 lifestyle features\n    if lifestyle_cols:\n        X_train_life = train_df[lifestyle_cols].fillna(0.0)\n        # PCA requires n_components <= min(n_samples, n_features)\n        n_comp_life = int(min(3, X_train_life.shape[1], max(1, X_train_life.shape[0])))\n        if n_comp_life >= 1:\n            scal_life = StandardScaler().fit(X_train_life)\n            pca_life  = PCA(n_components=n_comp_life, random_state=random_state).fit(scal_life.transform(X_train_life))\n\n            def _apply_life(df_in):\n                out = df_in.copy()\n                X = scal_life.transform(out[lifestyle_cols].fillna(0.0))\n                Z = pca_life.transform(X)\n                # write as many PCs as computed, fill the rest with zeros\n                pc_names = [\"pc1_activity_energy\",\"pc2_physiology\",\"pc3_sleep_rest\"]\n                for i in range(3):\n                    if i < n_comp_life:\n                        out[pc_names[i]] = Z[:, i]\n                    else:\n                        if pc_names[i] not in out.columns:\n                            out[pc_names[i]] = 0.0\n                return out\n\n            train_df = _apply_life(train_df)\n            test_df  = _apply_life(test_df)\n        # else: leave the pre-created zero PC columns as-is\n\n    # --- save & return (combined for convenience)\n    hourly_out = pd.concat([train_df, test_df], axis=0).sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n    hourly_out.to_csv(out_csv, index=False)\n    print(f\"âœ… Saved leakâ€‘free Ramadan hourly features to: {out_csv}\")\n\n    # Optionally save artifacts for reproducibility\n    if save_artifacts:\n        os.makedirs(\"/kaggle/working/artifacts\", exist_ok=True)\n        joblib.dump({\"scal_cgm\":scal_cgm, \"pca_cgm\":pca_cgm, \"cols\":cgm_cols},\n                    \"/kaggle/working/artifacts/cgm_pca.joblib\")\n        if lifestyle_cols and 'scal_life' in locals():\n            joblib.dump({\"scal_life\":scal_life, \"pca_life\":pca_life, \"cols\":lifestyle_cols},\n                        \"/kaggle/working/artifacts/life_pca.joblib\")\n\n    return hourly_out, (train_pat, test_pat)\n\n# ------------------------------------------------------\n# Part B â€” LSTM sequences from HOURLY CSV (supports PCs)\n# ------------------------------------------------------\ndef build_lstm_sequences_from_hourly(\n    hourly_df_or_path=OUT_HOURLY_CSV,\n    seq_len=SEQ_LEN,\n    feature_cols=ALL_FEATURES,\n    split_patients=None  # <-- (train_pat, test_pat)\n):\n    \"\"\"\n    Build sequences directly from the Ramadan hourly features CSV/DataFrame.\n    Uses ONLY dynamic signals (no statics, no visit/daily).\n    Pass split_patients from the leak-free PCA step to avoid leakage.\n    \"\"\"\n    if isinstance(hourly_df_or_path, (str, Path)):\n        if not os.path.exists(hourly_df_or_path):\n            raise FileNotFoundError(f\"Hourly features not found: {hourly_df_or_path}\")\n        hourly = pd.read_csv(hourly_df_or_path)\n    else:\n        hourly = hourly_df_or_path.copy()\n\n    req = {\"patientID\",\"hour\",\"hypo_label\"}\n    missing = req - set(hourly.columns)\n    if missing:\n        raise KeyError(f\"Hourly features missing required columns: {missing}\")\n\n    hourly[\"hour\"] = pd.to_datetime(hourly[\"hour\"], errors=\"coerce\")\n    feature_cols = list(feature_cols)\n\n    # Ensure all requested feature cols exist (some PCs might be zeros if PCA unavailable)\n    soft_missing = [c for c in feature_cols if c not in hourly.columns]\n    for c in soft_missing:\n        hourly[c] = 0.0\n\n    hourly = hourly.sort_values([\"patientID\",\"hour\"]).reset_index(drop=True)\n\n    if split_patients is None:\n        raise ValueError(\"Pass split_patients=(train_pat, test_pat) from the leakâ€‘free PCA step.\")\n    train_pat, test_pat = split_patients\n    train_df = hourly[hourly[\"patientID\"].isin(train_pat)].copy()\n    test_df  = hourly[hourly[\"patientID\"].isin(test_pat)].copy()\n\n    def build_sequences(df_in, feature_cols, label_col=\"hypo_label\", seq_len=SEQ_LEN):\n        X, y = [], []\n        for pid, grp in df_in.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            for i in range(len(grp) - seq_len):\n                X.append(grp.loc[i:i+seq_len-1, feature_cols].values)\n                y.append(int(grp.loc[i+seq_len, label_col]))\n        return np.array(X), np.array(y)\n\n    X_train, y_train = build_sequences(train_df, feature_cols, seq_len=seq_len)\n    X_test,  y_test  = build_sequences(test_df,  feature_cols, seq_len=seq_len)\n\n    print(f\"âœ… Sequences (Hourly/ALL_FEATURES, leakâ€‘free) â€” train: {X_train.shape}, test: {X_test.shape} | features={feature_cols}\")\n    return (X_train, y_train, X_test, y_test), (train_df, test_df)\n\n# ------------------------------------------------------\n# Balanced LSTM pipeline utilities (metrics, resampling)\n# ------------------------------------------------------\ndef _best_threshold_in_range(thresholds, scores, thr_min=THR_MIN, thr_max=THR_MAX):\n    thresholds = np.asarray(thresholds, dtype=float)\n    scores     = np.asarray(scores, dtype=float)\n    mask = np.isfinite(thresholds) & (thresholds >= thr_min) & (thresholds <= thr_max)\n    if mask.any():\n        idx_in = int(np.nanargmax(scores[mask]))\n        idx    = np.where(mask)[0][idx_in]\n        return float(thresholds[idx]), True\n    idx = int(np.nanargmax(scores))\n    return float(np.clip(thresholds[idx], thr_min, thr_max)), False\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    eps = tf.keras.backend.epsilon()\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n        ce = bce(y_true, y_pred)\n        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n        modulating = tf.pow(1.0 - p_t, gamma)\n        return alpha_t * modulating * ce\n    return loss\n\ndef _safe_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    if cm.shape != (2,2):\n        full = np.zeros((2,2), dtype=int)\n        full[:cm.shape[0], :cm.shape[1]] = cm\n        cm = full\n    return cm\n\ndef _specificity_overall(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef _specificity_per_class(y_true, y_pred, positive_label):\n    # TNR for the \"not positive_label\" class, computed by binarizing on that label\n    y_true_bin = (np.asarray(y_true).ravel() == positive_label).astype(int)\n    y_pred_bin = (np.asarray(y_pred).ravel() == positive_label).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1]).ravel()\n    return tn / (tn + fp + 1e-8)\n\ndef evaluate_full_metrics(y_true, y_pred, y_prob=None):\n    y_true = np.asarray(y_true).astype(int).ravel()\n    y_pred = np.asarray(y_pred).astype(int).ravel()\n    cm = _safe_confusion_matrix(y_true, y_pred)\n\n    metrics = {}\n    for lbl in [0,1]:\n        metrics[f\"Class{lbl}/Precision\"]   = precision_score(y_true, y_pred, pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Recall\"]      = recall_score(y_true, y_pred,    pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/F1\"]          = f1_score(y_true, y_pred,        pos_label=lbl, zero_division=0)\n        metrics[f\"Class{lbl}/Specificity\"] = _specificity_per_class(y_true, y_pred, positive_label=lbl)\n        metrics[f\"Class{lbl}/Support\"]     = int(np.sum(y_true == lbl))\n\n    # overall (hard preds)\n    metrics[\"Overall/Accuracy\"]             = accuracy_score(y_true, y_pred)\n    metrics[\"Overall/Precision_macro\"]      = precision_score(y_true, y_pred, average='macro',    zero_division=0)\n    metrics[\"Overall/Precision_weighted\"]   = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    metrics[\"Overall/Recall_macro\"]         = recall_score(y_true, y_pred,    average='macro',    zero_division=0)\n    metrics[\"Overall/Recall_weighted\"]      = recall_score(y_true, y_pred,    average='weighted', zero_division=0)\n    metrics[\"Overall/F1_macro\"]             = f1_score(y_true, y_pred,        average='macro',    zero_division=0)\n    metrics[\"Overall/F1_weighted\"]          = f1_score(y_true, y_pred,        average='weighted', zero_division=0)\n    metrics[\"Overall/Specificity\"]          = _specificity_overall(y_true, y_pred)\n    mse_pred                                = mean_squared_error(y_true, y_pred)\n    metrics[\"Overall/RMSE_pred\"]            = float(np.sqrt(mse_pred))\n\n    # prob-based\n    if y_prob is not None:\n        y_prob = np.asarray(y_prob, dtype=float).ravel()\n        try:  metrics[\"Overall/ROC-AUC\"] = roc_auc_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/ROC-AUC\"] = np.nan\n        try:  metrics[\"Overall/PR-AUC\"]  = average_precision_score(y_true, y_prob)\n        except ValueError: metrics[\"Overall/PR-AUC\"] = np.nan\n        mse_prob                          = mean_squared_error(y_true, y_prob)\n        metrics[\"Overall/RMSE_prob\"]      = float(np.sqrt(mse_prob))\n    else:\n        metrics[\"Overall/ROC-AUC\"]  = np.nan\n        metrics[\"Overall/PR-AUC\"]   = np.nan\n        metrics[\"Overall/RMSE_prob\"]= np.nan\n\n    return metrics\n\ndef make_class_weight(y):\n    y  = np.asarray(y).astype(int).ravel()\n    n0 = max(1, (y==0).sum()); n1 = max(1, (y==1).sum()); N = n0+n1\n    w0 = N/(2.0*n0); w1 = N/(2.0*n1)\n    return {0: float(w0), 1: float(w1)}\n\ndef augment(X, y, sigma=AUGMENT_SIGMA):\n    if sigma is None or sigma <= 0: \n        return X, y\n    noise = np.random.normal(0, sigma, X.shape)\n    return np.vstack([X, X+noise]), np.hstack([y, y])\n\ndef seq_resample(X, y, method=\"none\", random_state=RANDOM_STATE):\n    \"\"\"\n    Sequence-level resampling.\n    method âˆˆ {\"none\",\"oversample_seq\",\"undersample_seq\",\"smote\",\"smoteenn\",\"smotetomek\"}\n    \"\"\"\n    X = np.asarray(X); y = np.asarray(y).astype(int).ravel()\n    n, T, F = X.shape\n\n    if method == \"none\":\n        return X, y\n\n    if method in {\"oversample_seq\",\"undersample_seq\"}:\n        rng = np.random.default_rng(random_state)\n        idx0 = np.where(y==0)[0]; idx1 = np.where(y==1)[0]\n        n0, n1 = len(idx0), len(idx1)\n        if n0==0 or n1==0:\n            return X, y\n\n        if method == \"oversample_seq\":\n            if n1 < n0:\n                add = rng.choice(idx1, size=n0-n1, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n            else:\n                add = rng.choice(idx0, size=n1-n0, replace=True)\n                keep = np.concatenate([idx0, idx1, add])\n        else:\n            if n0 > n1:\n                keep0 = rng.choice(idx0, size=n1, replace=False)\n                keep  = np.concatenate([keep0, idx1])\n            else:\n                keep1 = rng.choice(idx1, size=n0, replace=False)\n                keep  = np.concatenate([idx0, keep1])\n\n        rng.shuffle(keep)\n        return X[keep], y[keep]\n\n    # SMOTE family on flattened sequences with guard\n    Xf = X.reshape(n, -1)\n    minority_n = int((y==1).sum())\n    majority_n = int((y==0).sum())\n    if minority_n < 2 or majority_n < 2:\n        print(\"âš ï¸ Not enough samples for SMOTE/SMOTEENN/SMOTETomek; skipping resampling.\")\n        return X, y\n\n    if method == \"smote\":\n        k_neighbors = max(1, min(5, minority_n-1))\n        sm = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n        Xr, yr = sm.fit_resample(Xf, y)\n    elif method == \"smoteenn\":\n        Xr, yr = SMOTEENN(random_state=random_state).fit_resample(Xf, y)\n    elif method == \"smotetomek\":\n        Xr, yr = SMOTETomek(random_state=random_state).fit_resample(Xf, y)\n    else:\n        raise ValueError(f\"Unknown resampling method: {method}\")\n    return Xr.reshape(-1, T, F), yr\n\ndef make_balanced_test(X_test, y_test, random_state=RANDOM_STATE):\n    X_test = np.asarray(X_test)\n    y_test = np.asarray(y_test).astype(int).ravel()\n    idx0, idx1 = np.where(y_test==0)[0], np.where(y_test==1)[0]\n    if len(idx0)==0 or len(idx1)==0: return X_test, y_test\n    m = min(len(idx0), len(idx1))\n    rs = np.random.RandomState(random_state)\n    keep = np.concatenate([rs.choice(idx0, m, replace=False), rs.choice(idx1, m, replace=False)])\n    rs.shuffle(keep)\n    return X_test[keep], y_test[keep]\n\ndef define_models(input_shape):\n    return {\n        \"LSTM_100\": Sequential([\n            Input(shape=input_shape),\n            LSTM(100, return_sequences=True), Dropout(0.2),\n            LSTM(50), Dropout(0.2),\n            Dense(25, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_50\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True), Dropout(0.2),\n            LSTM(25), Dropout(0.2),\n            Dense(10, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_25_L1\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l1(1e-5)), Dropout(0.2),\n            LSTM(25, kernel_regularizer=l1(1e-5)), Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l1(1e-5)),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"LSTM_25_L2\": Sequential([\n            Input(shape=input_shape),\n            LSTM(50, return_sequences=True, kernel_regularizer=l2(1e-5)), Dropout(0.2),\n            LSTM(25, kernel_regularizer=l2(1e-5)), Dropout(0.2),\n            Dense(10, activation='relu', kernel_regularizer=l2(1e-5)),\n            Dense(1, activation='sigmoid')\n        ]),\n        \"BiLSTM\": Sequential([\n            Input(shape=input_shape),\n            Bidirectional(LSTM(64, return_sequences=True)), Dropout(0.2),\n            Bidirectional(LSTM(32)), Dropout(0.2),\n            Dense(16, activation='relu'),\n            Dense(1, activation='sigmoid')\n        ])\n    }\n\ndef run_balanced_lstm_pipeline(X_train, y_train, X_test, y_test,\n                               resample_methods=RESAMPLE_METHODS,\n                               thr_min=THR_MIN, thr_max=THR_MAX,\n                               random_state=RANDOM_STATE):\n    \"\"\"\n    Trains all models across resampling methods, writes plots + CSV summary,\n    and returns the results DataFrame.\n    \"\"\"\n    os.makedirs(\"checkpoints\", exist_ok=True)\n    os.makedirs(\"plots\", exist_ok=True)\n    os.makedirs(\"outputs\", exist_ok=True)\n\n    # tiny augmentation (optional)\n    X_train_aug, y_train_aug = augment(X_train, y_train, sigma=AUGMENT_SIGMA)\n\n    # balanced test copy (for fair diagnostic read)\n    X_test_bal, y_test_bal = make_balanced_test(X_test, y_test)\n\n    results = {}       # key -> metrics dict\n    roc_data = {}      # (method, model) -> (fpr, tpr, auc)\n    pr_data  = {}      # (method, model) -> (recall, precision, ap)\n    best_thresholds = {}  # (method, model) -> {\"youden\": t, \"f1\": t}\n\n    def train_eval_one(method_name, model_name, model, Xtr, ytr, Xte, yte, XteB, yteB):\n        tag = f\"{method_name}__{model_name}\"\n        print(f\"\\nðŸš€ Training [{tag}] with class-weighted focal loss\")\n        es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n        cp = ModelCheckpoint(f\"checkpoints/{tag}.h5\", save_best_only=True, monitor='val_loss', verbose=0)\n        model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n\n        class_weight = make_class_weight(ytr)\n        t0 = time.time()\n        model.fit(Xtr, ytr, epochs=5, batch_size=32,\n                  validation_data=(Xte, yte),\n                  callbacks=[es, cp], verbose=1,\n                  class_weight=class_weight)\n        print(f\"â±ï¸ Training Time: {time.time()-t0:.2f}s\")\n\n        # probabilities\n        p_tr  = model.predict(Xtr,  verbose=0).ravel()\n        p_te  = model.predict(Xte,  verbose=0).ravel()\n        p_teB = model.predict(XteB, verbose=0).ravel()\n\n        # thresholds (on ORIGINAL test), constrained to [thr_min, thr_max]\n        try:\n            fpr, tpr, thr_roc = roc_curve(yte, p_te); auc_roc = auc(fpr, tpr)\n        except ValueError:\n            fpr, tpr, thr_roc, auc_roc = np.array([0,1]), np.array([0,1]), np.array([0.5]), np.nan\n        youden = tpr - fpr\n        t_roc, _ = _best_threshold_in_range(thr_roc, youden, thr_min, thr_max)\n\n        prec, rec, thr_pr = precision_recall_curve(yte, p_te)\n        f1s = 2*prec[:-1]*rec[:-1] / (prec[:-1]+rec[:-1]+1e-8)\n        t_pr, _ = _best_threshold_in_range(thr_pr, f1s, thr_min, thr_max)\n        ap_val  = average_precision_score(yte, p_te)\n\n        roc_data[(method_name, model_name)] = (fpr, tpr, auc_roc)\n        pr_data[(method_name, model_name)]  = (rec, prec, ap_val)\n        best_thresholds[(method_name, model_name)] = {\"youden\": t_roc, \"f1\": t_pr}\n        print(f\"ðŸ“Œ [{tag}] thresholds â†’ Youden={t_roc:.4f}, PR-F1={t_pr:.4f} (window [{thr_min},{thr_max}])\")\n\n        eval_ts = sorted(set([thr_min, 0.50, thr_max, float(t_roc), float(t_pr)]))\n        # evaluate at all thresholds on train / test / testBalanced\n        for t in eval_ts:\n            yhat_tr  = (p_tr  >= t).astype(int)\n            yhat_te  = (p_te  >= t).astype(int)\n            yhat_teB = (p_teB >= t).astype(int)\n\n            results[f\"{tag}__thr_{t:.2f}__train\"]         = evaluate_full_metrics(ytr,  yhat_tr,  p_tr)\n            results[f\"{tag}__thr_{t:.2f}__test\"]          = evaluate_full_metrics(yte,  yhat_te,  p_te)\n            results[f\"{tag}__thr_{t:.2f}__testBalanced\"]  = evaluate_full_metrics(yteB, yhat_teB, p_teB)\n\n    # run all methods x models\n    input_shape = (X_train_aug.shape[1], X_train_aug.shape[2])\n    for METHOD in resample_methods:\n        Xtr_rs, ytr_rs = seq_resample(X_train_aug, y_train_aug, method=METHOD, random_state=random_state)\n        print(f\"\\nðŸ” Resampling: {METHOD} â†’ X={Xtr_rs.shape}, y={Counter(ytr_rs)}\")\n        for mname, model in define_models(input_shape).items():\n            train_eval_one(METHOD, mname, model, Xtr_rs, ytr_rs, X_test, y_test, X_test_bal, y_test_bal)\n\n    # --------------------------\n    # Curves (optional plots)\n    # --------------------------\n    plt.figure(figsize=(14,6))\n    # ROC\n    plt.subplot(1,2,1)\n    for (meth, mname), (fpr, tpr, auc_roc) in roc_data.items():\n        plt.plot(fpr, tpr, label=f'{meth}/{mname} (AUC={auc_roc:.3f})')\n    plt.plot([0,1],[0,1],'--',label='Random')\n    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC'); plt.legend(fontsize=8)\n    # PR\n    plt.subplot(1,2,2)\n    for (meth, mname), (rec, prec, ap) in pr_data.items():\n        plt.plot(rec, prec, label=f'{meth}/{mname} (AP={ap:.3f})')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR'); plt.legend(fontsize=8)\n    plt.tight_layout(); plt.savefig(\"plots/combined_roc_pr_curves.png\", dpi=300); plt.show()\n\n    # --------------------------\n    # Summaries\n    # --------------------------\n    results_df = pd.DataFrame(results).T\n    results_df = results_df.reset_index().rename(columns={\"index\":\"Key\"})\n\n    # Split extraction\n    k = results_df[\"Key\"].str.strip()\n    split = np.where(k.str.endswith(\"__train\"), \"train\",\n             np.where(k.str.endswith(\"__testBalanced\"), \"testBalanced\",\n             np.where(k.str.endswith(\"__test\"), \"test\", np.nan)))\n    results_df[\"Split\"] = split\n\n    # Method, Model, Threshold extraction\n    parts = k.str.split(\"__\")\n    results_df[\"Method\"] = parts.str[0]\n    results_df[\"Model\"]  = parts.str[1]\n    thr_str = parts.str[2].str.replace(\"thr_\",\"\", regex=False)\n    with np.errstate(all='ignore'):\n        results_df[\"Threshold\"] = pd.to_numeric(thr_str, errors=\"coerce\")\n\n    # Save\n    results_df.round(6).to_csv(\"outputs/results_summary_all1.csv\", index=False)\n    print(\"\\nðŸ“ Saved files:\")\n    print(\" - plots/combined_roc_pr_curves.png\")\n    print(\" - outputs/results_summary_all1.csv\")\n\n    # Quick sanity\n    print(\"\\nSplit counts:\")\n    print(results_df[\"Split\"].value_counts(dropna=False))\n\n    # Leaderboards\n    eval_test_df  = results_df[results_df[\"Split\"]==\"test\"].copy()\n    eval_tbal_df  = results_df[results_df[\"Split\"]==\"testBalanced\"].copy()\n\n    def top_k(df, by_col, k=10, cols=None):\n        if df.empty:\n            return pd.DataFrame(columns=(cols or []))\n        if cols is None:\n            cols = ['Method','Model','Threshold','Overall/F1_weighted','Overall/Recall_weighted',\n                    'Overall/Precision_weighted','Overall/ROC-AUC','Overall/PR-AUC','Overall/Accuracy']\n        present = [c for c in cols if c in df.columns]\n        return df.sort_values(by_col, ascending=False)[present].head(k).round(4)\n\n    print(\"\\nðŸ”½ ORIGINAL TEST â€” top by Overall/F1_weighted\")\n    print(top_k(eval_test_df, 'Overall/F1_weighted'))\n\n    print(\"\\nðŸ”½ BALANCED TEST â€” top by Overall/F1_weighted\")\n    print(top_k(eval_tbal_df, 'Overall/F1_weighted'))\n\n    def best_per_model(df):\n        if df.empty:\n            return df\n        idx = df.groupby(['Method','Model'])['Overall/F1_weighted'].idxmax()\n        return df.loc[idx].sort_values(['Overall/F1_weighted'], ascending=False)\n\n    overall_cols = [\n        'Overall/Accuracy','Overall/Precision_macro','Overall/Recall_macro','Overall/F1_macro',\n        'Overall/Precision_weighted','Overall/Recall_weighted','Overall/F1_weighted',\n        'Overall/Specificity','Overall/ROC-AUC','Overall/PR-AUC',\n        'Overall/RMSE_pred','Overall/RMSE_prob'\n    ]\n    class_cols = [\n        'Class0/Precision','Class0/Recall','Class0/F1','Class0/Specificity','Class0/Support',\n        'Class1/Precision','Class1/Recall','Class1/F1','Class1/Specificity','Class1/Support'\n    ]\n\n    print(\"\\n=== ORIGINAL TEST â€” best per (Method,Model) ===\")\n    best_test = best_per_model(eval_test_df)\n    if best_test.empty:\n        print(\"âš ï¸ No TEST rows found â€” check Split counts above.\")\n    else:\n        print(best_test[['Method','Model','Threshold']+[c for c in overall_cols if c in best_test.columns]].round(4))\n        print(\"\\n--- Per-class breakdown:\")\n        print(best_test[['Method','Model','Threshold']+[c for c in class_cols if c in best_test.columns]].round(4))\n\n    print(\"\\n=== BALANCED TEST â€” best per (Method,Model) ===\")\n    best_tbal = best_per_model(eval_tbal_df)\n    if best_tbal.empty:\n        print(\"âš ï¸ No BALANCED TEST rows found â€” check Split counts above.\")\n    else:\n        print(best_tbal[['Method','Model','Threshold']+[c for c in overall_cols if c in best_tbal.columns]].round(4))\n        print(\"\\n--- Per-class breakdown:\")\n        print(best_tbal[['Method','Model','Threshold']+[c for c in class_cols if c in best_tbal.columns]].round(4))\n\n    return results_df\n\n# ------------------------------------------------------\n# all CGM- (raw intraday) sequence builder\n# ------------------------------------------------------\ndef build_cgm_only_sequences_from_intraday(\n    in_csv=CSV_INTRADAY_BASIC,\n    seq_len=SEQ_LEN,\n    label_cutoff=HYPO_CUTOFF,\n    min_cgm_per_hour=MIN_CGM_PER_H,\n    feature_cols=(\"cgm_mean\",\"cgm_std\",\"cgm_min\",\"cgm_max\"),  # customize: (\"cgm_mean\",\"cgm_std\",\"cgm_min\",\"cgm_max\")\n    test_size=0.4,\n    random_state=RANDOM_STATE\n):\n    \"\"\"\n    Build CGM- hourly features from intraday and convert to sequences.\n    Splits by patient to avoid leakage.\n    \"\"\"\n    if not os.path.exists(in_csv):\n        raise FileNotFoundError(f\"Input not found: {in_csv}\")\n\n    df = pd.read_csv(in_csv)\n    df[\"start\"] = to_dt(df.get(\"start\"))\n    df[\"hour\"]  = df[\"start\"].dt.floor(\"h\")\n\n    base = df[[\"patientID\",\"hour\",\"cgm\"]].dropna(subset=[\"cgm\"])\n    valid = base.groupby([\"patientID\",\"hour\"]).filter(lambda g: g[\"cgm\"].notna().sum() >= min_cgm_per_hour)\n\n    feats = (\n        valid.groupby([\"patientID\",\"hour\"])\n             .agg(\n                 cgm_std=(\"cgm\",\"std\"),\n                 cgm_min=(\"cgm\",\"min\"),\n                 cgm_mean=(\"cgm\",\"mean\"),\n                 cgm_max=(\"cgm\",\"max\"),\n                 hypo_label=(\"cgm\", lambda x: int((x < label_cutoff).any()))\n             )\n             .reset_index()\n             .sort_values([\"patientID\",\"hour\"])\n             .reset_index(drop=True)\n    )\n\n    # leakage-proof split\n    unique_patients = feats[\"patientID\"].unique()\n    train_pat, test_pat = train_test_split(unique_patients, test_size=test_size, random_state=random_state)\n    train_df = feats[feats[\"patientID\"].isin(train_pat)].copy()\n    test_df  = feats[feats[\"patientID\"].isin(test_pat)].copy()\n\n    feature_cols = list(feature_cols)\n    for col in feature_cols + [\"hypo_label\"]:\n        if col not in feats.columns:\n            raise KeyError(f\"Column '{col}' not found. Available: {feats.columns.tolist()}\")\n\n    def build_sequences(df_in, feature_cols, label_col=\"hypo_label\", seq_len=seq_len):\n        X, y = [], []\n        for pid, grp in df_in.groupby(\"patientID\"):\n            grp = grp.sort_values(\"hour\").reset_index(drop=True)\n            for i in range(len(grp) - seq_len):\n                X.append(grp.loc[i:i+seq_len-1, feature_cols].values)\n                y.append(int(grp.loc[i+seq_len, label_col]))\n        return np.array(X), np.array(y)\n\n    X_train, y_train = build_sequences(train_df, feature_cols)\n    X_test,  y_test  = build_sequences(test_df,  feature_cols)\n\n    print(f\"âœ… Sequences (CGM-only) â€” train: {X_train.shape}, test: {X_test.shape} | features={feature_cols}\")\n    return (X_train, y_train, X_test, y_test), (train_df, test_df)\n\n# ------------------------------------------------------\n# Main control flags\n# ------------------------------------------------------\nRUN_FEATURE_BUILD          = False  # Build Ramadan hourly features + leak-free PCA\nRUN_TRAINING_FROM_HOURLY   = True   # Train models using sequences built from hourly features\nRUN_TRAINING_FROM_CGM_ONLY = False  # Train models using CGM-only sequences from raw intraday\n\n# ------------------------------------------------------\n# Orchestrate\n# ------------------------------------------------------\nif __name__ == \"__main__\":\n    hourly_features = None\n    split_pat       = None\n\n    # Part A: Ramadan hourly features with leak-free PCA\n    if RUN_FEATURE_BUILD or RUN_TRAINING_FROM_HOURLY:\n        try:\n            hourly_features, split_pat = build_ramadan_hourly_features_leakfree(\n                in_csv=CSV_INTRADAY_WITH_VISITS,\n                out_csv=OUT_HOURLY_CSV,\n                min_cgm_per_hour=MIN_CGM_PER_H,\n                test_size=0.4,\n                random_state=RANDOM_STATE\n            )\n        except Exception as e:\n            print(f\"[WARN] Skipping Ramadan feature build: {e}\")\n\n    # Part B â€” Train from HOURLY sequences with ALL_FEATURES\n    if RUN_TRAINING_FROM_HOURLY:\n        try:\n            if split_pat is None:\n                raise RuntimeError(\"No patient split found. Ensure feature build step ran successfully.\")\n            (X_train, y_train, X_test, y_test), _ = build_lstm_sequences_from_hourly(\n                hourly_df_or_path=OUT_HOURLY_CSV,\n                seq_len=SEQ_LEN,\n                feature_cols=ALL_FEATURES,\n                split_patients=split_pat\n            )\n            _ = run_balanced_lstm_pipeline(X_train, y_train, X_test, y_test)\n        except Exception as e:\n            print(f\"[WARN] Skipping hourly-based training: {e}\")\n\n    # Part B option 2 â€” Train from CGM-only (raw intraday) sequences\n    if RUN_TRAINING_FROM_CGM_ONLY:\n        try:\n            (X_train_cgm, y_train_cgm, X_test_cgm, y_test_cgm), _ = build_cgm_only_sequences_from_intraday(\n                in_csv=CSV_INTRADAY_BASIC,\n                seq_len=SEQ_LEN,\n                label_cutoff=HYPO_CUTOFF,\n                min_cgm_per_hour=MIN_CGM_PER_H,\n                feature_cols=(\"cgm_mean\",)  # or: (\"cgm_mean\",\"cgm_std\",\"cgm_min\",\"cgm_max\")\n            )\n            _ = run_balanced_lstm_pipeline(X_train_cgm, y_train_cgm, X_test_cgm, y_test_cgm)\n        except Exception as e:\n            print(f\"[WARN] Skipping CGM-only training: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Post-run analysis: Top-15 ROC/PR curves + Best-model Confusion Matrix\n# (Run this AFTER your previous training script finished)\n# ============================================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import (\n    roc_curve, auc, precision_recall_curve, average_precision_score,\n    confusion_matrix, ConfusionMatrixDisplay\n)\n\n# ---- Config (must match your training run) ----\nRANDOM_STATE = 42\nSEQ_LEN      = 36\nOUT_HOURLY_CSV = \"/kaggle/working/dynamic_hourly_features_ramadan.csv\"\nCSV_INTRADAY_WITH_VISITS = \"/kaggle/input/hmcdataset/intraday_with_visits.csv\"\n\n# Feature set used in the training code\nALL_FEATURES = (\n    \"cgm_min\",\"cgm_max\",\"cgm_mean\",\"cgm_std\",\n    \"cgm_mean_plus_std\",\"cgm_mean_minus_std\",\n    \"pca_cgm1\",\"pca_cgm2\",\"pca_cgm3\",\n    \"pc1_activity_energy\",\"pc2_physiology\",\"pc3_sleep_rest\"\n)\n\nRESULTS_CSV = \"outputs/results_summary_all1.csv\"\nCKPT_FMT    = \"checkpoints/{method}__{model}.h5\"\nPLOT_ROC    = \"plots/top15_roc.png\"\nPLOT_PR     = \"plots/top15_pr.png\"\nPLOT_CM     = \"plots/best_confusion.png\"\n\n# ---- 0) Recreate the same test split + sequences (deterministic) ----\n# Uses your previously defined functions. If they live in another module, import them instead.\nhourly_features, split_pat = build_ramadan_hourly_features_leakfree(\n    in_csv=CSV_INTRADAY_WITH_VISITS,\n    out_csv=OUT_HOURLY_CSV,\n    min_cgm_per_hour=4,\n    test_size=0.3,\n    random_state=RANDOM_STATE,\n    save_artifacts=False\n)\n\n(X_train, y_train, X_test, y_test), _ = build_lstm_sequences_from_hourly(\n    hourly_df_or_path=OUT_HOURLY_CSV,\n    seq_len=SEQ_LEN,\n    feature_cols=ALL_FEATURES,\n    split_patients=split_pat\n)\n\ninput_shape = (X_test.shape[1], X_test.shape[2])\n\n# ---- 1) Load and rank results (TEST split) ----\nif not os.path.exists(RESULTS_CSV):\n    raise FileNotFoundError(f\"Results CSV not found: {RESULTS_CSV}. Run the training pipeline first.\")\n\ndf = pd.read_csv(RESULTS_CSV)\neval_test = df[df[\"Split\"]==\"test\"].copy()\nif eval_test.empty:\n    raise RuntimeError(\"No TEST rows in results CSV. Re-run training or check the saved splits.\")\n\n# Add Brier from probability MSE if present; else fallback to NaN\nif \"Overall/MSE_prob\" in eval_test.columns:\n    eval_test[\"Brier\"] = eval_test[\"Overall/MSE_prob\"]\nelif \"Brier\" not in eval_test.columns:\n    eval_test[\"Brier\"] = np.nan\n\n# Ranking: Class1/Recall DESC, then F1_weighted DESC, PR-AUC DESC, Brier ASC\nrank_cols = [\"Class1/Recall\", \"Overall/F1_weighted\", \"Overall/PR-AUC\", \"Brier\"]\nfor c in rank_cols:\n    if c not in eval_test.columns:\n        eval_test[c] = np.nan\n\nranked = eval_test.sort_values(\n    by=[\"Class1/Recall\",\"Overall/F1_weighted\",\"Overall/PR-AUC\",\"Brier\"],\n    ascending=[False, True if \"Brier\" in [\"Class1/Recall\",\"Overall/F1_weighted\",\"Overall/PR-AUC\"] else False, False, True]\n)\n\n# Note: the mixed ascending list above is tricky; explicitly set as below:\nranked = eval_test.sort_values(\n    by=[\"Class1/Recall\",\"Overall/F1_weighted\",\"Overall/PR-AUC\",\"Brier\"],\n    ascending=[False, False, False, True]\n)\n\ntop15 = ranked.head(15).copy()\nif top15.empty:\n    raise RuntimeError(\"Top-15 selection is empty. Check your results CSV contents.\")\n\n# Extract method/model/threshold for each of the top 15\ntop15_info = top15[[\"Method\",\"Model\",\"Threshold\"]].reset_index(drop=True)\n\n# ---- 2) For each of these top 15, rebuild model, load weights, predict on test ----\n# We reuse your define_models() helper to ensure identical architectures.\nmodels = define_models(input_shape)\n\nroc_curves = {}   # tag -> (fpr, tpr, auc)\npr_curves  = {}   # tag -> (rec, prec, ap)\nprobs_map  = {}   # tag -> predicted probabilities on test\n\nfor i, row in top15_info.iterrows():\n    method = str(row[\"Method\"])\n    model_name = str(row[\"Model\"])\n    tag = f\"{method}/{model_name}\"\n\n    ckpt_path = CKPT_FMT.format(method=method, model=model_name)\n    if not os.path.exists(ckpt_path):\n        print(f\"âš ï¸ Missing checkpoint for {tag}: {ckpt_path}. Skipping.\")\n        continue\n\n    # Build the same architecture and load weights\n    if model_name not in models:\n        print(f\"âš ï¸ Unknown model arch in results: {model_name}. Skipping.\")\n        continue\n    m = define_models(input_shape)[model_name]\n    m.load_weights(ckpt_path)\n\n    # Predict on ORIGINAL TEST\n    p_test = m.predict(X_test, verbose=0).ravel()\n    probs_map[tag] = p_test\n\n    # ROC\n    try:\n        fpr, tpr, thr = roc_curve(y_test, p_test)\n        auc_roc = auc(fpr, tpr)\n    except ValueError:\n        fpr, tpr, auc_roc = np.array([0,1]), np.array([0,1]), np.nan\n    roc_curves[tag] = (fpr, tpr, auc_roc)\n\n    # PR\n    prec, rec, thr_pr = precision_recall_curve(y_test, p_test)\n    ap = average_precision_score(y_test, p_test) if np.isfinite(p_test).all() else np.nan\n    pr_curves[tag] = (rec, prec, ap)\n\n# ---- 3) Plot ROC-AUC and PR-AUC for Top 15 ----\nos.makedirs(\"plots\", exist_ok=True)\n\n# ROC\nplt.figure(figsize=(8,6))\nplotted = 0\nfor tag, (fpr, tpr, auc_roc) in roc_curves.items():\n    plt.plot(fpr, tpr, label=f\"{tag} (AUC={auc_roc:.3f})\")\n    plotted += 1\nplt.plot([0,1],[0,1],'--',label=\"Random\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Top-15 Models â€” ROC Curves (TEST)\")\nplt.legend(fontsize=8, loc=\"lower right\", ncol=1)\nplt.tight_layout()\nplt.savefig(PLOT_ROC, dpi=300)\nplt.show()\nprint(f\"ðŸ–¼ï¸ Saved ROC plot: {PLOT_ROC}  (curves drawn for {plotted} configs)\")\n\n# PR\nplt.figure(figsize=(8,6))\nplotted = 0\nfor tag, (rec, prec, ap) in pr_curves.items():\n    plt.plot(rec, prec, label=f\"{tag} (AP={ap:.3f})\")\n    plotted += 1\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Top-15 Models â€” Precisionâ€“Recall Curves (TEST)\")\nplt.legend(fontsize=8, loc=\"lower left\", ncol=1)\nplt.tight_layout()\nplt.savefig(PLOT_PR, dpi=300)\nplt.show()\nprint(f\"ðŸ–¼ï¸ Saved PR plot: {PLOT_PR}  (curves drawn for {plotted} configs)\")\n\n# ---- 4) Confusion matrix for the SINGLE best-recall model at its saved threshold ----\nbest_row = top15.iloc[0]\nbest_method = str(best_row[\"Method\"])\nbest_model  = str(best_row[\"Model\"])\nbest_thr    = float(best_row[\"Threshold\"])\nbest_tag    = f\"{best_method}/{best_model}\"\n\nif best_tag not in probs_map:\n    # try to load it now if it was skipped earlier\n    ckpt_path = CKPT_FMT.format(method=best_method, model=best_model)\n    if os.path.exists(ckpt_path):\n        m = define_models(input_shape)[best_model]\n        m.load_weights(ckpt_path)\n        probs_map[best_tag] = m.predict(X_test, verbose=0).ravel()\n    else:\n        raise FileNotFoundError(f\"Checkpoint for best model not found: {ckpt_path}\")\n\nyhat_best = (probs_map[best_tag] >= best_thr).astype(int)\ncm = confusion_matrix(y_test, yhat_best, labels=[0,1])\n\nplt.figure(figsize=(5.5,5))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\ndisp.plot(values_format='d', cmap='Blues', colorbar=False)\nplt.title(f\"Best Recall Model â€” Confusion Matrix\\n{best_tag} @ thr={best_thr:.2f} (TEST)\")\nplt.tight_layout()\nplt.savefig(PLOT_CM, dpi=300)\nplt.show()\nprint(f\"ðŸ–¼ï¸ Saved confusion matrix: {PLOT_CM}\")\n\n# Also print the key line so you can drop it in the paper/table\ntn, fp, fn, tp = cm.ravel()\nprint(f\"[Best Recall] {best_tag} @ t={best_thr:.2f} â†’ TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}